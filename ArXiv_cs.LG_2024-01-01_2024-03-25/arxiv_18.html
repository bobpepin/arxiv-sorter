
<!doctype html>
<meta charset="utf-8">
<style>
body { margin: 20px; }
</style>
<script>
function toggle(arxiv) {
  let elt = document.getElementById(arxiv);
  console.log(elt, elt.style.display);
  if(elt.style.display == "block") {
    elt.style.display = "none";
  } else {
    elt.style.display = "block";
  }
}
</script>
<div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.14547v1")'>Estimating Physical Information Consistency of Channel Data Augmentation
  for Remote Sensing Images</div>
<div id='2403.14547v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T16:48:45Z</div><div>Authors: Tom Burgert, Begüm Demir</div><div style='padding-top: 10px; width: 80ex'>The application of data augmentation for deep learning (DL) methods plays an
important role in achieving state-of-the-art results in supervised,
semi-supervised, and self-supervised image classification. In particular,
channel transformations (e.g., solarize, grayscale, brightness adjustments) are
integrated into data augmentation pipelines for remote sensing (RS) image
classification tasks. However, contradicting beliefs exist about their proper
applications to RS images. A common point of critique is that the application
of channel augmentation techniques may lead to physically inconsistent spectral
data (i.e., pixel signatures). To shed light on the open debate, we propose an
approach to estimate whether a channel augmentation technique affects the
physical information of RS images. To this end, the proposed approach estimates
a score that measures the alignment of a pixel signature within a time series
that can be naturally subject to deviations caused by factors such as
acquisition conditions or phenological states of vegetation. We compare the
scores associated with original and augmented pixel signatures to evaluate the
physical consistency. Experimental results on a multi-label image
classification task show that channel augmentations yielding a score that
exceeds the expected deviation of original pixel signatures can not improve the
performance of a baseline model trained without augmentation.</div><div><a href='http://arxiv.org/abs/2403.14547v1'>2403.14547v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.07528v1")'>Automatic characterization of boulders on planetary surfaces from
  high-resolution satellite images</div>
<div id='2401.07528v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T08:14:23Z</div><div>Authors: Nils C. Prieur, Brian Amaro, Emiliano Gonzalez, Hannah Kerner, Sergei Medvedev, Lior Rubanenko, Stephanie C. Werner, Zhiyong Xiao8, Dmitry Zastrozhnov, Mathieu G. A. Lapôtre</div><div style='padding-top: 10px; width: 80ex'>Boulders form from a variety of geological processes, which their size,
shape, and orientation may help us better understand. Furthermore, they
represent potential hazards to spacecraft landing that need to be
characterized. However, mapping individual boulders across vast areas is
extremely labor-intensive, often limiting the extent over which they are
characterized and the statistical robustness of obtained boulder morphometrics.
To automate boulder characterization, we use an instance segmentation neural
network, Mask R-CNN, to detect and outline boulders in high-resolution
satellite images. Our neural network, BoulderNet, was trained from a dataset of
&gt; 33,000 boulders in &gt; 750 image tiles from Earth, the Moon, and Mars.
BoulderNet not only correctly detects the majority of boulders in images, but
it identifies the outline of boulders with high fidelity, achieving average
precision and recall values of 72% and 64% relative to manually digitized
boulders from the test dataset, when only detections with
intersection-over-union ratios &gt; 50% are considered valid. These values are
similar to those obtained by human mappers. On Earth, equivalent boulder
diameters, aspect ratios, and orientations extracted from predictions were
benchmarked against ground measurements and yield values within 15%, 0.20, and
20 degrees of their ground-truth values, respectively. BoulderNet achieves
better boulder detection and characterization performance relative to existing
methods, providing a versatile open-source tool to characterize entire boulder
fields on planetary surfaces.</div><div><a href='http://arxiv.org/abs/2401.07528v1'>2401.07528v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15039v1")'>Descripción automática de secciones delgadas de rocas: una
  aplicación Web</div>
<div id='2402.15039v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T01:22:32Z</div><div>Authors: Stalyn Paucar, Christian Mejía-Escobar y Víctor Collaguazo</div><div style='padding-top: 10px; width: 80ex'>The identification and characterization of various rock types is one of the
fundamental activities for geology and related areas such as mining, petroleum,
environment, industry and construction. Traditionally, a human specialist is
responsible for analyzing and explaining details about the type, composition,
texture, shape and other properties using rock samples collected in-situ or
prepared in a laboratory. The results become subjective based on experience, in
addition to consuming a large investment of time and effort. The present
proposal uses artificial intelligence techniques combining computer vision and
natural language processing to generate a textual and verbal description from a
thin section image of rock. We build a dataset of images and their respective
textual descriptions for the training of a model that associates the relevant
features of the image extracted by EfficientNetB7 with the textual description
generated by a Transformer network, reaching an accuracy value of 0.892 and a
BLEU value of 0.71. This model can be a useful resource for research,
professional and academic work, so it has been deployed through a Web
application for public use.</div><div><a href='http://arxiv.org/abs/2402.15039v1'>2402.15039v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.07124v1")'>Concrete Surface Crack Detection with Convolutional-based Deep Learning
  Models</div>
<div id='2401.07124v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-13T17:31:12Z</div><div>Authors: Sara Shomal Zadeh, Sina Aalipour birgani, Meisam Khorshidi, Farhad Kooban</div><div style='padding-top: 10px; width: 80ex'>Effective crack detection is pivotal for the structural health monitoring and
inspection of buildings. This task presents a formidable challenge to computer
vision techniques due to the inherently subtle nature of cracks, which often
exhibit low-level features that can be easily confounded with background
textures, foreign objects, or irregularities in construction. Furthermore, the
presence of issues like non-uniform lighting and construction irregularities
poses significant hurdles for autonomous crack detection during building
inspection and monitoring. Convolutional neural networks (CNNs) have emerged as
a promising framework for crack detection, offering high levels of accuracy and
precision. Additionally, the ability to adapt pre-trained networks through
transfer learning provides a valuable tool for users, eliminating the need for
an in-depth understanding of algorithm intricacies. Nevertheless, it is
imperative to acknowledge the limitations and considerations when deploying
CNNs, particularly in contexts where the outcomes carry immense significance,
such as crack detection in buildings. In this paper, our approach to surface
crack detection involves the utilization of various deep-learning models.
Specifically, we employ fine-tuning techniques on pre-trained deep learning
architectures: VGG19, ResNet50, Inception V3, and EfficientNetV2. These models
are chosen for their established performance and versatility in image analysis
tasks. We compare deep learning models using precision, recall, and F1 scores.</div><div><a href='http://arxiv.org/abs/2401.07124v1'>2401.07124v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.12851v1")'>Classification of grapevine varieties using UAV hyperspectral imaging</div>
<div id='2401.12851v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T15:35:50Z</div><div>Authors: Alfonso López, Carlos Javier Ogayar, Francisco Ramón Feito, Joaquim João Sousa</div><div style='padding-top: 10px; width: 80ex'>The classification of different grapevine varieties is a relevant phenotyping
task in Precision Viticulture since it enables estimating the growth of
vineyard rows dedicated to different varieties, among other applications
concerning the wine industry. This task can be performed with destructive
methods that require time-consuming tasks, including data collection and
analysis in the laboratory. However, Unmanned Aerial Vehicles (UAV) provide a
more efficient and less prohibitive approach to collecting hyperspectral data,
despite acquiring noisier data. Therefore, the first task is the processing of
these data to correct and downsample large amounts of data. In addition, the
hyperspectral signatures of grape varieties are very similar. In this work, a
Convolutional Neural Network (CNN) is proposed for classifying seventeen
varieties of red and white grape variants. Rather than classifying single
samples, these are processed together with their neighbourhood. Hence, the
extraction of spatial and spectral features is addressed with 1) a spatial
attention layer and 2) Inception blocks. The pipeline goes from processing to
dataset elaboration, finishing with the training phase. The fitted model is
evaluated in terms of response time, accuracy and data separability, and
compared with other state-of-the-art CNNs for classifying hyperspectral data.
Our network was proven to be much more lightweight with a reduced number of
input bands, a lower number of trainable weights and therefore, reduced
training time. Despite this, the evaluated metrics showed much better results
for our network (~99% overall accuracy), in comparison with previous works
barely achieving 81% OA.</div><div><a href='http://arxiv.org/abs/2401.12851v1'>2401.12851v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.04748v1")'>Convolutional Neural Network Ensemble Learning for Hyperspectral
  Imaging-based Blackberry Fruit Ripeness Detection in Uncontrolled Farm
  Environment</div>
<div id='2401.04748v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-09T12:00:17Z</div><div>Authors: Chollette C. Olisah, Ben Trewhella, Bo Li, Melvyn L. Smith, Benjamin Winstone, E. Charles Whitfield, Felicidad Fernández Fernández, Harriet Duncalfe</div><div style='padding-top: 10px; width: 80ex'>Fruit ripeness estimation models have for decades depended on spectral index
features or colour-based features, such as mean, standard deviation, skewness,
colour moments, and/or histograms for learning traits of fruit ripeness.
Recently, few studies have explored the use of deep learning techniques to
extract features from images of fruits with visible ripeness cues. However, the
blackberry (Rubus fruticosus) fruit does not show obvious and reliable visible
traits of ripeness when mature and therefore poses great difficulty to fruit
pickers. The mature blackberry, to the human eye, is black before, during, and
post-ripening. To address this engineering application challenge, this paper
proposes a novel multi-input convolutional neural network (CNN) ensemble
classifier for detecting subtle traits of ripeness in blackberry fruits. The
multi-input CNN was created from a pre-trained visual geometry group 16-layer
deep convolutional network (VGG16) model trained on the ImageNet dataset. The
fully connected layers were optimized for learning traits of ripeness of mature
blackberry fruits. The resulting model served as the base for building
homogeneous ensemble learners that were ensemble using the stack generalization
ensemble (SGE) framework. The input to the network is images acquired with a
stereo sensor using visible and near-infrared (VIS-NIR) spectral filters at
wavelengths of 700 nm and 770 nm. Through experiments, the proposed model
achieved 95.1% accuracy on unseen sets and 90.2% accuracy with in-field
conditions. Further experiments reveal that machine sensory is highly and
positively correlated to human sensory over blackberry fruit skin texture.</div><div><a href='http://arxiv.org/abs/2401.04748v1'>2401.04748v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.15785v1")'>Real-time object detection and robotic manipulation for agriculture
  using a YOLO-based learning approach</div>
<div id='2401.15785v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-28T22:30:50Z</div><div>Authors: Hongyu Zhao, Zezhi Tang, Zhenhong Li, Yi Dong, Yuancheng Si, Mingyang Lu, George Panoutsos</div><div style='padding-top: 10px; width: 80ex'>The optimisation of crop harvesting processes for commonly cultivated crops
is of great importance in the aim of agricultural industrialisation. Nowadays,
the utilisation of machine vision has enabled the automated identification of
crops, leading to the enhancement of harvesting efficiency, but challenges
still exist. This study presents a new framework that combines two separate
architectures of convolutional neural networks (CNNs) in order to
simultaneously accomplish the tasks of crop detection and harvesting (robotic
manipulation) inside a simulated environment. Crop images in the simulated
environment are subjected to random rotations, cropping, brightness, and
contrast adjustments to create augmented images for dataset generation. The you
only look once algorithmic framework is employed with traditional rectangular
bounding boxes for crop localization. The proposed method subsequently utilises
the acquired image data via a visual geometry group model in order to reveal
the grasping positions for the robotic manipulation.</div><div><a href='http://arxiv.org/abs/2401.15785v1'>2401.15785v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.16570v1")'>Searching a Lightweight Network Architecture for Thermal Infrared
  Pedestrian Tracking</div>
<div id='2402.16570v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T13:48:44Z</div><div>Authors: Peng Gao, Xiao Liu, Yu Wang, Ru-Yue Yuan</div><div style='padding-top: 10px; width: 80ex'>Manually-designed network architectures for thermal infrared pedestrian
tracking (TIR-PT) require substantial effort from human experts. Neural
networks with ResNet backbones are popular for TIR-PT. However, TIR-PT is a
tracking task and more challenging than classification and detection. This
paper makes an early attempt to search an optimal network architecture for
TIR-PT automatically, employing single-bottom and dual-bottom cells as basic
search units and incorporating eight operation candidates within the search
space. To expedite the search process, a random channel selection strategy is
employed prior to assessing operation candidates. Classification, batch hard
triplet, and center loss are jointly used to retrain the searched architecture.
The outcome is a high-performance network architecture that is both parameter-
and computation-efficient. Extensive experiments proved the effectiveness of
the automated method.</div><div><a href='http://arxiv.org/abs/2402.16570v1'>2402.16570v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.09515v1")'>Enhancing Surveillance Camera FOV Quality via Semantic Line Detection
  and Classification with Deep Hough Transform</div>
<div id='2401.09515v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T15:30:17Z</div><div>Authors: Andrew C. Freeman, Wenjing Shi, Bin Hwang</div><div style='padding-top: 10px; width: 80ex'>The quality of recorded videos and images is significantly influenced by the
camera's field of view (FOV). In critical applications like surveillance
systems and self-driving cars, an inadequate FOV can give rise to severe safety
and security concerns, including car accidents and thefts due to the failure to
detect individuals and objects. The conventional methods for establishing the
correct FOV heavily rely on human judgment and lack automated mechanisms to
assess video and image quality based on FOV. In this paper, we introduce an
innovative approach that harnesses semantic line detection and classification
alongside deep Hough transform to identify semantic lines, thus ensuring a
suitable FOV by understanding 3D view through parallel lines. Our approach
yields an effective F1 score of 0.729 on the public EgoCart dataset, coupled
with a notably high median score in the line placement metric. We illustrate
that our method offers a straightforward means of assessing the quality of the
camera's field of view, achieving a classification accuracy of 83.8\%. This
metric can serve as a proxy for evaluating the potential performance of video
and image quality applications.</div><div><a href='http://arxiv.org/abs/2401.09515v1'>2401.09515v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10886v1")'>SCENES: Subpixel Correspondence Estimation With Epipolar Supervision</div>
<div id='2401.10886v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T18:57:46Z</div><div>Authors: Dominik A. Kloepfer, João F. Henriques, Dylan Campbell</div><div style='padding-top: 10px; width: 80ex'>Extracting point correspondences from two or more views of a scene is a
fundamental computer vision problem with particular importance for relative
camera pose estimation and structure-from-motion. Existing local feature
matching approaches, trained with correspondence supervision on large-scale
datasets, obtain highly-accurate matches on the test sets. However, they do not
generalise well to new datasets with different characteristics to those they
were trained on, unlike classic feature extractors. Instead, they require
finetuning, which assumes that ground-truth correspondences or ground-truth
camera poses and 3D structure are available. We relax this assumption by
removing the requirement of 3D structure, e.g., depth maps or point clouds, and
only require camera pose information, which can be obtained from odometry. We
do so by replacing correspondence losses with epipolar losses, which encourage
putative matches to lie on the associated epipolar line. While weaker than
correspondence supervision, we observe that this cue is sufficient for
finetuning existing models on new data. We then further relax the assumption of
known camera poses by using pose estimates in a novel bootstrapping approach.
We evaluate on highly challenging datasets, including an indoor drone dataset
and an outdoor smartphone camera dataset, and obtain state-of-the-art results
without strong supervision.</div><div><a href='http://arxiv.org/abs/2401.10886v1'>2401.10886v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12959v1")'>WHAC: World-grounded Humans and Cameras</div>
<div id='2403.12959v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T17:58:02Z</div><div>Authors: Wanqi Yin, Zhongang Cai, Ruisi Wang, Fanzhou Wang, Chen Wei, Haiyi Mei, Weiye Xiao, Zhitao Yang, Qingping Sun, Atsushi Yamashita, Ziwei Liu, Lei Yang</div><div style='padding-top: 10px; width: 80ex'>Estimating human and camera trajectories with accurate scale in the world
coordinate system from a monocular video is a highly desirable yet challenging
and ill-posed problem. In this study, we aim to recover expressive parametric
human models (i.e., SMPL-X) and corresponding camera poses jointly, by
leveraging the synergy between three critical players: the world, the human,
and the camera. Our approach is founded on two key observations. Firstly,
camera-frame SMPL-X estimation methods readily recover absolute human depth.
Secondly, human motions inherently provide absolute spatial cues. By
integrating these insights, we introduce a novel framework, referred to as
WHAC, to facilitate world-grounded expressive human pose and shape estimation
(EHPS) alongside camera pose estimation, without relying on traditional
optimization techniques. Additionally, we present a new synthetic dataset,
WHAC-A-Mole, which includes accurately annotated humans and cameras, and
features diverse interactive human motions as well as realistic camera
trajectories. Extensive experiments on both standard and newly established
benchmarks highlight the superiority and efficacy of our framework. We will
make the code and dataset publicly available.</div><div><a href='http://arxiv.org/abs/2403.12959v1'>2403.12959v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14817v2")'>Cameras as Rays: Pose Estimation via Ray Diffusion</div>
<div id='2402.14817v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T18:59:56Z</div><div>Authors: Jason Y. Zhang, Amy Lin, Moneish Kumar, Tzu-Hsuan Yang, Deva Ramanan, Shubham Tulsiani</div><div style='padding-top: 10px; width: 80ex'>Estimating camera poses is a fundamental task for 3D reconstruction and
remains challenging given sparsely sampled views (&lt;10). In contrast to existing
approaches that pursue top-down prediction of global parametrizations of camera
extrinsics, we propose a distributed representation of camera pose that treats
a camera as a bundle of rays. This representation allows for a tight coupling
with spatial image features improving pose precision. We observe that this
representation is naturally suited for set-level transformers and develop a
regression-based approach that maps image patches to corresponding rays. To
capture the inherent uncertainties in sparse-view pose inference, we adapt this
approach to learn a denoising diffusion model which allows us to sample
plausible modes while improving performance. Our proposed methods, both
regression- and diffusion-based, demonstrate state-of-the-art performance on
camera pose estimation on CO3D while generalizing to unseen object categories
and in-the-wild captures.</div><div><a href='http://arxiv.org/abs/2402.14817v2'>2402.14817v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11036v1")'>Occlusion Resilient 3D Human Pose Estimation</div>
<div id='2402.11036v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T19:29:43Z</div><div>Authors: Soumava Kumar Roy, Ilia Badanin, Sina Honari, Pascal Fua</div><div style='padding-top: 10px; width: 80ex'>Occlusions remain one of the key challenges in 3D body pose estimation from
single-camera video sequences. Temporal consistency has been extensively used
to mitigate their impact but the existing algorithms in the literature do not
explicitly model them.
  Here, we apply this by representing the deforming body as a spatio-temporal
graph. We then introduce a refinement network that performs graph convolutions
over this graph to output 3D poses. To ensure robustness to occlusions, we
train this network with a set of binary masks that we use to disable some of
the edges as in drop-out techniques.
  In effect, we simulate the fact that some joints can be hidden for periods of
time and train the network to be immune to that. We demonstrate the
effectiveness of this approach compared to state-of-the-art techniques that
infer poses from single-camera sequences.</div><div><a href='http://arxiv.org/abs/2402.11036v1'>2402.11036v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09805v1")'>On the Utility of 3D Hand Poses for Action Recognition</div>
<div id='2403.09805v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T18:52:34Z</div><div>Authors: Md Salman Shamil, Dibyadip Chatterjee, Fadime Sener, Shugao Ma, Angela Yao</div><div style='padding-top: 10px; width: 80ex'>3D hand poses are an under-explored modality for action recognition. Poses
are compact yet informative and can greatly benefit applications with limited
compute budgets. However, poses alone offer an incomplete understanding of
actions, as they cannot fully capture objects and environments with which
humans interact. To efficiently model hand-object interactions, we propose
HandFormer, a novel multimodal transformer. HandFormer combines 3D hand poses
at a high temporal resolution for fine-grained motion modeling with sparsely
sampled RGB frames for encoding scene semantics. Observing the unique
characteristics of hand poses, we temporally factorize hand modeling and
represent each joint by its short-term trajectories. This factorized pose
representation combined with sparse RGB samples is remarkably efficient and
achieves high accuracy. Unimodal HandFormer with only hand poses outperforms
existing skeleton-based methods at 5x fewer FLOPs. With RGB, we achieve new
state-of-the-art performance on Assembly101 and H2O with significant
improvements in egocentric action recognition.</div><div><a href='http://arxiv.org/abs/2403.09805v1'>2403.09805v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08923v2")'>IMUOptimize: A Data-Driven Approach to Optimal IMU Placement for Human
  Pose Estimation with Transformer Architecture</div>
<div id='2402.08923v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T03:45:26Z</div><div>Authors: Varun Ramani, Hossein Khayami, Yang Bai, Nakul Garg, Nirupam Roy</div><div style='padding-top: 10px; width: 80ex'>This paper presents a novel approach for predicting human poses using IMU
data, diverging from previous studies such as DIP-IMU, IMUPoser, and TransPose,
which use up to 6 IMUs in conjunction with bidirectional RNNs. We introduce two
main innovations: a data-driven strategy for optimal IMU placement and a
transformer-based model architecture for time series analysis. Our findings
indicate that our approach not only outperforms traditional 6 IMU-based biRNN
models but also that the transformer architecture significantly enhances pose
reconstruction from data obtained from 24 IMU locations, with equivalent
performance to biRNNs when using only 6 IMUs. The enhanced accuracy provided by
our optimally chosen locations, when coupled with the parallelizability and
performance of transformers, provides significant improvements to the field of
IMU-based pose estimation.</div><div><a href='http://arxiv.org/abs/2402.08923v2'>2402.08923v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09871v1")'>ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric
  Thermal Image</div>
<div id='2403.09871v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T21:01:06Z</div><div>Authors: Fangqiang Ding, Yunzhou Zhu, Xiangyu Wen, Chris Xiaoxuan Lu</div><div style='padding-top: 10px; width: 80ex'>In this work, we present ThermoHands, a new benchmark for thermal image-based
egocentric 3D hand pose estimation, aimed at overcoming challenges like varying
lighting and obstructions (e.g., handwear). The benchmark includes a diverse
dataset from 28 subjects performing hand-object and hand-virtual interactions,
accurately annotated with 3D hand poses through an automated process. We
introduce a bespoken baseline method, TheFormer, utilizing dual transformer
modules for effective egocentric 3D hand pose estimation in thermal imagery.
Our experimental results highlight TheFormer's leading performance and affirm
thermal imaging's effectiveness in enabling robust 3D hand pose estimation in
adverse conditions.</div><div><a href='http://arxiv.org/abs/2403.09871v1'>2403.09871v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.15307v1")'>Representing Online Handwriting for Recognition in Large Vision-Language
  Models</div>
<div id='2402.15307v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T13:11:10Z</div><div>Authors: Anastasiia Fadeeva, Philippe Schlattner, Andrii Maksai, Mark Collier, Efi Kokiopoulou, Jesse Berent, Claudiu Musat</div><div style='padding-top: 10px; width: 80ex'>The adoption of tablets with touchscreens and styluses is increasing, and a
key feature is converting handwriting to text, enabling search, indexing, and
AI assistance. Meanwhile, vision-language models (VLMs) are now the go-to
solution for image understanding, thanks to both their state-of-the-art
performance across a variety of tasks and the simplicity of a unified approach
to training, fine-tuning, and inference. While VLMs obtain high performance on
image-based tasks, they perform poorly on handwriting recognition when applied
naively, i.e., by rendering handwriting as an image and performing optical
character recognition (OCR). In this paper, we study online handwriting
recognition with VLMs, going beyond naive OCR. We propose a novel tokenized
representation of digital ink (online handwriting) that includes both a
time-ordered sequence of strokes as text, and as image. We show that this
representation yields results comparable to or better than state-of-the-art
online handwriting recognizers. Wide applicability is shown through results
with two different VLM families, on multiple public datasets. Our approach can
be applied to off-the-shelf VLMs, does not require any changes in their
architecture, and can be used in both fine-tuning and parameter-efficient
tuning. We perform a detailed ablation study to identify the key elements of
the proposed representation.</div><div><a href='http://arxiv.org/abs/2402.15307v1'>2402.15307v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17098v1")'>CharNet: Generalized Approach for High-Complexity Character
  Classification</div>
<div id='2401.17098v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T15:29:32Z</div><div>Authors: Boris Kriuk</div><div style='padding-top: 10px; width: 80ex'>Handwritten character recognition (HCR) is a challenging problem for machine
learning researchers. Unlike printed text data, handwritten character datasets
have more variation due to human-introduced bias. With numerous unique
character classes present, some data, such as Logographic Scripts or
Sino-Korean character sequences, bring new complications to the HCR problem.
The classification task on such datasets requires the model to learn
high-complexity details of the images that share similar features. With recent
advances in computational resource availability and further computer vision
theory development, some research teams have effectively addressed the arising
challenges. Although known for achieving high efficiency, many common
approaches are still not generalizable and use dataset-specific solutions to
achieve better results. Due to complex structure and high computing demands,
existing methods frequently prevent the solutions from gaining popularity. This
paper proposes a straightforward, generalizable, and highly effective approach
(CharNet) for detailed character image classification and compares its
performance to that of existing approaches.</div><div><a href='http://arxiv.org/abs/2401.17098v1'>2401.17098v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08035v2")'>BanglaNet: Bangla Handwritten Character Recognition using Ensembling of
  Convolutional Neural Network</div>
<div id='2401.08035v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T01:08:19Z</div><div>Authors: Chandrika Saha, Md Mostafijur Rahman</div><div style='padding-top: 10px; width: 80ex'>Handwritten character recognition is a crucial task because of its abundant
applications. The recognition task of Bangla handwritten characters is
especially challenging because of the cursive nature of Bangla characters and
the presence of compound characters with more than one way of writing. In this
paper, a classification model based on the ensembling of several Convolutional
Neural Networks (CNN), namely, BanglaNet is proposed to classify Bangla basic
characters, compound characters, numerals, and modifiers. Three different
models based on the idea of state-of-the-art CNN models like Inception, ResNet,
and DenseNet have been trained with both augmented and non-augmented inputs.
Finally, all these models are averaged or ensembled to get the finishing model.
Rigorous experimentation on three benchmark Bangla handwritten characters
datasets, namely, CMATERdb, BanglaLekha-Isolated, and Ekush has exhibited
significant recognition accuracies compared to some recent CNN-based research.
The top-1 recognition accuracies obtained are 98.40%, 97.65%, and 97.32%, and
the top-3 accuracies are 99.79%, 99.74%, and 99.56% for CMATERdb,
BanglaLekha-Isolated, and Ekush datasets respectively.</div><div><a href='http://arxiv.org/abs/2401.08035v2'>2401.08035v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.03085v1")'>Consensus-Threshold Criterion for Offline Signature Verification using
  Convolutional Neural Network Learned Representations</div>
<div id='2401.03085v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T23:10:26Z</div><div>Authors: Paul Brimoh, Chollette C. Olisah</div><div style='padding-top: 10px; width: 80ex'>A genuine signer's signature is naturally unstable even at short
time-intervals whereas, expert forgers always try to perfectly mimic a genuine
signer's signature. This presents a challenge which puts a genuine signer at
risk of being denied access, while a forge signer is granted access. The
implication is a high false acceptance rate (FAR) which is the percentage of
forge signature classified as belonging to a genuine class. Existing work have
only scratched the surface of signature verification because the
misclassification error remains high. In this paper, a consensus-threshold
distance-based classifier criterion is proposed for offline writer-dependent
signature verification. Using features extracted from SigNet and SigNet-F deep
convolutional neural network models, the proposed classifier minimizes FAR.
This is demonstrated via experiments on four datasets: GPDS-300, MCYT, CEDAR
and Brazilian PUC-PR datasets. On GPDS-300, the consensus threshold classifier
improves the state-of-the-art performance by achieving a 1.27% FAR compared to
8.73% and 17.31% recorded in literature. This performance is consistent across
other datasets and guarantees that the risk of imposters gaining access to
sensitive documents or transactions is minimal.</div><div><a href='http://arxiv.org/abs/2401.03085v1'>2401.03085v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.09252v1")'>3D Scene Geometry Estimation from 360$^\circ$ Imagery: A Survey</div>
<div id='2401.09252v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T14:57:27Z</div><div>Authors: Thiago Lopes Trugillo da Silveira, Paulo Gamarra Lessa Pinto, Jeffri Erwin Murrugarra Llerena, Claudio Rosito Jung</div><div style='padding-top: 10px; width: 80ex'>This paper provides a comprehensive survey on pioneer and state-of-the-art 3D
scene geometry estimation methodologies based on single, two, or multiple
images captured under the omnidirectional optics. We first revisit the basic
concepts of the spherical camera model, and review the most common acquisition
technologies and representation formats suitable for omnidirectional (also
called 360$^\circ$, spherical or panoramic) images and videos. We then survey
monocular layout and depth inference approaches, highlighting the recent
advances in learning-based solutions suited for spherical data. The classical
stereo matching is then revised on the spherical domain, where methodologies
for detecting and describing sparse and dense features become crucial. The
stereo matching concepts are then extrapolated for multiple view camera setups,
categorizing them among light fields, multi-view stereo, and structure from
motion (or visual simultaneous localization and mapping). We also compile and
discuss commonly adopted datasets and figures of merit indicated for each
purpose and list recent results for completeness. We conclude this paper by
pointing out current and future trends.</div><div><a href='http://arxiv.org/abs/2401.09252v1'>2401.09252v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07284v1")'>CLIPPER: Robust Data Association without an Initial Guess</div>
<div id='2402.07284v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T19:16:01Z</div><div>Authors: Parker C. Lusk, Jonathan P. How</div><div style='padding-top: 10px; width: 80ex'>Identifying correspondences in noisy data is a critically important step in
estimation processes. When an informative initial estimation guess is
available, the data association challenge is less acute; however, the existence
of a high-quality initial guess is rare in most contexts. We explore
graph-theoretic formulations for data association, which do not require an
initial estimation guess. Existing graph-theoretic approaches optimize over
unweighted graphs, discarding important consistency information encoded in
weighted edges, and frequently attempt to solve NP-hard problems exactly. In
contrast, we formulate a new optimization problem that fully leverages weighted
graphs and seeks the densest edge-weighted clique. We introduce two relaxations
to this problem: a convex semidefinite relaxation which we find to be
empirically tight, and a fast first-order algorithm called CLIPPER which
frequently arrives at nearly-optimal solutions in milliseconds. When evaluated
on point cloud registration problems, our algorithms remain robust up to at
least 95% outliers while existing algorithms begin breaking down at 80%
outliers. Code is available at https://mit-acl.github.io/clipper.</div><div><a href='http://arxiv.org/abs/2402.07284v1'>2402.07284v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15058v1")'>Mixup Barcodes: Quantifying Geometric-Topological Interactions between
  Point Clouds</div>
<div id='2402.15058v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T02:19:26Z</div><div>Authors: Hubert Wagner, Nickolas Arustamyan, Matthew Wheeler, Peter Bubenik</div><div style='padding-top: 10px; width: 80ex'>We combine standard persistent homology with image persistent homology to
define a novel way of characterizing shapes and interactions between them. In
particular, we introduce: (1) a mixup barcode, which captures
geometric-topological interactions (mixup) between two point sets in arbitrary
dimension; (2) simple summary statistics, total mixup and total percentage
mixup, which quantify the complexity of the interactions as a single number;
(3) a software tool for playing with the above.
  As a proof of concept, we apply this tool to a problem arising from machine
learning. In particular, we study the disentanglement in embeddings of
different classes. The results suggest that topological mixup is a useful
method for characterizing interactions for low and high-dimensional data.
Compared to the typical usage of persistent homology, the new tool is sensitive
to the geometric locations of the topological features, which is often
desirable.</div><div><a href='http://arxiv.org/abs/2402.15058v1'>2402.15058v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.01430v1")'>On Diffusion Process in SE(3)-invariant Space</div>
<div id='2403.01430v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-03T07:56:55Z</div><div>Authors: Zihan Zhou, Ruiying Liu, Jiachen Zheng, Xiaoxue Wang, Tianshu Yu</div><div style='padding-top: 10px; width: 80ex'>Sampling viable 3D structures (e.g., molecules and point clouds) with
SE(3)-invariance using diffusion-based models proved promising in a variety of
real-world applications, wherein SE(3)-invariant properties can be naturally
characterized by the inter-point distance manifold. However, due to the
non-trivial geometry, we still lack a comprehensive understanding of the
diffusion mechanism within such SE(3)-invariant space. This study addresses
this gap by mathematically delineating the diffusion mechanism under
SE(3)-invariance, via zooming into the interaction behavior between coordinates
and the inter-point distance manifold through the lens of differential
geometry. Upon this analysis, we propose accurate and projection-free diffusion
SDE and ODE accordingly. Such formulations enable enhancing the performance and
the speed of generation pathways; meanwhile offering valuable insights into
other systems incorporating SE(3)-invariance.</div><div><a href='http://arxiv.org/abs/2403.01430v1'>2403.01430v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03287v1")'>A Lennard-Jones Layer for Distribution Normalization</div>
<div id='2402.03287v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T18:43:05Z</div><div>Authors: Mulun Na, Jonathan Klein, Biao Zhang, Wojtek Pałubicki, Sören Pirk, Dominik L. Michels</div><div style='padding-top: 10px; width: 80ex'>We introduce the Lennard-Jones layer (LJL) for the equalization of the
density of 2D and 3D point clouds through systematically rearranging points
without destroying their overall structure (distribution normalization). LJL
simulates a dissipative process of repulsive and weakly attractive interactions
between individual points by considering the nearest neighbor of each point at
a given moment in time. This pushes the particles into a potential valley,
reaching a well-defined stable configuration that approximates an equidistant
sampling after the stabilization process. We apply LJLs to redistribute
randomly generated point clouds into a randomized uniform distribution.
Moreover, LJLs are embedded in the generation process of point cloud networks
by adding them at later stages of the inference process. The improvements in 3D
point cloud generation utilizing LJLs are evaluated qualitatively and
quantitatively. Finally, we apply LJLs to improve the point distribution of a
score-based 3D point cloud denoising network. In general, we demonstrate that
LJLs are effective for distribution normalization which can be applied at
negligible cost without retraining the given neural network.</div><div><a href='http://arxiv.org/abs/2402.03287v1'>2402.03287v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14845v1")'>Adaptive Point Transformer</div>
<div id='2401.14845v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T13:24:45Z</div><div>Authors: Alessandro Baiocchi, Indro Spinelli, Alessandro Nicolosi, Simone Scardapane</div><div style='padding-top: 10px; width: 80ex'>The recent surge in 3D data acquisition has spurred the development of
geometric deep learning models for point cloud processing, boosted by the
remarkable success of transformers in natural language processing. While point
cloud transformers (PTs) have achieved impressive results recently, their
quadratic scaling with respect to the point cloud size poses a significant
scalability challenge for real-world applications. To address this issue, we
propose the Adaptive Point Cloud Transformer (AdaPT), a standard PT model
augmented by an adaptive token selection mechanism. AdaPT dynamically reduces
the number of tokens during inference, enabling efficient processing of large
point clouds. Furthermore, we introduce a budget mechanism to flexibly adjust
the computational cost of the model at inference time without the need for
retraining or fine-tuning separate models. Our extensive experimental
evaluation on point cloud classification tasks demonstrates that AdaPT
significantly reduces computational complexity while maintaining competitive
accuracy compared to standard PTs. The code for AdaPT is made publicly
available.</div><div><a href='http://arxiv.org/abs/2401.14845v1'>2401.14845v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12535v1")'>Locality-Sensitive Hashing-Based Efficient Point Transformer with
  Applications in High-Energy Physics</div>
<div id='2402.12535v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T20:48:09Z</div><div>Authors: Siqi Miao, Zhiyuan Lu, Mia Liu, Javier Duarte, Pan Li</div><div style='padding-top: 10px; width: 80ex'>This study introduces a novel transformer model optimized for large-scale
point cloud processing in scientific domains such as high-energy physics (HEP)
and astrophysics. Addressing the limitations of graph neural networks and
standard transformers, our model integrates local inductive bias and achieves
near-linear complexity with hardware-friendly regular operations. One
contribution of this work is the quantitative analysis of the error-complexity
tradeoff of various sparsification techniques for building efficient
transformers. Our findings highlight the superiority of using
locality-sensitive hashing (LSH), especially OR \&amp; AND-construction LSH, in
kernel approximation for large-scale point cloud data with local inductive
bias. Based on this finding, we propose LSH-based Efficient Point Transformer
(\textbf{HEPT}), which combines E$^2$LSH with OR \&amp; AND constructions and is
built upon regular computations. HEPT demonstrates remarkable performance in
two critical yet time-consuming HEP tasks, significantly outperforming existing
GNNs and transformers in accuracy and computational speed, marking a
significant advancement in geometric deep learning and large-scale scientific
data processing. Our code is available at
\url{https://github.com/Graph-COM/HEPT}.</div><div><a href='http://arxiv.org/abs/2402.12535v1'>2402.12535v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07710v1")'>Optimization of Sparse Convolution for 3D-Point Cloud on GPUs with CUDA</div>
<div id='2402.07710v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T15:23:19Z</div><div>Authors: Chester Luo, Kevin Lai</div><div style='padding-top: 10px; width: 80ex'>In recent years, there has been a significant increase in the utilization of
deep learning methods, particularly convolutional neural networks (CNNs), which
have emerged as the dominant approach in various domains that involve
structured grid data, such as picture analysis and processing. Nevertheless,
the exponential growth in the utilization of LiDAR and 3D sensors across many
domains has resulted in an increased need for the analysis of 3D point clouds.
The utilization of 3D point clouds is crucial in various applications,
including object recognition and segmentation, as they offer a spatial
depiction of things within a three-dimensional environment. In contrast to
photos, point clouds exhibit sparsity and lack a regular grid, hence posing
distinct processing and computational issues.</div><div><a href='http://arxiv.org/abs/2402.07710v1'>2402.07710v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01460v1")'>Point Cloud Classification via Deep Set Linearized Optimal Transport</div>
<div id='2401.01460v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T23:26:33Z</div><div>Authors: Scott Mahan, Caroline Moosmüller, Alexander Cloninger</div><div style='padding-top: 10px; width: 80ex'>We introduce Deep Set Linearized Optimal Transport, an algorithm designed for
the efficient simultaneous embedding of point clouds into an $L^2-$space. This
embedding preserves specific low-dimensional structures within the Wasserstein
space while constructing a classifier to distinguish between various classes of
point clouds. Our approach is motivated by the observation that $L^2-$distances
between optimal transport maps for distinct point clouds, originating from a
shared fixed reference distribution, provide an approximation of the
Wasserstein-2 distance between these point clouds, under certain assumptions.
To learn approximations of these transport maps, we employ input convex neural
networks (ICNNs) and establish that, under specific conditions, Euclidean
distances between samples from these ICNNs closely mirror Wasserstein-2
distances between the true distributions. Additionally, we train a
discriminator network that attaches weights these samples and creates a
permutation invariant classifier to differentiate between different classes of
point clouds. We showcase the advantages of our algorithm over the standard
deep set approach through experiments on a flow cytometry dataset with a
limited number of labeled point clouds.</div><div><a href='http://arxiv.org/abs/2401.01460v1'>2401.01460v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02345v1")'>Stereographic Spherical Sliced Wasserstein Distances</div>
<div id='2402.02345v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T05:03:06Z</div><div>Authors: Huy Tran, Yikun Bai, Abihith Kothapalli, Ashkan Shahbazi, Xinran Liu, Rocio Diaz Martin, Soheil Kolouri</div><div style='padding-top: 10px; width: 80ex'>Comparing spherical probability distributions is of great interest in various
fields, including geology, medical domains, computer vision, and deep
representation learning. The utility of optimal transport-based distances, such
as the Wasserstein distance, for comparing probability measures has spurred
active research in developing computationally efficient variations of these
distances for spherical probability measures. This paper introduces a
high-speed and highly parallelizable distance for comparing spherical measures
using the stereographic projection and the generalized Radon transform, which
we refer to as the Stereographic Spherical Sliced Wasserstein (S3W) distance.
We carefully address the distance distortion caused by the stereographic
projection and provide an extensive theoretical analysis of our proposed metric
and its rotationally invariant variation. Finally, we evaluate the performance
of the proposed metrics and compare them with recent baselines in terms of both
speed and accuracy through a wide range of numerical studies, including
gradient flows and self-supervised learning.</div><div><a href='http://arxiv.org/abs/2402.02345v1'>2402.02345v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.07706v2")'>Fast and Simple Explainability for Point Cloud Networks</div>
<div id='2403.07706v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T14:51:23Z</div><div>Authors: Meir Yossef Levi, Guy Gilboa</div><div style='padding-top: 10px; width: 80ex'>We propose a fast and simple explainable AI (XAI) method for point cloud
data. It computes pointwise importance with respect to a trained network
downstream task. This allows better understanding of the network properties,
which is imperative for safety-critical applications. In addition to debugging
and visualization, our low computational complexity facilitates online feedback
to the network at inference. This can be used to reduce uncertainty and to
increase robustness. In this work, we introduce \emph{Feature Based
Interpretability} (FBI), where we compute the features' norm, per point, before
the bottleneck. We analyze the use of gradients and post- and pre-bottleneck
strategies, showing pre-bottleneck is preferred, in terms of smoothness and
ranking. We obtain at least three orders of magnitude speedup, compared to
current XAI methods, thus, scalable for big point clouds or large-scale
architectures. Our approach achieves SOTA results, in terms of classification
explainability. We demonstrate how the proposed measure is helpful in analyzing
and characterizing various aspects of 3D learning, such as rotation invariance,
robustness to out-of-distribution (OOD) outliers or domain shift and dataset
bias.</div><div><a href='http://arxiv.org/abs/2403.07706v2'>2403.07706v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16077v1")'>Equivariant Frames and the Impossibility of Continuous Canonicalization</div>
<div id='2402.16077v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-25T12:40:42Z</div><div>Authors: Nadav Dym, Hannah Lawrence, Jonathan W. Siegel</div><div style='padding-top: 10px; width: 80ex'>Canonicalization provides an architecture-agnostic method for enforcing
equivariance, with generalizations such as frame-averaging recently gaining
prominence as a lightweight and flexible alternative to equivariant
architectures. Recent works have found an empirical benefit to using
probabilistic frames instead, which learn weighted distributions over group
elements. In this work, we provide strong theoretical justification for this
phenomenon: for commonly-used groups, there is no efficiently computable choice
of frame that preserves continuity of the function being averaged. In other
words, unweighted frame-averaging can turn a smooth, non-symmetric function
into a discontinuous, symmetric function. To address this fundamental
robustness problem, we formally define and construct \emph{weighted} frames,
which provably preserve continuity, and demonstrate their utility by
constructing efficient and continuous weighted frames for the actions of
$SO(2)$, $SO(3)$, and $S_n$ on point clouds.</div><div><a href='http://arxiv.org/abs/2402.16077v1'>2402.16077v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02484v1")'>Weisfeiler Leman for Euclidean Equivariant Machine Learning</div>
<div id='2402.02484v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T13:25:18Z</div><div>Authors: Snir Hordan, Tal Amir, Nadav Dym</div><div style='padding-top: 10px; width: 80ex'>The $k$-Weifeiler-Leman ($k$-WL) graph isomorphism test hierarchy is a common
method for assessing the expressive power of graph neural networks (GNNs).
Recently, the $2$-WL test was proven to be complete on weighted graphs which
encode $3\mathrm{D}$ point cloud data. Consequently, GNNs whose expressive
power is equivalent to the $2$-WL test are provably universal on point clouds.
Yet, this result is limited to invariant continuous functions on point clouds.
  In this paper we extend this result in three ways: Firstly, we show that
$2$-WL tests can be extended to point clouds which include both positions and
velocity, a scenario often encountered in applications. Secondly, we show that
PPGN (Maron et al., 2019) can simulate $2$-WL uniformly on all point clouds
with low complexity. Finally, we show that a simple modification of this PPGN
architecture can be used to obtain a universal equivariant architecture that
can approximate all continuous equivariant functions uniformly.
  Building on our results, we develop our WeLNet architecture, which can
process position-velocity pairs, compute functions fully equivariant to
permutations and rigid motions, and is provably complete and universal.
Remarkably, WeLNet is provably complete precisely in the setting in which it is
implemented in practice. Our theoretical results are complemented by
experiments showing WeLNet sets new state-of-the-art results on the N-Body
dynamics task and the GEOM-QM9 molecular conformation generation task.</div><div><a href='http://arxiv.org/abs/2402.02484v1'>2402.02484v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08529v1")'>Approximately Piecewise E(3) Equivariant Point Networks</div>
<div id='2402.08529v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T15:34:39Z</div><div>Authors: Matan Atzmon, Jiahui Huang, Francis Williams, Or Litany</div><div style='padding-top: 10px; width: 80ex'>Integrating a notion of symmetry into point cloud neural networks is a
provably effective way to improve their generalization capability. Of
particular interest are $E(3)$ equivariant point cloud networks where Euclidean
transformations applied to the inputs are preserved in the outputs. Recent
efforts aim to extend networks that are $E(3)$ equivariant, to accommodate
inputs made of multiple parts, each of which exhibits local $E(3)$ symmetry. In
practical settings, however, the partitioning into individually transforming
regions is unknown a priori. Errors in the partition prediction would
unavoidably map to errors in respecting the true input symmetry. Past works
have proposed different ways to predict the partition, which may exhibit
uncontrolled errors in their ability to maintain equivariance to the actual
partition. To this end, we introduce APEN: a general framework for constructing
approximate piecewise-$E(3)$ equivariant point networks. Our primary insight is
that functions that are equivariant with respect to a finer partition will also
maintain equivariance in relation to the true partition. Leveraging this
observation, we propose a design where the equivariance approximation error at
each layers can be bounded solely in terms of (i) uncertainty quantification of
the partition prediction, and (ii) bounds on the probability of failing to
suggest a proper subpartition of the ground truth one. We demonstrate the
effectiveness of APEN using two data types exemplifying part-based symmetry:
(i) real-world scans of room scenes containing multiple furniture-type objects;
and, (ii) human motions, characterized by articulated parts exhibiting rigid
movement. Our empirical results demonstrate the advantage of integrating
piecewise $E(3)$ symmetry into network design, showing a distinct improvement
in generalization compared to prior works for both classification and
segmentation tasks.</div><div><a href='http://arxiv.org/abs/2402.08529v1'>2402.08529v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.05666v1")'>Prepared for the Worst: A Learning-Based Adversarial Attack for
  Resilience Analysis of the ICP Algorithm</div>
<div id='2403.05666v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T20:43:57Z</div><div>Authors: Ziyu Zhang, Johann Laconte, Daniil Lisus, Timothy D. Barfoot</div><div style='padding-top: 10px; width: 80ex'>This paper presents a novel method to assess the resilience of the Iterative
Closest Point (ICP) algorithm via deep-learning-based attacks on lidar point
clouds. For safety-critical applications such as autonomous navigation,
ensuring the resilience of algorithms prior to deployments is of utmost
importance. The ICP algorithm has become the standard for lidar-based
localization. However, the pose estimate it produces can be greatly affected by
corruption in the measurements. Corruption can arise from a variety of
scenarios such as occlusions, adverse weather, or mechanical issues in the
sensor. Unfortunately, the complex and iterative nature of ICP makes assessing
its resilience to corruption challenging. While there have been efforts to
create challenging datasets and develop simulations to evaluate the resilience
of ICP empirically, our method focuses on finding the maximum possible ICP pose
error using perturbation-based adversarial attacks. The proposed attack induces
significant pose errors on ICP and outperforms baselines more than 88% of the
time across a wide range of scenarios. As an example application, we
demonstrate that our attack can be used to identify areas on a map where ICP is
particularly vulnerable to corruption in the measurements.</div><div><a href='http://arxiv.org/abs/2403.05666v1'>2403.05666v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.08882v1")'>Moving Object Proposals with Deep Learned Optical Flow for Video Object
  Segmentation</div>
<div id='2402.08882v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T01:13:55Z</div><div>Authors: Ge Shi, Zhili Yang</div><div style='padding-top: 10px; width: 80ex'>Dynamic scene understanding is one of the most conspicuous field of interest
among computer vision community. In order to enhance dynamic scene
understanding, pixel-wise segmentation with neural networks is widely accepted.
The latest researches on pixel-wise segmentation combined semantic and motion
information and produced good performance. In this work, we propose a state of
art architecture of neural networks to accurately and efficiently get the
moving object proposals (MOP). We first train an unsupervised convolutional
neural network (UnFlow) to generate optical flow estimation. Then we render the
output of optical flow net to a fully convolutional SegNet model. The main
contribution of our work is (1) Fine-tuning the pretrained optical flow model
on the brand new DAVIS Dataset; (2) Leveraging fully convolutional neural
networks with Encoder-Decoder architecture to segment objects. We developed the
codes with TensorFlow, and executed the training and evaluation processes on an
AWS EC2 instance.</div><div><a href='http://arxiv.org/abs/2402.08882v1'>2402.08882v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.02956v1")'>AdaTreeFormer: Few Shot Domain Adaptation for Tree Counting from a
  Single High-Resolution Image</div>
<div id='2402.02956v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T12:34:03Z</div><div>Authors: Hamed Amini Amirkolaee, Miaojing Shi, Lianghua He, Mark Mulligan</div><div style='padding-top: 10px; width: 80ex'>The process of estimating and counting tree density using only a single
aerial or satellite image is a difficult task in the fields of photogrammetry
and remote sensing. However, it plays a crucial role in the management of
forests. The huge variety of trees in varied topography severely hinders tree
counting models to perform well. The purpose of this paper is to propose a
framework that is learnt from the source domain with sufficient labeled trees
and is adapted to the target domain with only a limited number of labeled
trees. Our method, termed as AdaTreeFormer, contains one shared encoder with a
hierarchical feature extraction scheme to extract robust features from the
source and target domains. It also consists of three subnets: two for
extracting self-domain attention maps from source and target domains
respectively and one for extracting cross-domain attention maps. For the
latter, an attention-to-adapt mechanism is introduced to distill relevant
information from different domains while generating tree density maps; a
hierarchical cross-domain feature alignment scheme is proposed that
progressively aligns the features from the source and target domains. We also
adopt adversarial learning into the framework to further reduce the gap between
source and target domains. Our AdaTreeFormer is evaluated on six designed
domain adaptation tasks using three tree counting datasets, ie Jiangsu,
Yosemite, and London; and outperforms the state of the art methods
significantly.</div><div><a href='http://arxiv.org/abs/2402.02956v1'>2402.02956v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.14183v1")'>OTSeg: Multi-prompt Sinkhorn Attention for Zero-Shot Semantic
  Segmentation</div>
<div id='2403.14183v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T07:15:37Z</div><div>Authors: Kwanyoung Kim, Yujin Oh, Jong Chul Ye</div><div style='padding-top: 10px; width: 80ex'>The recent success of CLIP has demonstrated promising results in zero-shot
semantic segmentation by transferring muiltimodal knowledge to pixel-level
classification. However, leveraging pre-trained CLIP knowledge to closely align
text embeddings with pixel embeddings still has limitations in existing
approaches. To address this issue, we propose OTSeg, a novel multimodal
attention mechanism aimed at enhancing the potential of multiple text prompts
for matching associated pixel embeddings. We first propose Multi-Prompts
Sinkhorn (MPS) based on the Optimal Transport (OT) algorithm, which leads
multiple text prompts to selectively focus on various semantic features within
image pixels. Moreover, inspired by the success of Sinkformers in unimodal
settings, we introduce the extension of MPS, called Multi-Prompts Sinkhorn
Attention (MPSA), which effectively replaces cross-attention mechanisms within
Transformer framework in multimodal settings. Through extensive experiments, we
demonstrate that OTSeg achieves state-of-the-art (SOTA) performance with
significant gains on Zero-Shot Semantic Segmentation (ZS3) tasks across three
benchmark datasets.</div><div><a href='http://arxiv.org/abs/2403.14183v1'>2403.14183v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02946v1")'>HoughToRadon Transform: New Neural Network Layer for Features
  Improvement in Projection Space</div>
<div id='2402.02946v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T12:19:16Z</div><div>Authors: Alexandra Zhabitskaya, Alexander Sheshkus, Vladimir L. Arlazarov</div><div style='padding-top: 10px; width: 80ex'>In this paper, we introduce HoughToRadon Transform layer, a novel layer
designed to improve the speed of neural networks incorporated with Hough
Transform to solve semantic image segmentation problems. By placing it after a
Hough Transform layer, "inner" convolutions receive modified feature maps with
new beneficial properties, such as a smaller area of processed images and
parameter space linearity by angle and shift. These properties were not
presented in Hough Transform alone. Furthermore, HoughToRadon Transform layer
allows us to adjust the size of intermediate feature maps using two new
parameters, thus allowing us to balance the speed and quality of the resulting
neural network. Our experiments on the open MIDV-500 dataset show that this new
approach leads to time savings in document segmentation tasks and achieves
state-of-the-art 97.7% accuracy, outperforming HoughEncoder with larger
computational complexity.</div><div><a href='http://arxiv.org/abs/2402.02946v1'>2402.02946v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.12029v1")'>Align and Distill: Unifying and Improving Domain Adaptive Object
  Detection</div>
<div id='2403.12029v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T17:58:02Z</div><div>Authors: Justin Kay, Timm Haucke, Suzanne Stathatos, Siqi Deng, Erik Young, Pietro Perona, Sara Beery, Grant Van Horn</div><div style='padding-top: 10px; width: 80ex'>Object detectors often perform poorly on data that differs from their
training set. Domain adaptive object detection (DAOD) methods have recently
demonstrated strong results on addressing this challenge. Unfortunately, we
identify systemic benchmarking pitfalls that call past results into question
and hamper further progress: (a) Overestimation of performance due to
underpowered baselines, (b) Inconsistent implementation practices preventing
transparent comparisons of methods, and (c) Lack of generality due to outdated
backbones and lack of diversity in benchmarks. We address these problems by
introducing: (1) A unified benchmarking and implementation framework, Align and
Distill (ALDI), enabling comparison of DAOD methods and supporting future
development, (2) A fair and modern training and evaluation protocol for DAOD
that addresses benchmarking pitfalls, (3) A new DAOD benchmark dataset,
CFC-DAOD, enabling evaluation on diverse real-world data, and (4) A new method,
ALDI++, that achieves state-of-the-art results by a large margin. ALDI++
outperforms the previous state-of-the-art by +3.5 AP50 on Cityscapes to Foggy
Cityscapes, +5.7 AP50 on Sim10k to Cityscapes (where ours is the only method to
outperform a fair baseline), and +2.0 AP50 on CFC Kenai to Channel. Our
framework, dataset, and state-of-the-art method offer a critical reset for DAOD
and provide a strong foundation for future research. Code and data are
available: https://github.com/justinkay/aldi and
https://github.com/visipedia/caltech-fish-counting.</div><div><a href='http://arxiv.org/abs/2403.12029v1'>2403.12029v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17972v1")'>MelNet: A Real-Time Deep Learning Algorithm for Object Detection</div>
<div id='2401.17972v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T16:27:47Z</div><div>Authors: Yashar Azadvatan, Murat Kurt</div><div style='padding-top: 10px; width: 80ex'>In this study, a novel deep learning algorithm for object detection, named
MelNet, was introduced. MelNet underwent training utilizing the KITTI dataset
for object detection. Following 300 training epochs, MelNet attained an mAP
(mean average precision) score of 0.732. Additionally, three alternative models
-YOLOv5, EfficientDet, and Faster-RCNN-MobileNetv3- were trained on the KITTI
dataset and juxtaposed with MelNet for object detection.
  The outcomes underscore the efficacy of employing transfer learning in
certain instances. Notably, preexisting models trained on prominent datasets
(e.g., ImageNet, COCO, and Pascal VOC) yield superior results. Another finding
underscores the viability of creating a new model tailored to a specific
scenario and training it on a specific dataset. This investigation demonstrates
that training MelNet exclusively on the KITTI dataset also surpasses
EfficientDet after 150 epochs. Consequently, post-training, MelNet's
performance closely aligns with that of other pre-trained models.</div><div><a href='http://arxiv.org/abs/2401.17972v1'>2401.17972v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.04666v1")'>Benchmark Analysis of Various Pre-trained Deep Learning Models on ASSIRA
  Cats and Dogs Dataset</div>
<div id='2401.04666v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-09T16:48:11Z</div><div>Authors: Galib Muhammad Shahriar Himel, Md. Masudul Islam</div><div style='padding-top: 10px; width: 80ex'>As the most basic application and implementation of deep learning, image
classification has grown in popularity. Various datasets are provided by
renowned data science communities for benchmarking machine learning algorithms
and pre-trained models. The ASSIRA Cats &amp; Dogs dataset is one of them and is
being used in this research for its overall acceptance and benchmark standards.
A comparison of various pre-trained models is demonstrated by using different
types of optimizers and loss functions. Hyper-parameters are changed to gain
the best result from a model. By applying this approach, we have got higher
accuracy without major changes in the training model. To run the experiment, we
used three different computer architectures: a laptop equipped with NVIDIA
GeForce GTX 1070, a laptop equipped with NVIDIA GeForce RTX 3080Ti, and a
desktop equipped with NVIDIA GeForce RTX 3090. The acquired results demonstrate
supremacy in terms of accuracy over the previously done experiments on this
dataset. From this experiment, the highest accuracy which is 99.65% is gained
using the NASNet Large.</div><div><a href='http://arxiv.org/abs/2401.04666v1'>2401.04666v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.07113v1")'>Class Imbalance in Object Detection: An Experimental Diagnosis and Study
  of Mitigation Strategies</div>
<div id='2403.07113v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T19:06:04Z</div><div>Authors: Nieves Crasto</div><div style='padding-top: 10px; width: 80ex'>Object detection, a pivotal task in computer vision, is frequently hindered
by dataset imbalances, particularly the under-explored issue of
foreground-foreground class imbalance. This lack of attention to
foreground-foreground class imbalance becomes even more pronounced in the
context of single-stage detectors. This study introduces a benchmarking
framework utilizing the YOLOv5 single-stage detector to address the problem of
foreground-foreground class imbalance. We crafted a novel 10-class long-tailed
dataset from the COCO dataset, termed COCO-ZIPF, tailored to reflect common
real-world detection scenarios with a limited number of object classes. Against
this backdrop, we scrutinized three established techniques: sampling, loss
weighing, and data augmentation. Our comparative analysis reveals that sampling
and loss reweighing methods, while shown to be beneficial in two-stage detector
settings, do not translate as effectively in improving YOLOv5's performance on
the COCO-ZIPF dataset. On the other hand, data augmentation methods,
specifically mosaic and mixup, significantly enhance the model's mean Average
Precision (mAP), by introducing more variability and complexity into the
training data. (Code available:
https://github.com/craston/object_detection_cib)</div><div><a href='http://arxiv.org/abs/2403.07113v1'>2403.07113v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12729v2")'>Enhancing Object Detection Performance for Small Objects through
  Synthetic Data Generation and Proportional Class-Balancing Technique: A
  Comparative Study in Industrial Scenarios</div>
<div id='2401.12729v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T13:02:11Z</div><div>Authors: Jibinraj Antony, Vinit Hegiste, Ali Nazeri, Hooman Tavakoli, Snehal Walunj, Christiane Plociennik, Martin Ruskowski</div><div style='padding-top: 10px; width: 80ex'>Object Detection (OD) has proven to be a significant computer vision method
in extracting localized class information and has multiple applications in the
industry. Although many of the state-of-the-art (SOTA) OD models perform well
on medium and large sized objects, they seem to under perform on small objects.
In most of the industrial use cases, it is difficult to collect and annotate
data for small objects, as it is time-consuming and prone to human errors.
Additionally, those datasets are likely to be unbalanced and often result in an
inefficient model convergence. To tackle this challenge, this study presents a
novel approach that injects additional data points to improve the performance
of the OD models. Using synthetic data generation, the difficulties in data
collection and annotations for small object data points can be minimized and to
create a dataset with balanced distribution. This paper discusses the effects
of a simple proportional class-balancing technique, to enable better anchor
matching of the OD models. A comparison was carried out on the performances of
the SOTA OD models: YOLOv5, YOLOv7 and SSD, for combinations of real and
synthetic datasets within an industrial use case.</div><div><a href='http://arxiv.org/abs/2401.12729v2'>2401.12729v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02639v2")'>False Positive Sampling-based Data Augmentation for Enhanced 3D Object
  Detection Accuracy</div>
<div id='2403.02639v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T04:07:54Z</div><div>Authors: Jiyong Oh, Junhaeng Lee, Woongchan Byun, Minsang Kong, Sang Hun Lee</div><div style='padding-top: 10px; width: 80ex'>Recent studies have focused on enhancing the performance of 3D object
detection models. Among various approaches, ground-truth sampling has been
proposed as an augmentation technique to address the challenges posed by
limited ground-truth data. However, an inherent issue with ground-truth
sampling is its tendency to increase false positives. Therefore, this study
aims to overcome the limitations of ground-truth sampling and improve the
performance of 3D object detection models by developing a new augmentation
technique called false-positive sampling. False-positive sampling involves
retraining the model using point clouds that are identified as false positives
in the model's predictions. We propose an algorithm that utilizes both
ground-truth and false-positive sampling and an algorithm for building the
false-positive sample database. Additionally, we analyze the principles behind
the performance enhancement due to false-positive sampling and propose a
technique that applies the concept of curriculum learning to the sampling
strategy that encompasses both false-positive and ground-truth sampling
techniques. Our experiments demonstrate that models utilizing false-positive
sampling show a reduction in false positives and exhibit improved object
detection performance. On the KITTI and Waymo Open datasets, models with
false-positive sampling surpass the baseline models by a large margin.</div><div><a href='http://arxiv.org/abs/2403.02639v2'>2403.02639v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03235v1")'>ActiveAnno3D -- An Active Learning Framework for Multi-Modal 3D Object
  Detection</div>
<div id='2402.03235v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T17:52:58Z</div><div>Authors: Ahmed Ghita, Bjørk Antoniussen, Walter Zimmer, Ross Greer, Christian Creß, Andreas Møgelmose, Mohan M. Trivedi, Alois C. Knoll</div><div style='padding-top: 10px; width: 80ex'>The curation of large-scale datasets is still costly and requires much time
and resources. Data is often manually labeled, and the challenge of creating
high-quality datasets remains. In this work, we fill the research gap using
active learning for multi-modal 3D object detection. We propose ActiveAnno3D,
an active learning framework to select data samples for labeling that are of
maximum informativeness for training. We explore various continuous training
methods and integrate the most efficient method regarding computational demand
and detection performance. Furthermore, we perform extensive experiments and
ablation studies with BEVFusion and PV-RCNN on the nuScenes and TUM Traffic
Intersection dataset. We show that we can achieve almost the same performance
with PV-RCNN and the entropy-based query strategy when using only half of the
training data (77.25 mAP compared to 83.50 mAP) of the TUM Traffic Intersection
dataset. BEVFusion achieved an mAP of 64.31 when using half of the training
data and 75.0 mAP when using the complete nuScenes dataset. We integrate our
active learning framework into the proAnno labeling tool to enable AI-assisted
data selection and labeling and minimize the labeling costs. Finally, we
provide code, weights, and visualization results on our website:
https://active3d-framework.github.io/active3d-framework.</div><div><a href='http://arxiv.org/abs/2402.03235v1'>2402.03235v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06946v1")'>3D Object Detection and High-Resolution Traffic Parameters Extraction
  Using Low-Resolution LiDAR Data</div>
<div id='2401.06946v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-13T01:22:20Z</div><div>Authors: Linlin Zhang, Xiang Yu, Armstrong Aboah, Yaw Adu-Gyamfi</div><div style='padding-top: 10px; width: 80ex'>Traffic volume data collection is a crucial aspect of transportation
engineering and urban planning, as it provides vital insights into traffic
patterns, congestion, and infrastructure efficiency. Traditional manual methods
of traffic data collection are both time-consuming and costly. However, the
emergence of modern technologies, particularly Light Detection and Ranging
(LiDAR), has revolutionized the process by enabling efficient and accurate data
collection. Despite the benefits of using LiDAR for traffic data collection,
previous studies have identified two major limitations that have impeded its
widespread adoption. These are the need for multiple LiDAR systems to obtain
complete point cloud information of objects of interest, as well as the
labor-intensive process of annotating 3D bounding boxes for object detection
tasks. In response to these challenges, the current study proposes an
innovative framework that alleviates the need for multiple LiDAR systems and
simplifies the laborious 3D annotation process. To achieve this goal, the study
employed a single LiDAR system, that aims at reducing the data acquisition cost
and addressed its accompanying limitation of missing point cloud information by
developing a Point Cloud Completion (PCC) framework to fill in missing point
cloud information using point density. Furthermore, we also used zero-shot
learning techniques to detect vehicles and pedestrians, as well as proposed a
unique framework for extracting low to high features from the object of
interest, such as height, acceleration, and speed. Using the 2D bounding box
detection and extracted height information, this study is able to generate 3D
bounding boxes automatically without human intervention.</div><div><a href='http://arxiv.org/abs/2401.06946v1'>2401.06946v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16634v1")'>The Why, When, and How to Use Active Learning in Large-Data-Driven 3D
  Object Detection for Safe Autonomous Driving: An Empirical Exploration</div>
<div id='2401.16634v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T00:14:13Z</div><div>Authors: Ross Greer, Bjørk Antoniussen, Mathias V. Andersen, Andreas Møgelmose, Mohan M. Trivedi</div><div style='padding-top: 10px; width: 80ex'>Active learning strategies for 3D object detection in autonomous driving
datasets may help to address challenges of data imbalance, redundancy, and
high-dimensional data. We demonstrate the effectiveness of entropy querying to
select informative samples, aiming to reduce annotation costs and improve model
performance. We experiment using the BEVFusion model for 3D object detection on
the nuScenes dataset, comparing active learning to random sampling and
demonstrating that entropy querying outperforms in most cases. The method is
particularly effective in reducing the performance gap between majority and
minority classes. Class-specific analysis reveals efficient allocation of
annotated resources for limited data budgets, emphasizing the importance of
selecting diverse and informative data for model training. Our findings suggest
that entropy querying is a promising strategy for selecting data that enhances
model learning in resource-constrained environments.</div><div><a href='http://arxiv.org/abs/2401.16634v1'>2401.16634v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06631v1")'>Evaluating the Energy Efficiency of Few-Shot Learning for Object
  Detection in Industrial Settings</div>
<div id='2403.06631v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T11:41:30Z</div><div>Authors: Georgios Tsoumplekas, Vladislav Li, Ilias Siniosoglou, Vasileios Argyriou, Sotirios K. Goudos, Ioannis D. Moscholios, Panagiotis Radoglou-Grammatikis, Panagiotis Sarigiannidis</div><div style='padding-top: 10px; width: 80ex'>In the ever-evolving era of Artificial Intelligence (AI), model performance
has constituted a key metric driving innovation, leading to an exponential
growth in model size and complexity. However, sustainability and energy
efficiency have been critical requirements during deployment in contemporary
industrial settings, necessitating the use of data-efficient approaches such as
few-shot learning. In this paper, to alleviate the burden of lengthy model
training and minimize energy consumption, a finetuning approach to adapt
standard object detection models to downstream tasks is examined. Subsequently,
a thorough case study and evaluation of the energy demands of the developed
models, applied in object detection benchmark datasets from volatile industrial
environments is presented. Specifically, different finetuning strategies as
well as utilization of ancillary evaluation data during training are examined,
and the trade-off between performance and efficiency is highlighted in this
low-data regime. Finally, this paper introduces a novel way to quantify this
trade-off through a customized Efficiency Factor metric.</div><div><a href='http://arxiv.org/abs/2403.06631v1'>2403.06631v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.07263v1")'>Adaptive Bounding Box Uncertainties via Two-Step Conformal Prediction</div>
<div id='2403.07263v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T02:45:24Z</div><div>Authors: Alexander Timans, Christoph-Nikolas Straehle, Kaspar Sakmann, Eric Nalisnick</div><div style='padding-top: 10px; width: 80ex'>Quantifying a model's predictive uncertainty is essential for safety-critical
applications such as autonomous driving. We consider quantifying such
uncertainty for multi-object detection. In particular, we leverage conformal
prediction to obtain uncertainty intervals with guaranteed coverage for object
bounding boxes. One challenge in doing so is that bounding box predictions are
conditioned on the object's class label. Thus, we develop a novel two-step
conformal approach that propagates uncertainty in predicted class labels into
the uncertainty intervals for the bounding boxes. This broadens the validity of
our conformal coverage guarantees to include incorrectly classified objects,
ensuring their usefulness when maximal safety assurances are required.
Moreover, we investigate novel ensemble and quantile regression formulations to
ensure the bounding box intervals are adaptive to object size, leading to a
more balanced coverage across sizes. Validating our two-step approach on
real-world datasets for 2D bounding box localization, we find that desired
coverage levels are satisfied with actionably tight predictive uncertainty
intervals.</div><div><a href='http://arxiv.org/abs/2403.07263v1'>2403.07263v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07642v1")'>A Flow-based Credibility Metric for Safety-critical Pedestrian Detection</div>
<div id='2402.07642v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T13:30:34Z</div><div>Authors: Maria Lyssenko, Christoph Gladisch, Christian Heinzemann, Matthias Woehrle, Rudolph Triebel</div><div style='padding-top: 10px; width: 80ex'>Safety is of utmost importance for perception in automated driving (AD).
However, a prime safety concern in state-of-the art object detection is that
standard evaluation schemes utilize safety-agnostic metrics to argue sufficient
detection performance. Hence, it is imperative to leverage supplementary domain
knowledge to accentuate safety-critical misdetections during evaluation tasks.
To tackle the underspecification, this paper introduces a novel credibility
metric, called c-flow, for pedestrian bounding boxes. To this end, c-flow
relies on a complementary optical flow signal from image sequences and enhances
the analyses of safety-critical misdetections without requiring additional
labels. We implement and evaluate c-flow with a state-of-the-art pedestrian
detector on a large AD dataset. Our analysis demonstrates that c-flow allows
developers to identify safety-critical misdetections.</div><div><a href='http://arxiv.org/abs/2402.07642v1'>2402.07642v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02986v1")'>A Safety-Adapted Loss for Pedestrian Detection in Automated Driving</div>
<div id='2402.02986v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T13:16:38Z</div><div>Authors: Maria Lyssenko, Piyush Pimplikar, Maarten Bieshaar, Farzad Nozarian, Rudolph Triebel</div><div style='padding-top: 10px; width: 80ex'>In safety-critical domains like automated driving (AD), errors by the object
detector may endanger pedestrians and other vulnerable road users (VRU). As
common evaluation metrics are not an adequate safety indicator, recent works
employ approaches to identify safety-critical VRU and back-annotate the risk to
the object detector. However, those approaches do not consider the safety
factor in the deep neural network (DNN) training process. Thus,
state-of-the-art DNN penalizes all misdetections equally irrespective of their
criticality. Subsequently, to mitigate the occurrence of critical failure
cases, i.e., false negatives, a safety-aware training strategy might be
required to enhance the detection performance for critical pedestrians. In this
paper, we propose a novel safety-aware loss variation that leverages the
estimated per-pedestrian criticality scores during training. We exploit the
reachability set-based time-to-collision (TTC-RSB) metric from the motion
domain along with distance information to account for the worst-case threat
quantifying the criticality. Our evaluation results using RetinaNet and FCOS on
the nuScenes dataset demonstrate that training the models with our safety-aware
loss function mitigates the misdetection of critical pedestrians without
sacrificing performance for the general case, i.e., pedestrians outside the
safety-critical zone.</div><div><a href='http://arxiv.org/abs/2402.02986v1'>2402.02986v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17013v1")'>Evaluation of Out-of-Distribution Detection Performance on Autonomous
  Driving Datasets</div>
<div id='2401.17013v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T13:49:03Z</div><div>Authors: Jens Henriksson, Christian Berger, Stig Ursing, Markus Borg</div><div style='padding-top: 10px; width: 80ex'>Safety measures need to be systemically investigated to what extent they
evaluate the intended performance of Deep Neural Networks (DNNs) for critical
applications. Due to a lack of verification methods for high-dimensional DNNs,
a trade-off is needed between accepted performance and handling of
out-of-distribution (OOD) samples.
  This work evaluates rejecting outputs from semantic segmentation DNNs by
applying a Mahalanobis distance (MD) based on the most probable
class-conditional Gaussian distribution for the predicted class as an OOD
score. The evaluation follows three DNNs trained on the Cityscapes dataset and
tested on four automotive datasets and finds that classification risk can
drastically be reduced at the cost of pixel coverage, even when applied on
unseen datasets. The applicability of our findings will support legitimizing
safety measures and motivate their usage when arguing for safe usage of DNNs in
automotive perception.</div><div><a href='http://arxiv.org/abs/2401.17013v1'>2401.17013v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02154v1")'>Evaluating the Robustness of Off-Road Autonomous Driving Segmentation
  against Adversarial Attacks: A Dataset-Centric analysis</div>
<div id='2402.02154v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T13:48:57Z</div><div>Authors: Pankaj Deoli, Rohit Kumar, Axel Vierling, Karsten Berns</div><div style='padding-top: 10px; width: 80ex'>This study investigates the vulnerability of semantic segmentation models to
adversarial input perturbations, in the domain of off-road autonomous driving.
Despite good performance in generic conditions, the state-of-the-art
classifiers are often susceptible to (even) small perturbations, ultimately
resulting in inaccurate predictions with high confidence. Prior research has
directed their focus on making models more robust by modifying the architecture
and training with noisy input images, but has not explored the influence of
datasets in adversarial attacks. Our study aims to address this gap by
examining the impact of non-robust features in off-road datasets and comparing
the effects of adversarial attacks on different segmentation network
architectures. To enable this, a robust dataset is created consisting of only
robust features and training the networks on this robustified dataset. We
present both qualitative and quantitative analysis of our findings, which have
important implications on improving the robustness of machine learning models
in off-road autonomous driving applications. Additionally, this work
contributes to the safe navigation of autonomous robot Unimog U5023 in rough
off-road unstructured environments by evaluating the robustness of segmentation
outputs. The code is publicly available at
https://github.com/rohtkumar/adversarial_attacks_ on_segmentation</div><div><a href='http://arxiv.org/abs/2402.02154v1'>2402.02154v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.15297v1")'>Semi-supervised Counting via Pixel-by-pixel Density Distribution
  Modelling</div>
<div id='2402.15297v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T12:48:02Z</div><div>Authors: Hui Lin, Zhiheng Ma, Rongrong Ji, Yaowei Wang, Zhou Su, Xiaopeng Hong, Deyu Meng</div><div style='padding-top: 10px; width: 80ex'>This paper focuses on semi-supervised crowd counting, where only a small
portion of the training data are labeled. We formulate the pixel-wise density
value to regress as a probability distribution, instead of a single
deterministic value. On this basis, we propose a semi-supervised crowd-counting
model. Firstly, we design a pixel-wise distribution matching loss to measure
the differences in the pixel-wise density distributions between the prediction
and the ground truth; Secondly, we enhance the transformer decoder by using
density tokens to specialize the forwards of decoders w.r.t. different density
intervals; Thirdly, we design the interleaving consistency self-supervised
learning mechanism to learn from unlabeled data efficiently. Extensive
experiments on four datasets are performed to show that our method clearly
outperforms the competitors by a large margin under various labeled ratio
settings. Code will be released at
https://github.com/LoraLinH/Semi-supervised-Counting-via-Pixel-by-pixel-Density-Distribution-Modelling.</div><div><a href='http://arxiv.org/abs/2402.15297v1'>2402.15297v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02245v2")'>Revisiting Generative Adversarial Networks for Binary Semantic
  Segmentation on Imbalanced Datasets</div>
<div id='2402.02245v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T19:24:40Z</div><div>Authors: Lei Xu, Moncef Gabbouj</div><div style='padding-top: 10px; width: 80ex'>Anomalous crack region detection is a typical binary semantic segmentation
task, which aims to detect pixels representing cracks on pavement surface
images automatically by algorithms. Although existing deep learning-based
methods have achieved outcoming results on specific public pavement datasets,
the performance would deteriorate dramatically on imbalanced datasets. The
input datasets used in such tasks suffer from severely between-class imbalanced
problems, hence, it is a core challenge to obtain a robust performance on
diverse pavement datasets with generic deep learning models. To address this
problem, in this work, we propose a deep learning framework based on
conditional Generative Adversarial Networks (cGANs) for the anomalous crack
region detection tasks at the pixel level. In particular, the proposed
framework containing a cGANs and a novel auxiliary network is developed to
enhance and stabilize the generator's performance under two alternative
training stages, when estimating a multiscale probability feature map from
heterogeneous and imbalanced inputs iteratively. Moreover, several attention
mechanisms and entropy strategies are incorporated into the cGANs architecture
and the auxiliary network separately to mitigate further the performance
deterioration of model training on severely imbalanced datasets. We implement
extensive experiments on six accessible pavement datasets. The experimental
results from both visual and quantitative evaluation show that the proposed
framework can achieve state-of-the-art results on these datasets efficiently
and robustly without acceleration of computation complexity.</div><div><a href='http://arxiv.org/abs/2402.02245v2'>2402.02245v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.10790v1")'>Measuring the Impact of Scene Level Objects on Object Detection: Towards
  Quantitative Explanations of Detection Decisions</div>
<div id='2401.10790v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T16:21:55Z</div><div>Authors: Lynn Vonder Haar, Timothy Elvira, Luke Newcomb, Omar Ochoa</div><div style='padding-top: 10px; width: 80ex'>Although accuracy and other common metrics can provide a useful window into
the performance of an object detection model, they lack a deeper view of the
model's decision process. Regardless of the quality of the training data and
process, the features that an object detection model learns cannot be
guaranteed. A model may learn a relationship between certain background
context, i.e., scene level objects, and the presence of the labeled classes.
Furthermore, standard performance verification and metrics would not identify
this phenomenon. This paper presents a new black box explainability method for
additional verification of object detection models by finding the impact of
scene level objects on the identification of the objects within the image. By
comparing the accuracies of a model on test data with and without certain scene
level objects, the contributions of these objects to the model's performance
becomes clearer. The experiment presented here will assess the impact of
buildings and people in image context on the detection of emergency road
vehicles by a fine-tuned YOLOv8 model. A large increase in accuracy in the
presence of a scene level object will indicate the model's reliance on that
object to make its detections. The results of this research lead to providing a
quantitative explanation of the object detection model's decision process,
enabling a deeper understanding of the model's performance.</div><div><a href='http://arxiv.org/abs/2401.10790v1'>2401.10790v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15455v1")'>New Foggy Object Detecting Model</div>
<div id='2401.15455v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-27T16:29:53Z</div><div>Authors: Rahul Banavathu, Modem Veda Sree, Bollina Kavya Sri, Suddhasil De</div><div style='padding-top: 10px; width: 80ex'>Object detection in reduced visibility has become a prominent research area.
The existing techniques are not accurate enough in recognizing objects under
such circumstances. This paper introduces a new foggy object detection method
through a two-staged architecture of region identification from input images
and detecting objects in such regions. The paper confirms notable improvements
of the proposed method's accuracy and detection time over existing techniques.</div><div><a href='http://arxiv.org/abs/2401.15455v1'>2401.15455v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.15374v1")'>Outlier detection by ensembling uncertainty with negative objectness</div>
<div id='2402.15374v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T15:19:37Z</div><div>Authors: Anja Delić, Matej Grcić, Siniša Šegvić</div><div style='padding-top: 10px; width: 80ex'>Outlier detection is an essential capability in safety-critical applications
of supervised visual recognition. Most of the existing methods deliver best
results by encouraging standard closed-set models to produce low-confidence
predictions in negative training data. However, that approach conflates
prediction uncertainty with recognition of the negative class. We therefore
reconsider direct prediction of K+1 logits that correspond to K groundtruth
classes and one outlier class. This setup allows us to formulate a novel
anomaly score as an ensemble of in-distribution uncertainty and the posterior
of the outlier class which we term negative objectness. Now outliers can be
independently detected due to i) high prediction uncertainty or ii) similarity
with negative data. We embed our method into a dense prediction architecture
with mask-level recognition over K+2 classes. The training procedure encourages
the novel K+2-th class to learn negative objectness at pasted negative
instances. Our models outperform the current state-of-the art on standard
benchmarks for image-wide and pixel-level outlier detection with and without
training on real negative data.</div><div><a href='http://arxiv.org/abs/2402.15374v1'>2402.15374v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15143v1")'>PUAD: Frustratingly Simple Method for Robust Anomaly Detection</div>
<div id='2402.15143v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T06:57:31Z</div><div>Authors: Shota Sugawara, Ryuji Imamura</div><div style='padding-top: 10px; width: 80ex'>Developing an accurate and fast anomaly detection model is an important task
in real-time computer vision applications. There has been much research to
develop a single model that detects either structural or logical anomalies,
which are inherently distinct. The majority of the existing approaches
implicitly assume that the anomaly can be represented by identifying the
anomalous location. However, we argue that logical anomalies, such as the wrong
number of objects, can not be well-represented by the spatial feature maps and
require an alternative approach. In addition, we focused on the possibility of
detecting logical anomalies by using an out-of-distribution detection approach
on the feature space, which aggregates the spatial information of the feature
map. As a demonstration, we propose a method that incorporates a simple
out-of-distribution detection method on the feature space against
state-of-the-art reconstruction-based approaches. Despite the simplicity of our
proposal, our method PUAD (Picturable and Unpicturable Anomaly Detection)
achieves state-of-the-art performance on the MVTec LOCO AD dataset.</div><div><a href='http://arxiv.org/abs/2402.15143v1'>2402.15143v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14233v1")'>SoftPatch: Unsupervised Anomaly Detection with Noisy Data</div>
<div id='2403.14233v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T08:49:34Z</div><div>Authors: Xi Jiang, Ying Chen, Qiang Nie, Yong Liu, Jianlin Liu, Bin-Bin Gao, Jun Liu, Chengjie Wang, Feng Zheng</div><div style='padding-top: 10px; width: 80ex'>Although mainstream unsupervised anomaly detection (AD) algorithms perform
well in academic datasets, their performance is limited in practical
application due to the ideal experimental setting of clean training data.
Training with noisy data is an inevitable problem in real-world anomaly
detection but is seldom discussed. This paper considers label-level noise in
image sensory anomaly detection for the first time. To solve this problem, we
proposed a memory-based unsupervised AD method, SoftPatch, which efficiently
denoises the data at the patch level. Noise discriminators are utilized to
generate outlier scores for patch-level noise elimination before coreset
construction. The scores are then stored in the memory bank to soften the
anomaly detection boundary. Compared with existing methods, SoftPatch maintains
a strong modeling ability of normal data and alleviates the overconfidence
problem in coreset. Comprehensive experiments in various noise scenes
demonstrate that SoftPatch outperforms the state-of-the-art AD methods on the
MVTecAD and BTAD benchmarks and is comparable to those methods under the
setting without noise.</div><div><a href='http://arxiv.org/abs/2403.14233v1'>2403.14233v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14246v1")'>Reconstruction-Based Anomaly Localization via Knowledge-Informed
  Self-Training</div>
<div id='2402.14246v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T03:15:13Z</div><div>Authors: Cheng Qian, Xiaoxian Lao, Chunguang Li</div><div style='padding-top: 10px; width: 80ex'>Anomaly localization, which involves localizing anomalous regions within
images, is a significant industrial task. Reconstruction-based methods are
widely adopted for anomaly localization because of their low complexity and
high interpretability. Most existing reconstruction-based methods only use
normal samples to construct model. If anomalous samples are appropriately
utilized in the process of anomaly localization, the localization performance
can be improved. However, usually only weakly labeled anomalous samples are
available, which limits the improvement. In many cases, we can obtain some
knowledge of anomalies summarized by domain experts. Taking advantage of such
knowledge can help us better utilize the anomalous samples and thus further
improve the localization performance. In this paper, we propose a novel
reconstruction-based method named knowledge-informed self-training (KIST) which
integrates knowledge into reconstruction model through self-training.
Specifically, KIST utilizes weakly labeled anomalous samples in addition to the
normal ones and exploits knowledge to yield pixel-level pseudo-labels of the
anomalous samples. Based on the pseudo labels, a novel loss which promotes the
reconstruction of normal pixels while suppressing the reconstruction of
anomalous pixels is used. We conduct experiments on different datasets and
demonstrate the advantages of KIST over the existing reconstruction-based
methods.</div><div><a href='http://arxiv.org/abs/2402.14246v1'>2402.14246v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.01076v1")'>Extracting Usable Predictions from Quantized Networks through
  Uncertainty Quantification for OOD Detection</div>
<div id='2403.01076v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T03:03:29Z</div><div>Authors: Rishi Singhal, Srinath Srinivasan</div><div style='padding-top: 10px; width: 80ex'>OOD detection has become more pertinent with advances in network design and
increased task complexity. Identifying which parts of the data a given network
is misclassifying has become as valuable as the network's overall performance.
We can compress the model with quantization, but it suffers minor performance
loss. The loss of performance further necessitates the need to derive the
confidence estimate of the network's predictions. In line with this thinking,
we introduce an Uncertainty Quantification(UQ) technique to quantify the
uncertainty in the predictions from a pre-trained vision model. We subsequently
leverage this information to extract valuable predictions while ignoring the
non-confident predictions. We observe that our technique saves up to 80% of
ignored samples from being misclassified. The code for the same is available
here.</div><div><a href='http://arxiv.org/abs/2403.01076v1'>2403.01076v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.18292v1")'>FSL Model can Score Higher as It Is</div>
<div id='2402.18292v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T12:37:30Z</div><div>Authors: Yunwei Bai, Ying Kiat Tan, Tsuhan Chen</div><div style='padding-top: 10px; width: 80ex'>In daily life, we tend to present the front of our faces by staring squarely
at a facial recognition machine, instead of facing it sideways, in order to
increase the chance of being correctly recognised. Few-shot-learning (FSL)
classification is challenging in itself because a model has to identify images
that belong to classes previously unseen during training. Therefore, a warped
and non-typical query or support image during testing can make it even more
challenging for a model to predict correctly. In our work, to increase the
chance of correct prediction during testing, we aim to rectify the test input
of a trained FSL model by generating new samples of the tested classes through
image-to-image translation. An FSL model is usually trained on classes with
sufficient samples, and then tested on classes with few-shot samples. Our
proposed method first captures the style or shape of the test image, and then
identifies a suitable trained class sample. It then transfers the style or
shape of the test image to the train-class images for generation of more
test-class samples, before performing classification based on a set of
generated samples instead of just one sample. Our method has potential in
empowering a trained FSL model to score higher during the testing phase without
any extra training nor dataset. According to our experiments, by augmenting the
support set with just 1 additional generated sample, we can achieve around 2%
improvement for trained FSL models on datasets consisting of either animal
faces or traffic signs. By augmenting both the support set and the queries, we
can achieve even more performance improvement. Our Github Repository is
publicly available.</div><div><a href='http://arxiv.org/abs/2402.18292v1'>2402.18292v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06534v1")'>SARDet-100K: Towards Open-Source Benchmark and ToolKit for Large-Scale
  SAR Object Detection</div>
<div id='2403.06534v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T09:20:40Z</div><div>Authors: Yuxuan Li, Xiang Li, Weijie Li, Qibin Hou, Li Liu, Ming-Ming Cheng, Jian Yang</div><div style='padding-top: 10px; width: 80ex'>Synthetic Aperture Radar (SAR) object detection has gained significant
attention recently due to its irreplaceable all-weather imaging capabilities.
However, this research field suffers from both limited public datasets (mostly
comprising &lt;2K images with only mono-category objects) and inaccessible source
code. To tackle these challenges, we establish a new benchmark dataset and an
open-source method for large-scale SAR object detection. Our dataset,
SARDet-100K, is a result of intense surveying, collecting, and standardizing 10
existing SAR detection datasets, providing a large-scale and diverse dataset
for research purposes. To the best of our knowledge, SARDet-100K is the first
COCO-level large-scale multi-class SAR object detection dataset ever created.
With this high-quality dataset, we conducted comprehensive experiments and
uncovered a crucial challenge in SAR object detection: the substantial
disparities between the pretraining on RGB datasets and finetuning on SAR
datasets in terms of both data domain and model structure. To bridge these
gaps, we propose a novel Multi-Stage with Filter Augmentation (MSFA)
pretraining framework that tackles the problems from the perspective of data
input, domain transition, and model migration. The proposed MSFA method
significantly enhances the performance of SAR object detection models while
demonstrating exceptional generalizability and flexibility across diverse
models. This work aims to pave the way for further advancements in SAR object
detection. The dataset and code is available at
https://github.com/zcablii/SARDet_100K.</div><div><a href='http://arxiv.org/abs/2403.06534v1'>2403.06534v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14661v1")'>From Blurry to Brilliant Detection: YOLOv5-Based Aerial Object Detection
  with Super Resolution</div>
<div id='2401.14661v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T05:50:58Z</div><div>Authors: Ragib Amin Nihal, Benjamin Yen, Katsutoshi Itoyama, Kazuhiro Nakadai</div><div style='padding-top: 10px; width: 80ex'>The demand for accurate object detection in aerial imagery has surged with
the widespread use of drones and satellite technology. Traditional object
detection models, trained on datasets biased towards large objects, struggle to
perform optimally in aerial scenarios where small, densely clustered objects
are prevalent. To address this challenge, we present an innovative approach
that combines super-resolution and an adapted lightweight YOLOv5 architecture.
We employ a range of datasets, including VisDrone-2023, SeaDroneSee, VEDAI, and
NWPU VHR-10, to evaluate our model's performance. Our Super Resolved YOLOv5
architecture features Transformer encoder blocks, allowing the model to capture
global context and context information, leading to improved detection results,
especially in high-density, occluded conditions. This lightweight model not
only delivers improved accuracy but also ensures efficient resource
utilization, making it well-suited for real-time applications. Our experimental
results demonstrate the model's superior performance in detecting small and
densely clustered objects, underlining the significance of dataset choice and
architectural adaptation for this specific task. In particular, the method
achieves 52.5% mAP on VisDrone, exceeding top prior works. This approach
promises to significantly advance object detection in aerial imagery,
contributing to more accurate and reliable results in a variety of real-world
applications.</div><div><a href='http://arxiv.org/abs/2401.14661v1'>2401.14661v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.06874v1")'>COOD: Combined out-of-distribution detection using multiple measures for
  anomaly &amp; novel class detection in large-scale hierarchical classification</div>
<div id='2403.06874v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T16:26:35Z</div><div>Authors: L. E. Hogeweg, R. Gangireddy, D. Brunink, V. J. Kalkman, L. Cornelissen, J. W. Kamminga</div><div style='padding-top: 10px; width: 80ex'>High-performing out-of-distribution (OOD) detection, both anomaly and novel
class, is an important prerequisite for the practical use of classification
models. In this paper, we focus on the species recognition task in images
concerned with large databases, a large number of fine-grained hierarchical
classes, severe class imbalance, and varying image quality. We propose a
framework for combining individual OOD measures into one combined OOD (COOD)
measure using a supervised model. The individual measures are several existing
state-of-the-art measures and several novel OOD measures developed with novel
class detection and hierarchical class structure in mind. COOD was extensively
evaluated on three large-scale (500k+ images) biodiversity datasets in the
context of anomaly and novel class detection. We show that COOD outperforms
individual, including state-of-the-art, OOD measures by a large margin in terms
of TPR@1% FPR in the majority of experiments, e.g., improving detecting
ImageNet images (OOD) from 54.3% to 85.4% for the iNaturalist 2018 dataset.
SHAP (feature contribution) analysis shows that different individual OOD
measures are essential for various tasks, indicating that multiple OOD measures
and combinations are needed to generalize. Additionally, we show that
explicitly considering ID images that are incorrectly classified for the
original (species) recognition task is important for constructing
high-performing OOD detection methods and for practical applicability. The
framework can easily be extended or adapted to other tasks and media
modalities.</div><div><a href='http://arxiv.org/abs/2403.06874v1'>2403.06874v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.19142v1")'>ProtoP-OD: Explainable Object Detection with Prototypical Parts</div>
<div id='2402.19142v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T13:25:15Z</div><div>Authors: Pavlos Rath-Manakidis, Frederik Strothmann, Tobias Glasmachers, Laurenz Wiskott</div><div style='padding-top: 10px; width: 80ex'>Interpretation and visualization of the behavior of detection transformers
tends to highlight the locations in the image that the model attends to, but it
provides limited insight into the \emph{semantics} that the model is focusing
on. This paper introduces an extension to detection transformers that
constructs prototypical local features and uses them in object detection. These
custom features, which we call prototypical parts, are designed to be mutually
exclusive and align with the classifications of the model. The proposed
extension consists of a bottleneck module, the prototype neck, that computes a
discretized representation of prototype activations and a new loss term that
matches prototypes to object classes. This setup leads to interpretable
representations in the prototype neck, allowing visual inspection of the image
content perceived by the model and a better understanding of the model's
reliability. We show experimentally that our method incurs only a limited
performance penalty, and we provide examples that demonstrate the quality of
the explanations provided by our method, which we argue outweighs the
performance penalty.</div><div><a href='http://arxiv.org/abs/2402.19142v1'>2402.19142v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02851v1")'>Enhancing Compositional Generalization via Compositional Feature
  Alignment</div>
<div id='2402.02851v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T10:06:24Z</div><div>Authors: Haoxiang Wang, Haozhe Si, Huajie Shao, Han Zhao</div><div style='padding-top: 10px; width: 80ex'>Real-world applications of machine learning models often confront data
distribution shifts, wherein discrepancies exist between the training and test
data distributions. In the common multi-domain multi-class setup, as the number
of classes and domains scales up, it becomes infeasible to gather training data
for every domain-class combination. This challenge naturally leads the quest
for models with Compositional Generalization (CG) ability, where models can
generalize to unseen domain-class combinations. To delve into the CG challenge,
we develop CG-Bench, a suite of CG benchmarks derived from existing real-world
image datasets, and observe that the prevalent pretraining-finetuning paradigm
on foundational models, such as CLIP and DINOv2, struggles with the challenge.
To address this challenge, we propose Compositional Feature Alignment (CFA), a
simple two-stage finetuning technique that i) learns two orthogonal linear
heads on a pretrained encoder with respect to class and domain labels, and ii)
fine-tunes the encoder with the newly learned head frozen. We theoretically and
empirically justify that CFA encourages compositional feature learning of
pretrained models. We further conduct extensive experiments on CG-Bench for
CLIP and DINOv2, two powerful pretrained vision foundation models. Experiment
results show that CFA outperforms common finetuning techniques in compositional
generalization, corroborating CFA's efficacy in compositional feature learning.</div><div><a href='http://arxiv.org/abs/2402.02851v1'>2402.02851v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07241v1")'>Calibrating Multi-modal Representations: A Pursuit of Group Robustness
  without Annotations</div>
<div id='2403.07241v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T01:47:17Z</div><div>Authors: Chenyu You, Yifei Min, Weicheng Dai, Jasjeet S. Sekhon, Lawrence Staib, James S. Duncan</div><div style='padding-top: 10px; width: 80ex'>Fine-tuning pre-trained vision-language models, like CLIP, has yielded
success on diverse downstream tasks. However, several pain points persist for
this paradigm: (i) directly tuning entire pre-trained models becomes both
time-intensive and computationally costly. Additionally, these tuned models
tend to become highly specialized, limiting their practicality for real-world
deployment; (ii) recent studies indicate that pre-trained vision-language
classifiers may overly depend on spurious features -- patterns that correlate
with the target in training data, but are not related to the true labeling
function; and (iii) existing studies on mitigating the reliance on spurious
features, largely based on the assumption that we can identify such features,
does not provide definitive assurance for real-world applications. As a
piloting study, this work focuses on exploring mitigating the reliance on
spurious features for CLIP without using any group annotation. To this end, we
systematically study the existence of spurious correlation on CLIP and
CILP+ERM. We first, following recent work on Deep Feature Reweighting (DFR),
verify that last-layer retraining can greatly improve group robustness on
pretrained CLIP. In view of them, we advocate a lightweight representation
calibration method for fine-tuning CLIP, by first generating a calibration set
using the pretrained CLIP, and then calibrating representations of samples
within this set through contrastive learning, all without the need for group
labels. Extensive experiments and in-depth visualizations on several benchmarks
validate the effectiveness of our proposals, largely reducing reliance and
significantly boosting the model generalization.</div><div><a href='http://arxiv.org/abs/2403.07241v1'>2403.07241v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11497v1")'>Do CLIPs Always Generalize Better than ImageNet Models?</div>
<div id='2403.11497v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T06:04:02Z</div><div>Authors: Qizhou Wang, Yong Lin, Yongqiang Chen, Ludwig Schmidt, Bo Han, Tong Zhang</div><div style='padding-top: 10px; width: 80ex'>Large vision language models, such as CLIPs, have revolutionized modern
machine learning. CLIPs have demonstrated great generalizability under
distribution shifts, supported by an increasing body of literature. However,
the evaluation datasets for CLIPs are variations primarily designed for
ImageNet benchmarks, which may not fully reflect the extent to which CLIPs,
e.g., pre-trained on LAION, robust to spurious correlations. To bridge the gap,
we collect a real-world dataset called CounterAnimal that contains realistic
spurious features found in animal photos. CounterAnimal consists of a) the
common group: comprising animals on common backgrounds, and b) the counter
group: including animals on unusual backgrounds. The performance drops from the
common to counter groups quantify the reliance of models on spurious features
(i.e., backgrounds) to predict the animals. We find that CLIPs trained on
either LAION or the OpenAI data exhibit notable performance drops on the
counter group. Surprisingly, we observe that single-modal models trained on
ImageNet are more robust than CLIPs. We provide both theoretical and empirical
explanations for why CLIPs still learn spurious features. Our findings suggest
that distribution shifts remain an open problem for CLIPs, and one needs to be
cautious about test setups when evaluating foundation models pre-trained on a
significantly different scale and distribution.</div><div><a href='http://arxiv.org/abs/2403.11497v1'>2403.11497v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.19460v1")'>Benchmarking Uncertainty Disentanglement: Specialized Uncertainties for
  Specialized Tasks</div>
<div id='2402.19460v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T18:52:56Z</div><div>Authors: Bálint Mucsányi, Michael Kirchhof, Seong Joon Oh</div><div style='padding-top: 10px; width: 80ex'>Uncertainty quantification, once a singular task, has evolved into a spectrum
of tasks, including abstained prediction, out-of-distribution detection, and
aleatoric uncertainty quantification. The latest goal is disentanglement: the
construction of multiple estimators that are each tailored to one and only one
task. Hence, there is a plethora of recent advances with different intentions -
that often entirely deviate from practical behavior. This paper conducts a
comprehensive evaluation of numerous uncertainty estimators across diverse
tasks on ImageNet. We find that, despite promising theoretical endeavors,
disentanglement is not yet achieved in practice. Additionally, we reveal which
uncertainty estimators excel at which specific tasks, providing insights for
practitioners and guiding future research toward task-centric and disentangled
uncertainty estimation methods. Our code is available at
https://github.com/bmucsanyi/bud.</div><div><a href='http://arxiv.org/abs/2402.19460v1'>2402.19460v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12327v1")'>GT-Rain Single Image Deraining Challenge Report</div>
<div id='2403.12327v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T23:45:18Z</div><div>Authors: Howard Zhang, Yunhao Ba, Ethan Yang, Rishi Upadhyay, Alex Wong, Achuta Kadambi, Yun Guo, Xueyao Xiao, Xiaoxiong Wang, Yi Li, Yi Chang, Luxin Yan, Chaochao Zheng, Luping Wang, Bin Liu, Sunder Ali Khowaja, Jiseok Yoon, Ik-Hyun Lee, Zhao Zhang, Yanyan Wei, Jiahuan Ren, Suiyi Zhao, Huan Zheng</div><div style='padding-top: 10px; width: 80ex'>This report reviews the results of the GT-Rain challenge on single image
deraining at the UG2+ workshop at CVPR 2023. The aim of this competition is to
study the rainy weather phenomenon in real world scenarios, provide a novel
real world rainy image dataset, and to spark innovative ideas that will further
the development of single image deraining methods on real images. Submissions
were trained on the GT-Rain dataset and evaluated on an extension of the
dataset consisting of 15 additional scenes. Scenes in GT-Rain are comprised of
real rainy image and ground truth image captured moments after the rain had
stopped. 275 participants were registered in the challenge and 55 competed in
the final testing phase.</div><div><a href='http://arxiv.org/abs/2403.12327v1'>2403.12327v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.15414v1")'>Does Combining Parameter-efficient Modules Improve Few-shot Transfer
  Accuracy?</div>
<div id='2402.15414v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T16:20:29Z</div><div>Authors: Nader Asadi, Mahdi Beitollahi, Yasser Khalil, Yinchuan Li, Guojun Zhang, Xi Chen</div><div style='padding-top: 10px; width: 80ex'>Parameter-efficient fine-tuning stands as the standard for efficiently
fine-tuning large language and vision models on downstream tasks. Specifically,
the efficiency of low-rank adaptation has facilitated the creation and sharing
of hundreds of custom LoRA modules, each trained on distinct data from various
downstream tasks. In this paper, we explore the composability of LoRA modules,
examining if combining these pre-trained modules enhances generalization to
unseen downstream tasks. Our investigation involves evaluating two approaches:
(a) uniform composition, involving averaging upstream LoRA modules with equal
weights, and (b) learned composition, where we learn the weights for each
upstream module and perform weighted averaging. Our experimental results on
both vision and language models reveal that in few-shot settings, where only a
limited number of samples are available for the downstream task, both uniform
and learned composition methods result in better transfer accuracy;
outperforming full fine-tuning and training a LoRA from scratch. Moreover, in
full-shot settings, learned composition performs comparably to regular LoRA
training with significantly fewer number of trainable parameters. Our research
unveils the potential of uniform composition for enhancing transferability in
low-shot settings, without introducing additional learnable parameters.</div><div><a href='http://arxiv.org/abs/2402.15414v1'>2402.15414v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.01922v1")'>Unsupervised Object-Centric Learning from Multiple Unspecified
  Viewpoints</div>
<div id='2401.01922v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T15:09:25Z</div><div>Authors: Jinyang Yuan, Tonglin Chen, Zhimeng Shen, Bin Li, Xiangyang Xue</div><div style='padding-top: 10px; width: 80ex'>Visual scenes are extremely diverse, not only because there are infinite
possible combinations of objects and backgrounds but also because the
observations of the same scene may vary greatly with the change of viewpoints.
When observing a multi-object visual scene from multiple viewpoints, humans can
perceive the scene compositionally from each viewpoint while achieving the
so-called ``object constancy'' across different viewpoints, even though the
exact viewpoints are untold. This ability is essential for humans to identify
the same object while moving and to learn from vision efficiently. It is
intriguing to design models that have a similar ability. In this paper, we
consider a novel problem of learning compositional scene representations from
multiple unspecified (i.e., unknown and unrelated) viewpoints without using any
supervision and propose a deep generative model which separates latent
representations into a viewpoint-independent part and a viewpoint-dependent
part to solve this problem. During the inference, latent representations are
randomly initialized and iteratively updated by integrating the information in
different viewpoints with neural networks. Experiments on several specifically
designed synthetic datasets have shown that the proposed method can effectively
learn from multiple unspecified viewpoints.</div><div><a href='http://arxiv.org/abs/2401.01922v1'>2401.01922v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05490v1")'>Poly-View Contrastive Learning</div>
<div id='2403.05490v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T17:55:41Z</div><div>Authors: Amitis Shidani, Devon Hjelm, Jason Ramapuram, Russ Webb, Eeshan Gunesh Dhekane, Dan Busbridge</div><div style='padding-top: 10px; width: 80ex'>Contrastive learning typically matches pairs of related views among a number
of unrelated negative views. Views can be generated (e.g. by augmentations) or
be observed. We investigate matching when there are more than two related views
which we call poly-view tasks, and derive new representation learning
objectives using information maximization and sufficient statistics. We show
that with unlimited computation, one should maximize the number of related
views, and with a fixed compute budget, it is beneficial to decrease the number
of unique samples whilst increasing the number of views of those samples. In
particular, poly-view contrastive models trained for 128 epochs with batch size
256 outperform SimCLR trained for 1024 epochs at batch size 4096 on ImageNet1k,
challenging the belief that contrastive models require large batch sizes and
many training epochs.</div><div><a href='http://arxiv.org/abs/2403.05490v1'>2403.05490v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02013v1")'>SwitchTab: Switched Autoencoders Are Effective Tabular Learners</div>
<div id='2401.02013v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T01:05:45Z</div><div>Authors: Jing Wu, Suiyao Chen, Qi Zhao, Renat Sergazinov, Chen Li, Shengjie Liu, Chongchao Zhao, Tianpei Xie, Hanqing Guo, Cheng Ji, Daniel Cociorva, Hakan Brunzel</div><div style='padding-top: 10px; width: 80ex'>Self-supervised representation learning methods have achieved significant
success in computer vision and natural language processing, where data samples
exhibit explicit spatial or semantic dependencies. However, applying these
methods to tabular data is challenging due to the less pronounced dependencies
among data samples. In this paper, we address this limitation by introducing
SwitchTab, a novel self-supervised method specifically designed to capture
latent dependencies in tabular data. SwitchTab leverages an asymmetric
encoder-decoder framework to decouple mutual and salient features among data
pairs, resulting in more representative embeddings. These embeddings, in turn,
contribute to better decision boundaries and lead to improved results in
downstream tasks. To validate the effectiveness of SwitchTab, we conduct
extensive experiments across various domains involving tabular data. The
results showcase superior performance in end-to-end prediction tasks with
fine-tuning. Moreover, we demonstrate that pre-trained salient embeddings can
be utilized as plug-and-play features to enhance the performance of various
traditional classification methods (e.g., Logistic Regression, XGBoost, etc.).
Lastly, we highlight the capability of SwitchTab to create explainable
representations through visualization of decoupled mutual and salient features
in the latent space.</div><div><a href='http://arxiv.org/abs/2401.02013v1'>2401.02013v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00963v1")'>Tree-Regularized Tabular Embeddings</div>
<div id='2403.00963v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T20:26:33Z</div><div>Authors: Xuan Li, Yun Wang, Bo Li</div><div style='padding-top: 10px; width: 80ex'>Tabular neural network (NN) has attracted remarkable attentions and its
recent advances have gradually narrowed the performance gap with respect to
tree-based models on many public datasets. While the mainstreams focus on
calibrating NN to fit tabular data, we emphasize the importance of homogeneous
embeddings and alternately concentrate on regularizing tabular inputs through
supervised pretraining. Specifically, we extend a recent work (DeepTLF) and
utilize the structure of pretrained tree ensembles to transform raw variables
into a single vector (T2V), or an array of tokens (T2T). Without loss of space
efficiency, these binarized embeddings can be consumed by canonical tabular NN
with fully-connected or attention-based building blocks. Through quantitative
experiments on 88 OpenML datasets with binary classification task, we validated
that the proposed tree-regularized representation not only tapers the
difference with respect to tree-based models, but also achieves on-par and
better performance when compared with advanced NN models. Most importantly, it
possesses better robustness and can be easily scaled and generalized as
standalone encoder for tabular modality. Codes:
https://github.com/milanlx/tree-regularized-embedding.</div><div><a href='http://arxiv.org/abs/2403.00963v1'>2403.00963v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.06971v1")'>In-Context Data Distillation with TabPFN</div>
<div id='2402.06971v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-10T15:23:45Z</div><div>Authors: Junwei Ma, Valentin Thomas, Guangwei Yu, Anthony Caterini</div><div style='padding-top: 10px; width: 80ex'>Foundation models have revolutionized tasks in computer vision and natural
language processing. However, in the realm of tabular data, tree-based models
like XGBoost continue to dominate. TabPFN, a transformer model tailored for
tabular data, mirrors recent foundation models in its exceptional in-context
learning capability, being competitive with XGBoost's performance without the
need for task-specific training or hyperparameter tuning. Despite its promise,
TabPFN's applicability is hindered by its data size constraint, limiting its
use in real-world scenarios. To address this, we present in-context data
distillation (ICD), a novel methodology that effectively eliminates these
constraints by optimizing TabPFN's context. ICD efficiently enables TabPFN to
handle significantly larger datasets with a fixed memory budget, improving
TabPFN's quadratic memory complexity but at the cost of a linear number of
tuning steps. Notably, TabPFN, enhanced with ICD, demonstrates very strong
performance against established tree-based models and modern deep learning
methods on 48 large tabular datasets from OpenML.</div><div><a href='http://arxiv.org/abs/2402.06971v1'>2402.06971v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04822v1")'>UniTable: Towards a Unified Framework for Table Structure Recognition
  via Self-Supervised Pretraining</div>
<div id='2403.04822v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T15:44:50Z</div><div>Authors: ShengYun Peng, Seongmin Lee, Xiaojing Wang, Rajarajeswari Balasubramaniyan, Duen Horng Chau</div><div style='padding-top: 10px; width: 80ex'>Tables convey factual and quantitative data with implicit conventions created
by humans that are often challenging for machines to parse. Prior work on table
structure recognition (TSR) has mainly centered around complex task-specific
combinations of available inputs and tools. We present UniTable, a training
framework that unifies both the training paradigm and training objective of
TSR. Its training paradigm combines the simplicity of purely pixel-level inputs
with the effectiveness and scalability empowered by self-supervised pretraining
(SSP) from diverse unannotated tabular images. Our framework unifies the
training objectives of all three TSR tasks - extracting table structure, cell
content, and cell bounding box (bbox) - into a unified task-agnostic training
objective: language modeling. Extensive quantitative and qualitative analyses
highlight UniTable's state-of-the-art (SOTA) performance on four of the largest
TSR datasets. To promote reproducible research, enhance transparency, and SOTA
innovations, we open-source our code at https://github.com/poloclub/unitable
and release the first-of-its-kind Jupyter Notebook of the whole inference
pipeline, fine-tuned across multiple TSR datasets, supporting all three TSR
tasks.</div><div><a href='http://arxiv.org/abs/2403.04822v1'>2403.04822v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14926v1")'>Boosting gets full Attention for Relational Learning</div>
<div id='2402.14926v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T19:16:01Z</div><div>Authors: Mathieu Guillame-Bert, Richard Nock</div><div style='padding-top: 10px; width: 80ex'>More often than not in benchmark supervised ML, tabular data is flat, i.e.
consists of a single $m \times d$ (rows, columns) file, but cases abound in the
real world where observations are described by a set of tables with structural
relationships. Neural nets-based deep models are a classical fit to incorporate
general topological dependence among description features (pixels, words,
etc.), but their suboptimality to tree-based models on tabular data is still
well documented. In this paper, we introduce an attention mechanism for
structured data that blends well with tree-based models in the training context
of (gradient) boosting. Each aggregated model is a tree whose training involves
two steps: first, simple tabular models are learned descending tables in a
top-down fashion with boosting's class residuals on tables' features. Second,
what has been learned progresses back bottom-up via attention and aggregation
mechanisms, progressively crafting new features that complete at the end the
set of observation features over which a single tree is learned, boosting's
iteration clock is incremented and new class residuals are computed.
Experiments on simulated and real-world domains display the competitiveness of
our method against a state of the art containing both tree-based and neural
nets-based models.</div><div><a href='http://arxiv.org/abs/2402.14926v1'>2402.14926v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16785v1")'>CARTE: pretraining and transfer for tabular learning</div>
<div id='2402.16785v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T18:00:29Z</div><div>Authors: Myung Jun Kim, Léo Grinsztajn, Gaël Varoquaux</div><div style='padding-top: 10px; width: 80ex'>Pretrained deep-learning models are the go-to solution for images or text.
However, for tabular data the standard is still to train tree-based models.
Pre-training or transfer is a huge challenge as in general tables have columns
about different quantities and naming conventions that vary vastly across
sources. Data integration tackles correspondences across multiple sources:
schema matching for columns, and entity matching for entries. We propose a
neural architecture that does not need such matches. As a result, we can
pretrain it on background data that has not been matched. The architecture -
CARTE for Context Aware Representation of Table Entries - uses a graph
representation of tabular (or relational) data to process tables with different
columns, string embeddings of entries and columns names to model an open
vocabulary, and a graph-attentional network to contextualize entries with
column names and neighboring entries. An extensive benchmark shows that CARTE
facilitates learning, outperforming a solid set of baselines including the best
tree-based models. CARTE also enables joint learning across tables with
unmatched columns, enhancing a small table with bigger ones. CARTE opens the
door to large pretrained models embarking information for tabular data.</div><div><a href='http://arxiv.org/abs/2402.16785v1'>2402.16785v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08867v1")'>MambaTab: A Simple Yet Effective Approach for Handling Tabular Data</div>
<div id='2401.08867v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T22:44:12Z</div><div>Authors: Md Atik Ahamed, Qiang Cheng</div><div style='padding-top: 10px; width: 80ex'>Tabular data remains ubiquitous across domains despite growing use of images
and texts for machine learning. While deep learning models like convolutional
neural networks and transformers achieve strong performance on tabular data,
they require extensive data preprocessing, tuning, and resources, limiting
accessibility and scalability. This work develops an innovative approach based
on a structured state-space model (SSM), MambaTab, for tabular data. SSMs have
strong capabilities for efficiently extracting effective representations from
data with long-range dependencies. MambaTab leverages Mamba, an emerging SSM
variant, for end-to-end supervised learning on tables. Compared to
state-of-the-art baselines, MambaTab delivers superior performance while
requiring significantly fewer parameters and minimal preprocessing, as
empirically validated on diverse benchmark datasets. MambaTab's efficiency,
scalability, generalizability, and predictive gains signify it as a
lightweight, "out-of-the-box" solution for diverse tabular data with promise
for enabling wider practical applications.</div><div><a href='http://arxiv.org/abs/2401.08867v1'>2401.08867v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01204v2")'>A Survey on Self-Supervised Learning for Non-Sequential Tabular Data</div>
<div id='2402.01204v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T08:17:41Z</div><div>Authors: Wei-Yao Wang, Wei-Wei Du, Derek Xu, Wei Wang, Wen-Chih Peng</div><div style='padding-top: 10px; width: 80ex'>Self-supervised learning (SSL) has been incorporated into many
state-of-the-art models in various domains, where SSL defines pretext tasks
based on unlabeled datasets to learn contextualized and robust representations.
Recently, SSL has been a new trend in exploring the representation learning
capability in the realm of tabular data, which is more challenging due to not
having explicit relations for learning descriptive representations. This survey
aims to systematically review and summarize the recent progress and challenges
of SSL for non-sequential tabular data (SSL4NS-TD). We first present a formal
definition of NS-TD and clarify its correlation to related studies. Then, these
approaches are categorized into three groups -- predictive learning,
contrastive learning, and hybrid learning, with their motivations and strengths
of representative methods within each direction. On top of this, application
issues of SSL4NS-TD are presented, including automatic data engineering,
cross-table transferability, and domain knowledge integration. In addition, we
elaborate on existing benchmarks and datasets for NS-TD applications to discuss
the performance of existing tabular models. Finally, we discuss the challenges
of SSL4NS-TD and provide potential directions for future research. We expect
our work to be useful in terms of encouraging more research on lowering the
barrier to entry SSL for the tabular domain and improving the foundations for
implicit tabular data.</div><div><a href='http://arxiv.org/abs/2402.01204v2'>2402.01204v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06367v1")'>FeatAug: Automatic Feature Augmentation From One-to-Many Relationship
  Tables</div>
<div id='2403.06367v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T01:44:14Z</div><div>Authors: Danrui Qi, Weiling Zheng, Jiannan Wang</div><div style='padding-top: 10px; width: 80ex'>Feature augmentation from one-to-many relationship tables is a critical but
challenging problem in ML model development. To augment good features, data
scientists need to come up with SQL queries manually, which is time-consuming.
Featuretools [1] is a widely used tool by the data science community to
automatically augment the training data by extracting new features from
relevant tables. It represents each feature as a group-by aggregation SQL query
on relevant tables and can automatically generate these SQL queries. However,
it does not include predicates in these queries, which significantly limits its
application in many real-world scenarios. To overcome this limitation, we
propose FEATAUG, a new feature augmentation framework that automatically
extracts predicate-aware SQL queries from one-to-many relationship tables. This
extension is not trivial because considering predicates will exponentially
increase the number of candidate queries. As a result, the original
Featuretools framework, which materializes all candidate queries, will not work
and needs to be redesigned. We formally define the problem and model it as a
hyperparameter optimization problem. We discuss how the Bayesian Optimization
can be applied here and propose a novel warm-up strategy to optimize it. To
make our algorithm more practical, we also study how to identify promising
attribute combinations for predicates. We show that how the beam search idea
can partially solve the problem and propose several techniques to further
optimize it. Our experiments on four real-world datasets demonstrate that
FeatAug extracts more effective features compared to Featuretools and other
baselines. The code is open-sourced at https://github.com/sfu-db/FeatAug</div><div><a href='http://arxiv.org/abs/2403.06367v1'>2403.06367v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.02334v2")'>Arithmetic Feature Interaction Is Necessary for Deep Tabular Learning</div>
<div id='2402.02334v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T04:07:39Z</div><div>Authors: Yi Cheng, Renjun Hu, Haochao Ying, Xing Shi, Jian Wu, Wei Lin</div><div style='padding-top: 10px; width: 80ex'>Until recently, the question of the effective inductive bias of deep models
on tabular data has remained unanswered. This paper investigates the hypothesis
that arithmetic feature interaction is necessary for deep tabular learning. To
test this point, we create a synthetic tabular dataset with a mild feature
interaction assumption and examine a modified transformer architecture enabling
arithmetical feature interactions, referred to as AMFormer. Results show that
AMFormer outperforms strong counterparts in fine-grained tabular data modeling,
data efficiency in training, and generalization. This is attributed to its
parallel additive and multiplicative attention operators and prompt-based
optimization, which facilitate the separation of tabular samples in an extended
space with arithmetically-engineered features. Our extensive experiments on
real-world data also validate the consistent effectiveness, efficiency, and
rationale of AMFormer, suggesting it has established a strong inductive bias
for deep learning on tabular data. Code is available at
https://github.com/aigc-apps/AMFormer.</div><div><a href='http://arxiv.org/abs/2402.02334v2'>2402.02334v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15238v1")'>Deep Learning with Tabular Data: A Self-supervised Approach</div>
<div id='2401.15238v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T23:12:41Z</div><div>Authors: Tirth Kiranbhai Vyas</div><div style='padding-top: 10px; width: 80ex'>We have described a novel approach for training tabular data using the
TabTransformer model with self-supervised learning. Traditional machine
learning models for tabular data, such as GBDT are being widely used though our
paper examines the effectiveness of the TabTransformer which is a Transformer
based model optimised specifically for tabular data. The TabTransformer
captures intricate relationships and dependencies among features in tabular
data by leveraging the self-attention mechanism of Transformers. We have used a
self-supervised learning approach in this study, where the TabTransformer
learns from unlabelled data by creating surrogate supervised tasks, eliminating
the need for the labelled data. The aim is to find the most effective
TabTransformer model representation of categorical and numerical features. To
address the challenges faced during the construction of various input settings
into the Transformers. Furthermore, a comparative analysis is also been
conducted to examine performance of the TabTransformer model against baseline
models such as MLP and supervised TabTransformer.
  The research has presented with a novel approach by creating various variants
of TabTransformer model namely, Binned-TT, Vanilla-MLP-TT, MLP- based-TT which
has helped to increase the effective capturing of the underlying relationship
between various features of the tabular dataset by constructing optimal inputs.
And further we have employed a self-supervised learning approach in the form of
a masking-based unsupervised setting for tabular data. The findings shed light
on the best way to represent categorical and numerical features, emphasizing
the TabTransormer performance when compared to established machine learning
models and other self-supervised learning methods.</div><div><a href='http://arxiv.org/abs/2401.15238v1'>2401.15238v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14335v1")'>HyperFast: Instant Classification for Tabular Data</div>
<div id='2402.14335v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T07:07:16Z</div><div>Authors: David Bonet, Daniel Mas Montserrat, Xavier Giró-i-Nieto, Alexander G. Ioannidis</div><div style='padding-top: 10px; width: 80ex'>Training deep learning models and performing hyperparameter tuning can be
computationally demanding and time-consuming. Meanwhile, traditional machine
learning methods like gradient-boosting algorithms remain the preferred choice
for most tabular data applications, while neural network alternatives require
extensive hyperparameter tuning or work only in toy datasets under limited
settings. In this paper, we introduce HyperFast, a meta-trained hypernetwork
designed for instant classification of tabular data in a single forward pass.
HyperFast generates a task-specific neural network tailored to an unseen
dataset that can be directly used for classification inference, removing the
need for training a model. We report extensive experiments with OpenML and
genomic data, comparing HyperFast to competing tabular data neural networks,
traditional ML methods, AutoML systems, and boosting machines. HyperFast shows
highly competitive results, while being significantly faster. Additionally, our
approach demonstrates robust adaptability across a variety of classification
tasks with little to no fine-tuning, positioning HyperFast as a strong solution
for numerous applications and rapid model deployment. HyperFast introduces a
promising paradigm for fast classification, with the potential to substantially
decrease the computational burden of deep learning. Our code, which offers a
scikit-learn-like interface, along with the trained HyperFast model, can be
found at https://github.com/AI-sandbox/HyperFast.</div><div><a href='http://arxiv.org/abs/2402.14335v1'>2402.14335v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.15123v1")'>Quantification using Permutation-Invariant Networks based on Histograms</div>
<div id='2403.15123v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T11:25:38Z</div><div>Authors: Olaya Pérez-Mon, Alejandro Moreo, Juan José del Coz, Pablo González</div><div style='padding-top: 10px; width: 80ex'>Quantification, also known as class prevalence estimation, is the supervised
learning task in which a model is trained to predict the prevalence of each
class in a given bag of examples. This paper investigates the application of
deep neural networks to tasks of quantification in scenarios where it is
possible to apply a symmetric supervised approach that eliminates the need for
classification as an intermediary step, directly addressing the quantification
problem. Additionally, it discusses existing permutation-invariant layers
designed for set processing and assesses their suitability for quantification.
In light of our analysis, we propose HistNetQ, a novel neural architecture that
relies on a permutation-invariant representation based on histograms that is
specially suited for quantification problems. Our experiments carried out in
the only quantification competition held to date, show that HistNetQ
outperforms other deep neural architectures devised for set processing, as well
as the state-of-the-art quantification methods. Furthermore, HistNetQ offers
two significant advantages over traditional quantification methods: i) it does
not require the labels of the training examples but only the prevalence values
of a collection of training bags, making it applicable to new scenarios; and
ii) it is able to optimize any custom quantification-oriented loss function.</div><div><a href='http://arxiv.org/abs/2403.15123v1'>2403.15123v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.17052v1")'>Making Parametric Anomaly Detection on Tabular Data Non-Parametric Again</div>
<div id='2401.17052v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T14:33:18Z</div><div>Authors: Hugo Thimonier, Fabrice Popineau, Arpad Rimmel, Bich-Liên Doan</div><div style='padding-top: 10px; width: 80ex'>Deep learning for tabular data has garnered increasing attention in recent
years, yet employing deep models for structured data remains challenging. While
these models excel with unstructured data, their efficacy with structured data
has been limited. Recent research has introduced retrieval-augmented models to
address this gap, demonstrating promising results in supervised tasks such as
classification and regression. In this work, we investigate using
retrieval-augmented models for anomaly detection on tabular data. We propose a
reconstruction-based approach in which a transformer model learns to
reconstruct masked features of \textit{normal} samples. We test the
effectiveness of KNN-based and attention-based modules to select relevant
samples to help in the reconstruction process of the target sample. Our
experiments on a benchmark of 31 tabular datasets reveal that augmenting this
reconstruction-based anomaly detection (AD) method with non-parametric
relationships via retrieval modules may significantly boost performance.</div><div><a href='http://arxiv.org/abs/2401.17052v1'>2401.17052v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07502v1")'>ClusterTabNet: Supervised clustering method for table detection and
  table structure recognition</div>
<div id='2402.07502v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T09:10:24Z</div><div>Authors: Marek Polewczyk, Marco Spinaci</div><div style='padding-top: 10px; width: 80ex'>We present a novel deep-learning-based method to cluster words in documents
which we apply to detect and recognize tables given the OCR output. We
interpret table structure bottom-up as a graph of relations between pairs of
words (belonging to the same row, column, header, as well as to the same table)
and use a transformer encoder model to predict its adjacency matrix. We
demonstrate the performance of our method on the PubTables-1M dataset as well
as PubTabNet and FinTabNet datasets. Compared to the current state-of-the-art
detection methods such as DETR and Faster R-CNN, our method achieves similar or
better accuracy, while requiring a significantly smaller model.</div><div><a href='http://arxiv.org/abs/2402.07502v1'>2402.07502v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.07787v1")'>Improving OCR Quality in 19th Century Historical Documents Using a
  Combined Machine Learning Based Approach</div>
<div id='2401.07787v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T15:53:13Z</div><div>Authors: David Fleischhacker, Wolfgang Goederle, Roman Kern</div><div style='padding-top: 10px; width: 80ex'>This paper addresses a major challenge to historical research on the 19th
century. Large quantities of sources have become digitally available for the
first time, while extraction techniques are lagging behind. Therefore, we
researched machine learning (ML) models to recognise and extract complex data
structures in a high-value historical primary source, the Schematismus. It
records every single person in the Habsburg civil service above a certain
hierarchical level between 1702 and 1918 and documents the genesis of the
central administration over two centuries. Its complex and intricate structure
as well as its enormous size have so far made any more comprehensive analysis
of the administrative and social structure of the later Habsburg Empire on the
basis of this source impossible. We pursued two central objectives: Primarily,
the improvement of the OCR quality, for which we considered an improved
structure recognition to be essential; in the further course, it turned out
that this also made the extraction of the data structure possible. We chose
Faster R-CNN as base for the ML architecture for structure recognition. In
order to obtain the required amount of training data quickly and economically,
we synthesised Hof- und Staatsschematismus-style data, which we used to train
our model. The model was then fine-tuned with a smaller set of manually
annotated historical source data. We then used Tesseract-OCR, which was further
optimised for the style of our documents, to complete the combined structure
extraction and OCR process. Results show a significant decrease in the two
standard parameters of OCR-performance, WER and CER (where lower values are
better). Combined structure detection and fine-tuned OCR improved CER and WER
values by remarkable 71.98 percent (CER) respectively 52.49 percent (WER).</div><div><a href='http://arxiv.org/abs/2401.07787v1'>2401.07787v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.14551v1")'>CLCE: An Approach to Refining Cross-Entropy and Contrastive Learning for
  Optimized Learning Fusion</div>
<div id='2402.14551v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T13:45:01Z</div><div>Authors: Zijun Long, George Killick, Lipeng Zhuang, Gerardo Aragon-Camarasa, Zaiqiao Meng, Richard Mccreadie</div><div style='padding-top: 10px; width: 80ex'>State-of-the-art pre-trained image models predominantly adopt a two-stage
approach: initial unsupervised pre-training on large-scale datasets followed by
task-specific fine-tuning using Cross-Entropy loss~(CE). However, it has been
demonstrated that CE can compromise model generalization and stability. While
recent works employing contrastive learning address some of these limitations
by enhancing the quality of embeddings and producing better decision
boundaries, they often overlook the importance of hard negative mining and rely
on resource intensive and slow training using large sample batches. To counter
these issues, we introduce a novel approach named CLCE, which integrates
Label-Aware Contrastive Learning with CE. Our approach not only maintains the
strengths of both loss functions but also leverages hard negative mining in a
synergistic way to enhance performance. Experimental results demonstrate that
CLCE significantly outperforms CE in Top-1 accuracy across twelve benchmarks,
achieving gains of up to 3.52% in few-shot learning scenarios and 3.41% in
transfer learning settings with the BEiT-3 model. Importantly, our proposed
CLCE approach effectively mitigates the dependency of contrastive learning on
large batch sizes such as 4096 samples per batch, a limitation that has
previously constrained the application of contrastive learning in
budget-limited hardware environments.</div><div><a href='http://arxiv.org/abs/2402.14551v1'>2402.14551v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.02656v1")'>GTA: Guided Transfer of Spatial Attention from Object-Centric
  Representations</div>
<div id='2401.02656v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T06:24:41Z</div><div>Authors: SeokHyun Seo, Jinwoo Hong, JungWoo Chae, Kyungyul Kim, Sangheum Hwang</div><div style='padding-top: 10px; width: 80ex'>Utilizing well-trained representations in transfer learning often results in
superior performance and faster convergence compared to training from scratch.
However, even if such good representations are transferred, a model can easily
overfit the limited training dataset and lose the valuable properties of the
transferred representations. This phenomenon is more severe in ViT due to its
low inductive bias. Through experimental analysis using attention maps in ViT,
we observe that the rich representations deteriorate when trained on a small
dataset. Motivated by this finding, we propose a novel and simple
regularization method for ViT called Guided Transfer of spatial Attention
(GTA). Our proposed method regularizes the self-attention maps between the
source and target models. A target model can fully exploit the knowledge
related to object localization properties through this explicit regularization.
Our experimental results show that the proposed GTA consistently improves the
accuracy across five benchmark datasets especially when the number of training
data is small.</div><div><a href='http://arxiv.org/abs/2401.02656v1'>2401.02656v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12712v1")'>Addressing Source Scale Bias via Image Warping for Domain Adaptation</div>
<div id='2403.12712v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T13:19:41Z</div><div>Authors: Shen Zheng, Anurag Ghosh, Srinivasa G. Narasimhan</div><div style='padding-top: 10px; width: 80ex'>In visual recognition, scale bias is a key challenge due to the imbalance of
object and image size distribution inherent in real scene datasets.
Conventional solutions involve injecting scale invariance priors, oversampling
the dataset at different scales during training, or adjusting scale at
inference. While these strategies mitigate scale bias to some extent, their
ability to adapt across diverse datasets is limited. Besides, they increase
computational load during training and latency during inference. In this work,
we use adaptive attentional processing -- oversampling salient object regions
by warping images in-place during training. Discovering that shifting the
source scale distribution improves backbone features, we developed a
instance-level warping guidance aimed at object region sampling to mitigate
source scale bias in domain adaptation. Our approach improves adaptation across
geographies, lighting and weather conditions, is agnostic to the task, domain
adaptation algorithm, saliency guidance, and underlying model architecture.
Highlights include +6.1 mAP50 for BDD100K Clear $\rightarrow$ DENSE Foggy, +3.7
mAP50 for BDD100K Day $\rightarrow$ Night, +3.0 mAP50 for BDD100K Clear
$\rightarrow$ Rainy, and +6.3 mIoU for Cityscapes $\rightarrow$ ACDC. Our
approach adds minimal memory during training and has no additional latency at
inference time. Please see Appendix for more results and analysis.</div><div><a href='http://arxiv.org/abs/2403.12712v1'>2403.12712v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18614v1")'>Deep Neural Network Models Trained With A Fixed Random Classifier
  Transfer Better Across Domains</div>
<div id='2402.18614v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T15:52:30Z</div><div>Authors: Hafiz Tiomoko Ali, Umberto Michieli, Ji Joong Moon, Daehyun Kim, Mete Ozay</div><div style='padding-top: 10px; width: 80ex'>The recently discovered Neural collapse (NC) phenomenon states that the
last-layer weights of Deep Neural Networks (DNN), converge to the so-called
Equiangular Tight Frame (ETF) simplex, at the terminal phase of their training.
This ETF geometry is equivalent to vanishing within-class variability of the
last layer activations. Inspired by NC properties, we explore in this paper the
transferability of DNN models trained with their last layer weight fixed
according to ETF. This enforces class separation by eliminating class
covariance information, effectively providing implicit regularization. We show
that DNN models trained with such a fixed classifier significantly improve
transfer performance, particularly on out-of-domain datasets. On a broad range
of fine-grained image classification datasets, our approach outperforms i)
baseline methods that do not perform any covariance regularization (up to 22%),
as well as ii) methods that explicitly whiten covariance of activations
throughout training (up to 19%). Our findings suggest that DNNs trained with
fixed ETF classifiers offer a powerful mechanism for improving transfer
learning across domains.</div><div><a href='http://arxiv.org/abs/2402.18614v1'>2402.18614v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14356v1")'>DomainLab: A modular Python package for domain generalization in deep
  learning</div>
<div id='2403.14356v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T12:35:46Z</div><div>Authors: Xudong Sun, Carla Feistner, Alexej Gossmann, George Schwarz, Rao Muhammad Umer, Lisa Beer, Patrick Rockenschaub, Rahul Babu Shrestha, Armin Gruber, Nutan Chen, Sayedali Shetab Boushehri, Florian Buettner, Carsten Marr</div><div style='padding-top: 10px; width: 80ex'>Poor generalization performance caused by distribution shifts in unseen
domains often hinders the trustworthy deployment of deep neural networks. Many
domain generalization techniques address this problem by adding a domain
invariant regularization loss terms during training. However, there is a lack
of modular software that allows users to combine the advantages of different
methods with minimal effort for reproducibility. DomainLab is a modular Python
package for training user specified neural networks with composable
regularization loss terms. Its decoupled design allows the separation of neural
networks from regularization loss construction. Hierarchical combinations of
neural networks, different domain generalization methods, and associated
hyperparameters, can all be specified together with other experimental setup in
a single configuration file. Hierarchical combinations of neural networks,
different domain generalization methods, and associated hyperparameters, can
all be specified together with other experimental setup in a single
configuration file. In addition, DomainLab offers powerful benchmarking
functionality to evaluate the generalization performance of neural networks in
out-of-distribution data. The package supports running the specified benchmark
on an HPC cluster or on a standalone machine. The package is well tested with
over 95 percent coverage and well documented. From the user perspective, it is
closed to modification but open to extension. The package is under the MIT
license, and its source code, tutorial and documentation can be found at
https://github.com/marrlab/DomainLab.</div><div><a href='http://arxiv.org/abs/2403.14356v1'>2403.14356v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12683v1")'>TorchCP: A Library for Conformal Prediction based on PyTorch</div>
<div id='2402.12683v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T03:14:47Z</div><div>Authors: Hongxin Wei, Jianguo Huang</div><div style='padding-top: 10px; width: 80ex'>TorchCP is a Python toolbox for conformal prediction research on deep
learning models. It contains various implementations for posthoc and training
methods for classification and regression tasks (including multi-dimension
output). TorchCP is built on PyTorch (Paszke et al., 2019) and leverages the
advantages of matrix computation to provide concise and efficient inference
implementations. The code is licensed under the LGPL license and is
open-sourced at $\href{https://github.com/ml-stat-Sustech/TorchCP}{\text{this
https URL}}$.</div><div><a href='http://arxiv.org/abs/2402.12683v1'>2402.12683v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.18884v1")'>Supervised Contrastive Representation Learning: Landscape Analysis with
  Unconstrained Features</div>
<div id='2402.18884v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T06:02:45Z</div><div>Authors: Tina Behnia, Christos Thrampoulidis</div><div style='padding-top: 10px; width: 80ex'>Recent findings reveal that over-parameterized deep neural networks, trained
beyond zero training-error, exhibit a distinctive structural pattern at the
final layer, termed as Neural-collapse (NC). These results indicate that the
final hidden-layer outputs in such networks display minimal within-class
variations over the training set. While existing research extensively
investigates this phenomenon under cross-entropy loss, there are fewer studies
focusing on its contrastive counterpart, supervised contrastive (SC) loss.
Through the lens of NC, this paper employs an analytical approach to study the
solutions derived from optimizing the SC loss. We adopt the unconstrained
features model (UFM) as a representative proxy for unveiling NC-related
phenomena in sufficiently over-parameterized deep networks. We show that,
despite the non-convexity of SC loss minimization, all local minima are global
minima. Furthermore, the minimizer is unique (up to a rotation). We prove our
results by formalizing a tight convex relaxation of the UFM. Finally, through
this convex formulation, we delve deeper into characterizing the properties of
global solutions under label-imbalanced training data.</div><div><a href='http://arxiv.org/abs/2402.18884v1'>2402.18884v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02058v1")'>Neural Collapse for Cross-entropy Class-Imbalanced Learning with
  Unconstrained ReLU Feature Model</div>
<div id='2401.02058v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T04:53:31Z</div><div>Authors: Hien Dang, Tho Tran, Tan Nguyen, Nhat Ho</div><div style='padding-top: 10px; width: 80ex'>The current paradigm of training deep neural networks for classification
tasks includes minimizing the empirical risk that pushes the training loss
value towards zero, even after the training error has been vanished. In this
terminal phase of training, it has been observed that the last-layer features
collapse to their class-means and these class-means converge to the vertices of
a simplex Equiangular Tight Frame (ETF). This phenomenon is termed as Neural
Collapse (NC). To theoretically understand this phenomenon, recent works employ
a simplified unconstrained feature model to prove that NC emerges at the global
solutions of the training problem. However, when the training dataset is
class-imbalanced, some NC properties will no longer be true. For example, the
class-means geometry will skew away from the simplex ETF when the loss
converges. In this paper, we generalize NC to imbalanced regime for
cross-entropy loss under the unconstrained ReLU feature model. We prove that,
while the within-class features collapse property still holds in this setting,
the class-means will converge to a structure consisting of orthogonal vectors
with different lengths. Furthermore, we find that the classifier weights are
aligned to the scaled and centered class-means with scaling factors depend on
the number of training samples of each class, which generalizes NC in the
class-balanced setting. We empirically prove our results through experiments on
practical architectures and dataset.</div><div><a href='http://arxiv.org/abs/2401.02058v1'>2401.02058v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02041v2")'>$α$-Divergence Loss Function for Neural Density Ratio Estimation</div>
<div id='2402.02041v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T05:33:01Z</div><div>Authors: Yoshiaki Kitazawa</div><div style='padding-top: 10px; width: 80ex'>Recently, neural networks have produced state-of-the-art results for
density-ratio estimation (DRE), a fundamental technique in machine learning.
However, existing methods bear optimization issues that arise from the loss
functions of DRE: a large sample requirement of Kullback--Leibler
(KL)-divergence, vanishing of train loss gradients, and biased gradients of the
loss functions. Thus, an $\alpha$-divergence loss function ($\alpha$-Div) that
offers concise implementation and stable optimization is proposed in this
paper. Furthermore, technical justifications for the proposed loss function are
presented. The stability of the proposed loss function is empirically
demonstrated and the estimation accuracy of DRE tasks is investigated.
Additionally, this study presents a sample requirement for DRE using the
proposed loss function in terms of the upper bound of $L_1$ error, which
connects a curse of dimensionality as a common problem in high-dimensional DRE
tasks.</div><div><a href='http://arxiv.org/abs/2402.02041v2'>2402.02041v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16788v1")'>Why Transformers Need Adam: A Hessian Perspective</div>
<div id='2402.16788v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T18:01:41Z</div><div>Authors: Yushun Zhang, Congliang Chen, Tian Ding, Ziniu Li, Ruoyu Sun, Zhi-Quan Luo</div><div style='padding-top: 10px; width: 80ex'>SGD performs worse than Adam by a significant margin on Transformers, but the
reason remains unclear. In this work, we provide an explanation of SGD's
failure on Transformers through the lens of Hessian: (i) Transformers are
``heterogeneous'': the Hessian spectrum across parameter blocks vary
dramatically, a phenomenon we call ``block heterogeneity"; (ii) Heterogeneity
hampers SGD: SGD performs badly on problems with block heterogeneity. To
validate that heterogeneity hampers SGD, we check various Transformers, CNNs,
MLPs, and quadratic problems, and find that SGD works well on problems without
block heterogeneity but performs badly when the heterogeneity exists. Our
initial theoretical analysis indicates that SGD fails because it applies one
single learning rate for all blocks, which cannot handle the heterogeneity
among blocks. The failure could be rescued if we could assign different
learning rates across blocks, as designed in Adam.</div><div><a href='http://arxiv.org/abs/2402.16788v1'>2402.16788v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18824v1")'>Batch size invariant Adam</div>
<div id='2402.18824v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T03:16:47Z</div><div>Authors: Xi Wang, Laurence Aitchison</div><div style='padding-top: 10px; width: 80ex'>We propose a batch size invariant version of Adam, for use in large-scale,
distributed settings, in which the mini-batch is divided into micro-batches
which are distributed among worker nodes. For the v term, standard Adam first
computes the average over micro-batch gradients, then squares, while in the
batch size invariant Adam proposed here, we first square the micro-batch
gradients, then average. Previous work (e.g. Malladi et al. 2022) used an
alternative approach that involved a square-root scaling of the learning rate,
but this approach requires strong assumptions to work; in particular that the
gradient variance dominates the square of the expected gradient. In contrast,
the approach proposed here gives batch size invariance without this assumption.
We confirm that in practice our scheme gives batch size invariance in a much
larger range of scenarios than the previous approach.</div><div><a href='http://arxiv.org/abs/2402.18824v1'>2402.18824v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08909v2")'>Leveraging Gradients for Unsupervised Accuracy Estimation under
  Distribution Shift</div>
<div id='2401.08909v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T01:33:23Z</div><div>Authors: Renchunzi Xie, Ambroise Odonnat, Vasilii Feofanov, Ievgen Redko, Jianfeng Zhang, Bo An</div><div style='padding-top: 10px; width: 80ex'>Estimating test accuracy without access to the ground-truth test labels under
varying test environments is a challenging, yet extremely important problem in
the safe deployment of machine learning algorithms. Existing works rely on the
information from either the outputs or the extracted features of neural
networks to formulate an estimation score correlating with the ground-truth
test accuracy. In this paper, we investigate--both empirically and
theoretically--how the information provided by the gradients can be predictive
of the ground-truth test accuracy even under a distribution shift.
Specifically, we use the norm of classification-layer gradients, backpropagated
from the cross-entropy loss after only one gradient step over test data. Our
key idea is that the model should be adjusted with a higher magnitude of
gradients when it does not generalize to the test dataset with a distribution
shift. We provide theoretical insights highlighting the main ingredients of
such an approach ensuring its empirical success. Extensive experiments
conducted on diverse distribution shifts and model structures demonstrate that
our method significantly outperforms state-of-the-art algorithms.</div><div><a href='http://arxiv.org/abs/2401.08909v2'>2401.08909v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14715v1")'>Understanding Why Label Smoothing Degrades Selective Classification and
  How to Fix It</div>
<div id='2403.14715v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T06:46:24Z</div><div>Authors: Guoxuan Xia, Olivier Laurent, Gianni Franchi, Christos-Savvas Bouganis</div><div style='padding-top: 10px; width: 80ex'>Label smoothing (LS) is a popular regularisation method for training deep
neural network classifiers due to its effectiveness in improving test accuracy
and its simplicity in implementation. "Hard" one-hot labels are "smoothed" by
uniformly distributing probability mass to other classes, reducing overfitting.
In this work, we reveal that LS negatively affects selective classification
(SC) - where the aim is to reject misclassifications using a model's predictive
uncertainty. We first demonstrate empirically across a range of tasks and
architectures that LS leads to a consistent degradation in SC. We then explain
this by analysing logit-level gradients, showing that LS exacerbates
overconfidence and underconfidence by regularising the max logit more when the
probability of error is low, and less when the probability of error is high.
This elucidates previously reported experimental results where strong
classifiers underperform in SC. We then demonstrate the empirical effectiveness
of logit normalisation for recovering lost SC performance caused by LS.
Furthermore, based on our gradient analysis, we explain why such normalisation
is effective. We will release our code shortly.</div><div><a href='http://arxiv.org/abs/2403.14715v1'>2403.14715v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08151v1")'>Gradient-flow adaptive importance sampling for Bayesian leave one out
  cross-validation for sigmoidal classification models</div>
<div id='2402.08151v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T01:03:39Z</div><div>Authors: Joshua C Chang, Xiangting Li, Shixin Xu, Hao-Ren Yao, Julia Porcino, Carson Chow</div><div style='padding-top: 10px; width: 80ex'>We introduce a set of gradient-flow-guided adaptive importance sampling (IS)
transformations to stabilize Monte-Carlo approximations of point-wise leave one
out cross-validated (LOO) predictions for Bayesian classification models. One
can leverage this methodology for assessing model generalizability by for
instance computing a LOO analogue to the AIC or computing LOO ROC/PRC curves
and derived metrics like the AUROC and AUPRC. By the calculus of variations and
gradient flow, we derive two simple nonlinear single-step transformations that
utilize gradient information to shift a model's pre-trained full-data posterior
closer to the target LOO posterior predictive distributions. In doing so, the
transformations stabilize importance weights. Because the transformations
involve the gradient of the likelihood function, the resulting Monte Carlo
integral depends on Jacobian determinants with respect to the model Hessian. We
derive closed-form exact formulae for these Jacobian determinants in the cases
of logistic regression and shallow ReLU-activated artificial neural networks,
and provide a simple approximation that sidesteps the need to compute full
Hessian matrices and their spectra. We test the methodology on an $n\ll p$
dataset that is known to produce unstable LOO IS weights.</div><div><a href='http://arxiv.org/abs/2402.08151v1'>2402.08151v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14339v1")'>$\nabla τ$: Gradient-based and Task-Agnostic machine Unlearning</div>
<div id='2403.14339v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T12:11:26Z</div><div>Authors: Daniel Trippa, Cesare Campagnano, Maria Sofia Bucarelli, Gabriele Tolomei, Fabrizio Silvestri</div><div style='padding-top: 10px; width: 80ex'>Machine Unlearning, the process of selectively eliminating the influence of
certain data examples used during a model's training, has gained significant
attention as a means for practitioners to comply with recent data protection
regulations. However, existing unlearning methods face critical drawbacks,
including their prohibitively high cost, often associated with a large number
of hyperparameters, and the limitation of forgetting only relatively small data
portions. This often makes retraining the model from scratch a quicker and more
effective solution. In this study, we introduce Gradient-based and
Task-Agnostic machine Unlearning ($\nabla \tau$), an optimization framework
designed to remove the influence of a subset of training data efficiently. It
applies adaptive gradient ascent to the data to be forgotten while using
standard gradient descent for the remaining data. $\nabla \tau$ offers multiple
benefits over existing approaches. It enables the unlearning of large sections
of the training dataset (up to 30%). It is versatile, supporting various
unlearning tasks (such as subset forgetting or class removal) and applicable
across different domains (images, text, etc.). Importantly, $\nabla \tau$
requires no hyperparameter adjustments, making it a more appealing option than
retraining the model from scratch. We evaluate our framework's effectiveness
using a set of well-established Membership Inference Attack metrics,
demonstrating up to 10% enhancements in performance compared to
state-of-the-art methods without compromising the original model's accuracy.</div><div><a href='http://arxiv.org/abs/2403.14339v1'>2403.14339v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01199v1")'>JMA: a General Algorithm to Craft Nearly Optimal Targeted Adversarial
  Example</div>
<div id='2401.01199v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T13:03:29Z</div><div>Authors: Benedetta Tondi, Wei Guo, Mauro Barni</div><div style='padding-top: 10px; width: 80ex'>Most of the approaches proposed so far to craft targeted adversarial examples
against Deep Learning classifiers are highly suboptimal and typically rely on
increasing the likelihood of the target class, thus implicitly focusing on
one-hot encoding settings. In this paper, we propose a more general,
theoretically sound, targeted attack that resorts to the minimization of a
Jacobian-induced MAhalanobis distance (JMA) term, taking into account the
effort (in the input space) required to move the latent space representation of
the input sample in a given direction. The minimization is solved by exploiting
the Wolfe duality theorem, reducing the problem to the solution of a
Non-Negative Least Square (NNLS) problem. The proposed algorithm provides an
optimal solution to a linearized version of the adversarial example problem
originally introduced by Szegedy et al. \cite{szegedy2013intriguing}. The
experiments we carried out confirm the generality of the proposed attack which
is proven to be effective under a wide variety of output encoding schemes.
Noticeably, the JMA attack is also effective in a multi-label classification
scenario, being capable to induce a targeted modification of up to half the
labels in a complex multilabel classification scenario with 20 labels, a
capability that is out of reach of all the attacks proposed so far. As a
further advantage, the JMA attack usually requires very few iterations, thus
resulting more efficient than existing methods.</div><div><a href='http://arxiv.org/abs/2401.01199v1'>2401.01199v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06014v1")'>Hard-label based Small Query Black-box Adversarial Attack</div>
<div id='2403.06014v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-09T21:26:22Z</div><div>Authors: Jeonghwan Park, Paul Miller, Niall McLaughlin</div><div style='padding-top: 10px; width: 80ex'>We consider the hard label based black box adversarial attack setting which
solely observes predicted classes from the target model. Most of the attack
methods in this setting suffer from impractical number of queries required to
achieve a successful attack. One approach to tackle this drawback is utilising
the adversarial transferability between white box surrogate models and black
box target model. However, the majority of the methods adopting this approach
are soft label based to take the full advantage of zeroth order optimisation.
Unlike mainstream methods, we propose a new practical setting of hard label
based attack with an optimisation process guided by a pretrained surrogate
model. Experiments show the proposed method significantly improves the query
efficiency of the hard label based black-box attack across various target model
architectures. We find the proposed method achieves approximately 5 times
higher attack success rate compared to the benchmarks, especially at the small
query budgets as 100 and 250.</div><div><a href='http://arxiv.org/abs/2403.06014v1'>2403.06014v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.08793v1")'>Neural Loss Function Evolution for Large-Scale Image Classifier
  Convolutional Neural Networks</div>
<div id='2403.08793v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T17:21:28Z</div><div>Authors: Brandon Morgan, Dean Hougen</div><div style='padding-top: 10px; width: 80ex'>For classification, neural networks typically learn by minimizing
cross-entropy, but are evaluated and compared using accuracy. This disparity
suggests neural loss function search (NLFS), the search for a drop-in
replacement loss function of cross-entropy for neural networks. We apply NLFS
to image classifier convolutional neural networks. We propose a new search
space for NLFS that encourages more diverse loss functions to be explored, and
a surrogate function that accurately transfers to large-scale convolutional
neural networks. We search the space using regularized evolution, a
mutation-only aging genetic algorithm. After evolution and a proposed loss
function elimination protocol, we transferred the final loss functions across
multiple architectures, datasets, and image augmentation techniques to assess
generalization. In the end, we discovered three new loss functions, called
NeuroLoss1, NeuroLoss2, and NeuroLoss3 that were able to outperform
cross-entropy in terms of a higher mean test accuracy as a simple drop-in
replacement loss function across the majority of experiments.</div><div><a href='http://arxiv.org/abs/2403.08793v1'>2403.08793v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13212v1")'>AdCorDA: Classifier Refinement via Adversarial Correction and Domain
  Adaptation</div>
<div id='2401.13212v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T03:49:51Z</div><div>Authors: Lulan Shen, Ali Edalati, Brett Meyer, Warren Gross, James J. Clark</div><div style='padding-top: 10px; width: 80ex'>This paper describes a simple yet effective technique for refining a
pretrained classifier network. The proposed AdCorDA method is based on
modification of the training set and making use of the duality between network
weights and layer inputs. We call this input space training. The method
consists of two stages - adversarial correction followed by domain adaptation.
Adversarial correction uses adversarial attacks to correct incorrect
training-set classifications. The incorrectly classified samples of the
training set are removed and replaced with the adversarially corrected samples
to form a new training set, and then, in the second stage, domain adaptation is
performed back to the original training set. Extensive experimental validations
show significant accuracy boosts of over 5% on the CIFAR-100 dataset. The
technique can be straightforwardly applied to refinement of weight-quantized
neural networks, where experiments show substantial enhancement in performance
over the baseline. The adversarial correction technique also results in
enhanced robustness to adversarial attacks.</div><div><a href='http://arxiv.org/abs/2401.13212v1'>2401.13212v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15179v2")'>Advancing Parameter Efficiency in Fine-tuning via Representation Editing</div>
<div id='2402.15179v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T08:21:02Z</div><div>Authors: Muling Wu, Wenhao Liu, Xiaohua Wang, Tianlong Li, Changze Lv, Zixuan Ling, Jianhao Zhu, Cenyuan Zhang, Xiaoqing Zheng, Xuanjing Huang</div><div style='padding-top: 10px; width: 80ex'>Parameter Efficient Fine-Tuning (PEFT) has gained significant attention for
its ability to achieve competitive results while updating only a small subset
of trainable parameters. Despite the promising performance of current PEFT
methods, they present challenges in hyperparameter selection, such as
determining the rank of LoRA or Adapter, or specifying the length of soft
prompts. In addressing these challenges, we propose a novel approach to
fine-tuning neural models, termed Representation EDiting (RED), which scales
and biases the representation produced at each layer. RED substantially reduces
the number of trainable parameters by a factor of $25,700$ compared to full
parameter fine-tuning, and by a factor of $32$ compared to LoRA. Remarkably,
RED achieves comparable or superior results to full parameter fine-tuning and
other PEFT methods. Extensive experiments were conducted across models of
varying architectures and scales, including RoBERTa, GPT-2, T5, and Llama-2,
and the results demonstrate the efficiency and efficacy of RED, positioning it
as a promising PEFT approach for large neural models.</div><div><a href='http://arxiv.org/abs/2402.15179v2'>2402.15179v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03660v1")'>Cross-Task Linearity Emerges in the Pretraining-Finetuning Paradigm</div>
<div id='2402.03660v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-06T03:28:36Z</div><div>Authors: Zhanpeng Zhou, Zijun Chen, Yilan Chen, Bo Zhang, Junchi Yan</div><div style='padding-top: 10px; width: 80ex'>The pretraining-finetuning paradigm has become the prevailing trend in modern
deep learning. In this work, we discover an intriguing linear phenomenon in
models that are initialized from a common pretrained checkpoint and finetuned
on different tasks, termed as Cross-Task Linearity (CTL). Specifically, if we
linearly interpolate the weights of two finetuned models, the features in the
weight-interpolated model are approximately equal to the linear interpolation
of features in two finetuned models at each layer. Such cross-task linearity
has not been noted in peer literature. We provide comprehensive empirical
evidence supporting that CTL consistently occurs for finetuned models that
start from the same pretrained checkpoint. We conjecture that in the
pretraining-finetuning paradigm, neural networks essentially function as linear
maps, mapping from the parameter space to the feature space. Based on this
viewpoint, our study unveils novel insights into explaining model
merging/editing, particularly by translating operations from the parameter
space to the feature space. Furthermore, we delve deeper into the underlying
factors for the emergence of CTL, emphasizing the impact of pretraining.</div><div><a href='http://arxiv.org/abs/2402.03660v1'>2402.03660v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17440v1")'>Principled Architecture-aware Scaling of Hyperparameters</div>
<div id='2402.17440v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T11:52:49Z</div><div>Authors: Wuyang Chen, Junru Wu, Zhangyang Wang, Boris Hanin</div><div style='padding-top: 10px; width: 80ex'>Training a high-quality deep neural network requires choosing suitable
hyperparameters, which is a non-trivial and expensive process. Current works
try to automatically optimize or design principles of hyperparameters, such
that they can generalize to diverse unseen scenarios. However, most designs or
optimization methods are agnostic to the choice of network structures, and thus
largely ignore the impact of neural architectures on hyperparameters. In this
work, we precisely characterize the dependence of initializations and maximal
learning rates on the network architecture, which includes the network depth,
width, convolutional kernel size, and connectivity patterns. By pursuing every
parameter to be maximally updated with the same mean squared change in
pre-activations, we can generalize our initialization and learning rates across
MLPs (multi-layer perception) and CNNs (convolutional neural network) with
sophisticated graph topologies. We verify our principles with comprehensive
experiments. More importantly, our strategy further sheds light on advancing
current benchmarks for architecture design. A fair comparison of AutoML
algorithms requires accurate network rankings. However, we demonstrate that
network rankings can be easily changed by better training networks in
benchmarks with our architecture-aware learning rates and initialization.</div><div><a href='http://arxiv.org/abs/2402.17440v1'>2402.17440v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16438v1")'>Do deep neural networks utilize the weight space efficiently?</div>
<div id='2401.16438v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T21:51:49Z</div><div>Authors: Onur Can Koyun, Behçet Uğur Töreyin</div><div style='padding-top: 10px; width: 80ex'>Deep learning models like Transformers and Convolutional Neural Networks
(CNNs) have revolutionized various domains, but their parameter-intensive
nature hampers deployment in resource-constrained settings. In this paper, we
introduce a novel concept utilizes column space and row space of weight
matrices, which allows for a substantial reduction in model parameters without
compromising performance. Leveraging this paradigm, we achieve
parameter-efficient deep learning models.. Our approach applies to both
Bottleneck and Attention layers, effectively halving the parameters while
incurring only minor performance degradation. Extensive experiments conducted
on the ImageNet dataset with ViT and ResNet50 demonstrate the effectiveness of
our method, showcasing competitive performance when compared to traditional
models. This approach not only addresses the pressing demand for parameter
efficient deep learning solutions but also holds great promise for practical
deployment in real-world scenarios.</div><div><a href='http://arxiv.org/abs/2401.16438v1'>2401.16438v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12819v1")'>Dynamic Layer Tying for Parameter-Efficient Transformers</div>
<div id='2401.12819v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T14:53:20Z</div><div>Authors: Tamir David Hay, Lior Wolf</div><div style='padding-top: 10px; width: 80ex'>In the pursuit of reducing the number of trainable parameters in deep
transformer networks, we employ Reinforcement Learning to dynamically select
layers during training and tie them together. Every few iterations, the RL
agent is asked whether to train each layer $i$ independently or to copy the
weights of a previous layer $j&lt;i$. This facilitates weight sharing, reduces the
number of trainable parameters, and also serves as an effective regularization
technique. Experimental evaluations validate that our model modestly
outperforms the baseline transformer model with regard to perplexity and
drastically reduces the number of trainable parameters. In particular, the
memory consumption during training is up to one order of magnitude less than
the conventional training method.</div><div><a href='http://arxiv.org/abs/2401.12819v1'>2401.12819v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12237v1")'>Efficient Transformer-based Hyper-parameter Optimization for
  Resource-constrained IoT Environments</div>
<div id='2403.12237v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T20:35:35Z</div><div>Authors: Ibrahim Shaer, Soodeh Nikan, Abdallah Shami</div><div style='padding-top: 10px; width: 80ex'>The hyper-parameter optimization (HPO) process is imperative for finding the
best-performing Convolutional Neural Networks (CNNs). The automation process of
HPO is characterized by its sizable computational footprint and its lack of
transparency; both important factors in a resource-constrained Internet of
Things (IoT) environment. In this paper, we address these problems by proposing
a novel approach that combines transformer architecture and actor-critic
Reinforcement Learning (RL) model, TRL-HPO, equipped with multi-headed
attention that enables parallelization and progressive generation of layers.
These assumptions are founded empirically by evaluating TRL-HPO on the MNIST
dataset and comparing it with state-of-the-art approaches that build CNN models
from scratch. The results show that TRL-HPO outperforms the classification
results of these approaches by 6.8% within the same time frame, demonstrating
the efficiency of TRL-HPO for the HPO process. The analysis of the results
identifies the main culprit for performance degradation attributed to stacking
fully connected layers. This paper identifies new avenues for improving
RL-based HPO processes in resource-constrained environments.</div><div><a href='http://arxiv.org/abs/2403.12237v1'>2403.12237v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06922v1")'>Open RAN LSTM Traffic Prediction and Slice Management using Deep
  Reinforcement Learning</div>
<div id='2401.06922v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T22:43:07Z</div><div>Authors: Fatemeh Lotfi, Fatemeh Afghah</div><div style='padding-top: 10px; width: 80ex'>With emerging applications such as autonomous driving, smart cities, and
smart factories, network slicing has become an essential component of 5G and
beyond networks as a means of catering to a service-aware network. However,
managing different network slices while maintaining quality of services (QoS)
is a challenge in a dynamic environment. To address this issue, this paper
leverages the heterogeneous experiences of distributed units (DUs) in ORAN
systems and introduces a novel approach to ORAN slicing xApp using distributed
deep reinforcement learning (DDRL). Additionally, to enhance the
decision-making performance of the RL agent, a prediction rApp based on long
short-term memory (LSTM) is incorporated to provide additional information from
the dynamic environment to the xApp. Simulation results demonstrate significant
improvements in network performance, particularly in reducing QoS violations.
This emphasizes the importance of using the prediction rApp and distributed
actors' information jointly as part of a dynamic xApp.</div><div><a href='http://arxiv.org/abs/2401.06922v1'>2401.06922v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06538v1")'>Intelligent Data-Driven Architectural Features Orchestration for Network
  Slicing</div>
<div id='2401.06538v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T12:32:36Z</div><div>Authors: Rodrigo Moreira, Flavio de Oliveira Silva, Tereza Cristina Melo de Brito Carvalho, Joberto S. B. Martins</div><div style='padding-top: 10px; width: 80ex'>Network slicing is a crucial enabler and a trend for the Next Generation
Mobile Network (NGMN) and various other new systems like the Internet of
Vehicles (IoV) and Industrial IoT (IIoT). Orchestration and machine learning
are key elements with a crucial role in the network-slicing processes since the
NS process needs to orchestrate resources and functionalities, and machine
learning can potentially optimize the orchestration process. However, existing
network-slicing architectures lack the ability to define intelligent approaches
to orchestrate features and resources in the slicing process. This paper
discusses machine learning-based orchestration of features and capabilities in
network slicing architectures. Initially, the slice resource orchestration and
allocation in the slicing planning, configuration, commissioning, and operation
phases are analyzed. In sequence, we highlight the need for optimized
architectural feature orchestration and recommend using ML-embed agents,
federated learning intrinsic mechanisms for knowledge acquisition, and a
data-driven approach embedded in the network slicing architecture. We further
develop an architectural features orchestration case embedded in the SFI2
network slicing architecture. An attack prevention security mechanism is
developed for the SFI2 architecture using distributed embedded and cooperating
ML agents. The case presented illustrates the architectural feature's
orchestration process and benefits, highlighting its importance for the network
slicing process.</div><div><a href='http://arxiv.org/abs/2401.06538v1'>2401.06538v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.05525v1")'>Towards Safe Load Balancing based on Control Barrier Functions and Deep
  Reinforcement Learning</div>
<div id='2401.05525v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T19:43:12Z</div><div>Authors: Lam Dinh, Pham Tran Anh Quang, Jérémie Leguay</div><div style='padding-top: 10px; width: 80ex'>Deep Reinforcement Learning (DRL) algorithms have recently made significant
strides in improving network performance. Nonetheless, their practical use is
still limited in the absence of safe exploration and safe decision-making. In
the context of commercial solutions, reliable and safe-to-operate systems are
of paramount importance. Taking this problem into account, we propose a safe
learning-based load balancing algorithm for Software Defined-Wide Area Network
(SD-WAN), which is empowered by Deep Reinforcement Learning (DRL) combined with
a Control Barrier Function (CBF). It safely projects unsafe actions into
feasible ones during both training and testing, and it guides learning towards
safe policies. We successfully implemented the solution on GPU to accelerate
training by approximately 110x times and achieve model updates for on-policy
methods within a few seconds, making the solution practical. We show that our
approach delivers near-optimal Quality-of-Service (QoS performance in terms of
end-to-end delay while respecting safety requirements related to link capacity
constraints. We also demonstrated that on-policy learning based on Proximal
Policy Optimization (PPO) performs better than off-policy learning with Deep
Deterministic Policy Gradient (DDPG) when both are combined with a CBF for safe
load balancing.</div><div><a href='http://arxiv.org/abs/2401.05525v1'>2401.05525v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.10650v1")'>PALM: Pushing Adaptive Learning Rate Mechanisms for Continual Test-Time
  Adaptation</div>
<div id='2403.10650v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T19:35:10Z</div><div>Authors: Sarthak Kumar Maharana, Baoming Zhang, Yunhui Guo</div><div style='padding-top: 10px; width: 80ex'>Real-world vision models in dynamic environments face rapid shifts in domain
distributions, leading to decreased recognition performance. Continual
test-time adaptation (CTTA) directly adjusts a pre-trained source
discriminative model to these changing domains using test data. A highly
effective CTTA method involves applying layer-wise adaptive learning rates, and
selectively adapting pre-trained layers. However, it suffers from the poor
estimation of domain shift and the inaccuracies arising from the pseudo-labels.
In this work, we aim to overcome these limitations by identifying layers
through the quantification of model prediction uncertainty without relying on
pseudo-labels. We utilize the magnitude of gradients as a metric, calculated by
backpropagating the KL divergence between the softmax output and a uniform
distribution, to select layers for further adaptation. Subsequently, for the
parameters exclusively belonging to these selected layers, with the remaining
ones frozen, we evaluate their sensitivity in order to approximate the domain
shift, followed by adjusting their learning rates accordingly. Overall, this
approach leads to a more robust and stable optimization than prior approaches.
We conduct extensive image classification experiments on CIFAR-10C, CIFAR-100C,
and ImageNet-C and demonstrate the efficacy of our method against standard
benchmarks and prior methods.</div><div><a href='http://arxiv.org/abs/2403.10650v1'>2403.10650v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02683v1")'>Learning to Defer to a Population: A Meta-Learning Approach</div>
<div id='2403.02683v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T06:10:28Z</div><div>Authors: Dharmesh Tailor, Aditya Patra, Rajeev Verma, Putra Manggala, Eric Nalisnick</div><div style='padding-top: 10px; width: 80ex'>The learning to defer (L2D) framework allows autonomous systems to be safe
and robust by allocating difficult decisions to a human expert. All existing
work on L2D assumes that each expert is well-identified, and if any expert were
to change, the system should be re-trained. In this work, we alleviate this
constraint, formulating an L2D system that can cope with never-before-seen
experts at test-time. We accomplish this by using meta-learning, considering
both optimization- and model-based variants. Given a small context set to
characterize the currently available expert, our framework can quickly adapt
its deferral policy. For the model-based approach, we employ an attention
mechanism that is able to look for points in the context set that are similar
to a given test point, leading to an even more precise assessment of the
expert's abilities. In the experiments, we validate our methods on image
recognition, traffic sign detection, and skin lesion diagnosis benchmarks.</div><div><a href='http://arxiv.org/abs/2403.02683v1'>2403.02683v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.02998v1")'>Careful with that Scalpel: Improving Gradient Surgery with an EMA</div>
<div id='2402.02998v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T13:37:00Z</div><div>Authors: Yu-Guan Hsieh, James Thornton, Eugene Ndiaye, Michal Klein, Marco Cuturi, Pierre Ablin</div><div style='padding-top: 10px; width: 80ex'>Beyond minimizing a single training loss, many deep learning estimation
pipelines rely on an auxiliary objective to quantify and encourage desirable
properties of the model (e.g. performance on another dataset, robustness,
agreement with a prior). Although the simplest approach to incorporating an
auxiliary loss is to sum it with the training loss as a regularizer, recent
works have shown that one can improve performance by blending the gradients
beyond a simple sum; this is known as gradient surgery. We cast the problem as
a constrained minimization problem where the auxiliary objective is minimized
among the set of minimizers of the training loss. To solve this bilevel
problem, we follow a parameter update direction that combines the training loss
gradient and the orthogonal projection of the auxiliary gradient to the
training gradient. In a setting where gradients come from mini-batches, we
explain how, using a moving average of the training loss gradients, we can
carefully maintain this critical orthogonality property. We demonstrate that
our method, Bloop, can lead to much better performances on NLP and vision
experiments than other gradient surgery methods without EMA.</div><div><a href='http://arxiv.org/abs/2402.02998v1'>2402.02998v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08818v1")'>Corridor Geometry in Gradient-Based Optimization</div>
<div id='2402.08818v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T21:54:15Z</div><div>Authors: Benoit Dherin, Mihaela Rosca</div><div style='padding-top: 10px; width: 80ex'>We characterize regions of a loss surface as corridors when the continuous
curves of steepest descent -- the solutions of the gradient flow -- become
straight lines. We show that corridors provide insights into gradient-based
optimization, since corridors are exactly the regions where gradient descent
and the gradient flow follow the same trajectory, while the loss decreases
linearly. As a result, inside corridors there are no implicit regularization
effects or training instabilities that have been shown to occur due to the
drift between gradient descent and the gradient flow. Using the loss linear
decrease on corridors, we devise a learning rate adaptation scheme for gradient
descent; we call this scheme Corridor Learning Rate (CLR). The CLR formulation
coincides with a special case of Polyak step-size, discovered in the context of
convex optimization. The Polyak step-size has been shown recently to have also
good convergence properties for neural networks; we further confirm this here
with results on CIFAR-10 and ImageNet.</div><div><a href='http://arxiv.org/abs/2402.08818v1'>2402.08818v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.09135v1")'>Asynchronous Local-SGD Training for Language Modeling</div>
<div id='2401.09135v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T11:17:04Z</div><div>Authors: Bo Liu, Rachita Chhaparia, Arthur Douillard, Satyen Kale, Andrei A. Rusu, Jiajun Shen, Arthur Szlam, Marc'Aurelio Ranzato</div><div style='padding-top: 10px; width: 80ex'>Local stochastic gradient descent (Local-SGD), also referred to as federated
averaging, is an approach to distributed optimization where each device
performs more than one SGD update per communication. This work presents an
empirical study of {\it asynchronous} Local-SGD for training language models;
that is, each worker updates the global parameters as soon as it has finished
its SGD steps. We conduct a comprehensive investigation by examining how worker
hardware heterogeneity, model size, number of workers, and optimizer could
impact the learning performance. We find that with naive implementations,
asynchronous Local-SGD takes more iterations to converge than its synchronous
counterpart despite updating the (global) model parameters more frequently. We
identify momentum acceleration on the global parameters when worker gradients
are stale as a key challenge. We propose a novel method that utilizes a delayed
Nesterov momentum update and adjusts the workers' local training steps based on
their computation speed. This approach, evaluated with models up to 150M
parameters on the C4 dataset, matches the performance of synchronous Local-SGD
in terms of perplexity per update step, and significantly surpasses it in terms
of wall clock time.</div><div><a href='http://arxiv.org/abs/2401.09135v1'>2401.09135v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11592v2")'>Revisiting Zeroth-Order Optimization for Memory-Efficient LLM
  Fine-Tuning: A Benchmark</div>
<div id='2402.11592v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T14:08:48Z</div><div>Authors: Yihua Zhang, Pingzhi Li, Junyuan Hong, Jiaxiang Li, Yimeng Zhang, Wenqing Zheng, Pin-Yu Chen, Jason D. Lee, Wotao Yin, Mingyi Hong, Zhangyang Wang, Sijia Liu, Tianlong Chen</div><div style='padding-top: 10px; width: 80ex'>In the evolving landscape of natural language processing (NLP), fine-tuning
pre-trained Large Language Models (LLMs) with first-order (FO) optimizers like
SGD and Adam has become standard. Yet, as LLMs grow {in size}, the substantial
memory overhead from back-propagation (BP) for FO gradient computation presents
a significant challenge. Addressing this issue is crucial, especially for
applications like on-device training where memory efficiency is paramount. This
paper proposes a shift towards BP-free, zeroth-order (ZO) optimization as a
solution for reducing memory costs during LLM fine-tuning, building on the
initial concept introduced by MeZO. Unlike traditional ZO-SGD methods, our work
expands the exploration to a wider array of ZO optimization techniques, through
a comprehensive, first-of-its-kind benchmarking study across five LLM families
(Roberta, OPT, LLaMA, Vicuna, Mistral), three task complexities, and five
fine-tuning schemes. Our study unveils previously overlooked optimization
principles, highlighting the importance of task alignment, the role of the
forward gradient method, and the balance between algorithm complexity and
fine-tuning performance. We further introduce novel enhancements to ZO
optimization, including block-wise descent, hybrid training, and gradient
sparsity. Our study offers a promising direction for achieving further
memory-efficient LLM fine-tuning. Codes to reproduce all our experiments are at
https://github.com/ZO-Bench/ZO-LLM .</div><div><a href='http://arxiv.org/abs/2402.11592v2'>2402.11592v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.08822v1")'>LoRA-SP: Streamlined Partial Parameter Adaptation for Resource-Efficient
  Fine-Tuning of Large Language Models</div>
<div id='2403.08822v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T06:50:10Z</div><div>Authors: Yichao Wu, Yafei Xiang, Shuning Huo, Yulu Gong, Penghao Liang</div><div style='padding-top: 10px; width: 80ex'>In addressing the computational and memory demands of fine-tuning Large
Language Models(LLMs), we propose LoRA-SP(Streamlined Partial Parameter
Adaptation), a novel approach utilizing randomized half-selective parameter
freezing within the Low-Rank Adaptation(LoRA)framework. This method efficiently
balances pre-trained knowledge retention and adaptability for task-specific
optimizations. Through a randomized mechanism, LoRA-SP determines which
parameters to update or freeze, significantly reducing computational and memory
requirements without compromising model performance. We evaluated LoRA-SP
across several benchmark NLP tasks, demonstrating its ability to achieve
competitive performance with substantially lower resource consumption compared
to traditional full-parameter fine-tuning and other parameter-efficient
techniques. LoRA-SP innovative approach not only facilitates the deployment of
advanced NLP models in resource-limited settings but also opens new research
avenues into effective and efficient model adaptation strategies.</div><div><a href='http://arxiv.org/abs/2403.08822v1'>2403.08822v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14608v1")'>Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey</div>
<div id='2403.14608v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T17:55:50Z</div><div>Authors: Zeyu Han, Chao Gao, Jinyang Liu, Jeff, Zhang, Sai Qian Zhang</div><div style='padding-top: 10px; width: 80ex'>Large models represent a groundbreaking advancement in multiple application
fields, enabling remarkable achievements across various tasks. However, their
unprecedented scale comes with significant computational costs. These models,
often consisting of billions of parameters, require vast amounts of
computational resources for execution. Especially, the expansive scale and
computational demands pose considerable challenges when customizing them for
particular downstream tasks, particularly over the hardware platforms
constrained by computational capabilities. Parameter Efficient Fine-Tuning
(PEFT) provides a practical solution by efficiently adapt the large models over
the various downstream tasks. In particular, PEFT refers to the process of
adjusting the parameters of a pre-trained large models to adapt it to a
specific task while minimizing the number of additional parameters introduced
or computational resources required. This approach is particularly important
when dealing with large language models with high parameter counts, as
fine-tuning these models from scratch can be computationally expensive and
resource-intensive, posing considerable challenges in the supporting system
platform design. In this survey, we present comprehensive studies of various
PEFT algorithms, examining their performance and computational overhead.
Moreover, we provide an overview of applications developed using different PEFT
algorithms and discuss common techniques employed to mitigate computation costs
for PEFT. In addition to the algorithmic perspective, we overview various
real-world system designs to investigate the implementation costs associated
with different PEFT algorithms. This survey serves as an indispensable resource
for researchers aiming to understand both the PEFT algorithm and its system
implementation, offering detailed insights into recent advancements and
practical applications.</div><div><a href='http://arxiv.org/abs/2403.14608v1'>2403.14608v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17193v1")'>When Scaling Meets LLM Finetuning: The Effect of Data, Model and
  Finetuning Method</div>
<div id='2402.17193v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T04:18:49Z</div><div>Authors: Biao Zhang, Zhongtao Liu, Colin Cherry, Orhan Firat</div><div style='padding-top: 10px; width: 80ex'>While large language models (LLMs) often adopt finetuning to unlock their
capabilities for downstream applications, our understanding on the inductive
biases (especially the scaling properties) of different finetuning methods is
still limited. To fill this gap, we conduct systematic experiments studying
whether and how different scaling factors, including LLM model size,
pretraining data size, new finetuning parameter size and finetuning data size,
affect the finetuning performance. We consider two types of finetuning --
full-model tuning (FMT) and parameter efficient tuning (PET, including prompt
tuning and LoRA), and explore their scaling behaviors in the data-limited
regime where the LLM model size substantially outweighs the finetuning data
size. Based on two sets of pretrained bilingual LLMs from 1B to 16B and
experiments on bilingual machine translation and multilingual summarization
benchmarks, we find that 1) LLM finetuning follows a powerbased multiplicative
joint scaling law between finetuning data size and each other scaling factor;
2) LLM finetuning benefits more from LLM model scaling than pretraining data
scaling, and PET parameter scaling is generally ineffective; and 3) the optimal
finetuning method is highly task- and finetuning data-dependent. We hope our
findings could shed light on understanding, selecting and developing LLM
finetuning methods.</div><div><a href='http://arxiv.org/abs/2402.17193v1'>2402.17193v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12413v2")'>How Far Can 100 Samples Go? Unlocking Overall Zero-Shot Multilingual
  Translation via Tiny Multi-Parallel Data</div>
<div id='2401.12413v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-22T23:55:00Z</div><div>Authors: Di Wu, Shaomu Tan, Yan Meng, David Stap, Christof Monz</div><div style='padding-top: 10px; width: 80ex'>Zero-shot translation aims to translate between language pairs not seen
during training in Multilingual Machine Translation (MMT) and is largely
considered an open problem. A common, albeit resource-consuming, solution is to
add as many related translation directions as possible to the training corpus.
In this paper, we show that for an English-centric model, surprisingly large
zero-shot improvements can be achieved by simply fine-tuning with a very small
amount of multi-parallel data. For example, on the EC30 dataset, we obtain up
to +21.7 ChrF non-English overall improvements (870 directions) by using only
100 multi-parallel samples while preserving English-centric translation
quality. When investigating the size effect of fine-tuning data and its
transfer capabilities, we found that already a small, randomly sampled set of
fine-tuning directions is sufficient to achieve comparable improvements. The
resulting non-English performance is close to the complete translation upper
bound. Even in a minimal setting -- fine-tuning with only one single sample --
the well-known off-target issue is almost completely resolved, explaining parts
-- but not all -- of the observed improvements in translation quality.</div><div><a href='http://arxiv.org/abs/2401.12413v2'>2401.12413v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01772v1")'>Disentangling the Roles of Target-Side Transfer and Regularization in
  Multilingual Machine Translation</div>
<div id='2402.01772v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T10:55:03Z</div><div>Authors: Yan Meng, Christof Monz</div><div style='padding-top: 10px; width: 80ex'>Multilingual Machine Translation (MMT) benefits from knowledge transfer
across different language pairs. However, improvements in one-to-many
translation compared to many-to-one translation are only marginal and sometimes
even negligible. This performance discrepancy raises the question of to what
extent positive transfer plays a role on the target-side for one-to-many MT. In
this paper, we conduct a large-scale study that varies the auxiliary target
side languages along two dimensions, i.e., linguistic similarity and corpus
size, to show the dynamic impact of knowledge transfer on the main language
pairs. We show that linguistically similar auxiliary target languages exhibit
strong ability to transfer positive knowledge. With an increasing size of
similar target languages, the positive transfer is further enhanced to benefit
the main language pairs. Meanwhile, we find distant auxiliary target languages
can also unexpectedly benefit main language pairs, even with minimal positive
transfer ability. Apart from transfer, we show distant auxiliary target
languages can act as a regularizer to benefit translation performance by
enhancing the generalization and model inference calibration.</div><div><a href='http://arxiv.org/abs/2402.01772v1'>2402.01772v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00144v1")'>EBBS: An Ensemble with Bi-Level Beam Search for Zero-Shot Machine
  Translation</div>
<div id='2403.00144v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T21:49:31Z</div><div>Authors: Yuqiao Wen, Behzad Shayegh, Chenyang Huang, Yanshuai Cao, Lili Mou</div><div style='padding-top: 10px; width: 80ex'>The ability of zero-shot translation emerges when we train a multilingual
model with certain translation directions; the model can then directly
translate in unseen directions. Alternatively, zero-shot translation can be
accomplished by pivoting through a third language (e.g., English). In our work,
we observe that both direct and pivot translations are noisy and achieve less
satisfactory performance. We propose EBBS, an ensemble method with a novel
bi-level beam search algorithm, where each ensemble component explores its own
prediction step by step at the lower level but they are synchronized by a "soft
voting" mechanism at the upper level. Results on two popular multilingual
translation datasets show that EBBS consistently outperforms direct and pivot
translations as well as existing ensemble techniques. Further, we can distill
the ensemble's knowledge back to the multilingual model to improve inference
efficiency; profoundly, our EBBS-based distillation does not sacrifice, or even
improves, the translation quality.</div><div><a href='http://arxiv.org/abs/2403.00144v1'>2403.00144v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16902v1")'>PRoLoRA: Partial Rotation Empowers More Parameter-Efficient LoRA</div>
<div id='2402.16902v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-24T13:39:05Z</div><div>Authors: Sheng Wang, Boyang Xue, Jiacheng Ye, Jiyue Jiang, Liheng Chen, Lingpeng Kong, Chuan Wu</div><div style='padding-top: 10px; width: 80ex'>With the rapid scaling of large language models (LLMs), serving numerous
LoRAs concurrently has become increasingly impractical, leading to unaffordable
costs and necessitating more parameter-efficient finetuning methods. In this
work, we introduce Partially Rotation-enhanced Low-Rank Adaptation (PRoLoRA),
an intra-layer sharing mechanism comprising four essential components:
broadcast reduction, rotation enhancement, partially-sharing refinement, and
rectified initialization strategy. As a superset of LoRA, PRoLoRA pertains its
advantages, and effectively circumvent the drawbacks of peer parameter-sharing
methods with superior model capacity, practical feasibility, and broad
applicability. Empirical experiments demonstrate the remarkably higher
parameter efficiency of PRoLoRA in both specific parameter budget and
performance target scenarios, and its scalability to larger LLMs. Notably, with
one time less trainable parameters, PRoLoRA still outperforms LoRA on multiple
instruction tuning datasets. Subsequently, an ablation study is conducted to
validate the necessity of individual components and highlight the superiority
of PRoLoRA over three potential variants. Hopefully, the conspicuously higher
parameter efficiency can establish PRoLoRA as a resource-friendly alternative
to LoRA.</div><div><a href='http://arxiv.org/abs/2402.16902v1'>2402.16902v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02545v2")'>Wukong: Towards a Scaling Law for Large-Scale Recommendation</div>
<div id='2403.02545v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-04T23:40:20Z</div><div>Authors: Buyun Zhang, Liang Luo, Yuxin Chen, Jade Nie, Xi Liu, Daifeng Guo, Yanli Zhao, Shen Li, Yuchen Hao, Yantao Yao, Guna Lakshminarayanan, Ellie Dingqiao Wen, Jongsoo Park, Maxim Naumov, Wenlin Chen</div><div style='padding-top: 10px; width: 80ex'>Scaling laws play an instrumental role in the sustainable improvement in
model quality. Unfortunately, recommendation models to date do not exhibit such
laws similar to those observed in the domain of large language models, due to
the inefficiencies of their upscaling mechanisms. This limitation poses
significant challenges in adapting these models to increasingly more complex
real-world datasets. In this paper, we propose an effective network
architecture based purely on stacked factorization machines, and a synergistic
upscaling strategy, collectively dubbed Wukong, to establish a scaling law in
the domain of recommendation. Wukong's unique design makes it possible to
capture diverse, any-order of interactions simply through taller and wider
layers. We conducted extensive evaluations on six public datasets, and our
results demonstrate that Wukong consistently outperforms state-of-the-art
models quality-wise. Further, we assessed Wukong's scalability on an internal,
large-scale dataset. The results show that Wukong retains its superiority in
quality over state-of-the-art models, while holding the scaling law across two
orders of magnitude in model complexity, extending beyond 100 Gflop or
equivalently up to Large Language Model (GPT-3) training compute scale, where
prior arts fall short.</div><div><a href='http://arxiv.org/abs/2403.02545v2'>2403.02545v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15623v1")'>Language-Based User Profiles for Recommendation</div>
<div id='2402.15623v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T21:58:50Z</div><div>Authors: Joyce Zhou, Yijia Dai, Thorsten Joachims</div><div style='padding-top: 10px; width: 80ex'>Most conventional recommendation methods (e.g., matrix factorization)
represent user profiles as high-dimensional vectors. Unfortunately, these
vectors lack interpretability and steerability, and often perform poorly in
cold-start settings. To address these shortcomings, we explore the use of user
profiles that are represented as human-readable text. We propose the
Language-based Factorization Model (LFM), which is essentially an
encoder/decoder model where both the encoder and the decoder are large language
models (LLMs). The encoder LLM generates a compact natural-language profile of
the user's interests from the user's rating history. The decoder LLM uses this
summary profile to complete predictive downstream tasks. We evaluate our LFM
approach on the MovieLens dataset, comparing it against matrix factorization
and an LLM model that directly predicts from the user's rating history. In
cold-start settings, we find that our method can have higher accuracy than
matrix factorization. Furthermore, we find that generating a compact and
human-readable summary often performs comparably with or better than direct LLM
prediction, while enjoying better interpretability and shorter model input
length. Our results motivate a number of future research directions and
potential improvements.</div><div><a href='http://arxiv.org/abs/2402.15623v1'>2402.15623v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17152v1")'>Actions Speak Louder than Words: Trillion-Parameter Sequential
  Transducers for Generative Recommendations</div>
<div id='2402.17152v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T02:37:37Z</div><div>Authors: Jiaqi Zhai, Lucy Liao, Xing Liu, Yueming Wang, Rui Li, Xuan Cao, Leon Gao, Zhaojie Gong, Fangda Gu, Michael He, Yinghai Lu, Yu Shi</div><div style='padding-top: 10px; width: 80ex'>Large-scale recommendation systems are characterized by their reliance on
high cardinality, heterogeneous features and the need to handle tens of
billions of user actions on a daily basis. Despite being trained on huge volume
of data with thousands of features, most Deep Learning Recommendation Models
(DLRMs) in industry fail to scale with compute.
  Inspired by success achieved by Transformers in language and vision domains,
we revisit fundamental design choices in recommendation systems. We reformulate
recommendation problems as sequential transduction tasks within a generative
modeling framework (``Generative Recommenders''), and propose a new
architecture, HSTU, designed for high cardinality, non-stationary streaming
recommendation data.
  HSTU outperforms baselines over synthetic and public datasets by up to 65.8\%
in NDCG, and is 5.3x to 15.2x faster than FlashAttention2-based Transformers on
8192 length sequences. HSTU-based Generative Recommenders, with 1.5 trillion
parameters, improve metrics in online A/B tests by 12.4\% and have been
deployed on multiple surfaces of a large internet platform with billions of
users. More importantly, the model quality of Generative Recommenders
empirically scales as a power-law of training compute across three orders of
magnitude, up to GPT-3/LLaMa-2 scale, which reduces carbon footprint needed for
future model developments, and further paves the way for the first foundational
models in recommendations.</div><div><a href='http://arxiv.org/abs/2402.17152v1'>2402.17152v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.04732v1")'>A case study of Generative AI in MSX Sales Copilot: Improving seller
  productivity with a real-time question-answering system for content
  recommendation</div>
<div id='2401.04732v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T13:32:44Z</div><div>Authors: Manpreet Singh, Ravdeep Pasricha, Nitish Singh, Ravi Prasad Kondapalli, Manoj R, Kiran R, Laurent Boué</div><div style='padding-top: 10px; width: 80ex'>In this paper, we design a real-time question-answering system specifically
targeted for helping sellers get relevant material/documentation they can share
live with their customers or refer to during a call. Taking the Seismic content
repository as a relatively large scale example of a diverse dataset of sales
material, we demonstrate how LLM embeddings of sellers' queries can be matched
with the relevant content. We achieve this by engineering prompts in an
elaborate fashion that makes use of the rich set of meta-features available for
documents and sellers. Using a bi-encoder with cross-encoder re-ranker
architecture, we show how the solution returns the most relevant content
recommendations in just a few seconds even for large datasets. Our recommender
system is deployed as an AML endpoint for real-time inferencing and has been
integrated into a Copilot interface that is now deployed in the production
version of the Dynamics CRM, known as MSX, used daily by Microsoft sellers.</div><div><a href='http://arxiv.org/abs/2401.04732v1'>2401.04732v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.00737v1")'>Searching, fast and slow, through product catalogs</div>
<div id='2401.00737v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-01T12:30:46Z</div><div>Authors: Dayananda Ubrangala, Juhi Sharma, Sharath Kumar Rangappa, Kiran R, Ravi Prasad Kondapalli, Laurent Boué</div><div style='padding-top: 10px; width: 80ex'>String matching algorithms in the presence of abbreviations, such as in Stock
Keeping Unit (SKU) product catalogs, remains a relatively unexplored topic. In
this paper, we present a unified architecture for SKU search that provides both
a real-time suggestion system (based on a Trie data structure) as well as a
lower latency search system (making use of character level TF-IDF in
combination with language model vector embeddings) where users initiate the
search process explicitly. We carry out ablation studies that justify designing
a complex search system composed of multiple components to address the delicate
trade-off between speed and accuracy. Using SKU search in the Dynamics CRM as
an example, we show how our system vastly outperforms, in all aspects, the
results provided by the default search engine. Finally, we show how SKU
descriptions may be enhanced via generative text models (using gpt-3.5-turbo)
so that the consumers of the search results may get more context and a
generally better experience when presented with the results of their SKU
search.</div><div><a href='http://arxiv.org/abs/2401.00737v1'>2401.00737v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00584v1")'>Generalized User Representations for Transfer Learning</div>
<div id='2403.00584v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T15:05:21Z</div><div>Authors: Ghazal Fazelnia, Sanket Gupta, Claire Keum, Mark Koh, Ian Anderson, Mounia Lalmas</div><div style='padding-top: 10px; width: 80ex'>We present a novel framework for user representation in large-scale
recommender systems, aiming at effectively representing diverse user taste in a
generalized manner. Our approach employs a two-stage methodology combining
representation learning and transfer learning. The representation learning
model uses an autoencoder that compresses various user features into a
representation space. In the second stage, downstream task-specific models
leverage user representations via transfer learning instead of curating user
features individually. We further augment this methodology on the
representation's input features to increase flexibility and enable reaction to
user events, including new user experiences, in Near-Real Time. Additionally,
we propose a novel solution to manage deployment of this framework in
production models, allowing downstream models to work independently. We
validate the performance of our framework through rigorous offline and online
experiments within a large-scale system, showcasing its remarkable efficacy
across multiple evaluation tasks. Finally, we show how the proposed framework
can significantly reduce infrastructure costs compared to alternative
approaches.</div><div><a href='http://arxiv.org/abs/2403.00584v1'>2403.00584v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05122v1")'>Multi-Tower Multi-Interest Recommendation with User Representation Repel</div>
<div id='2403.05122v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T07:36:14Z</div><div>Authors: Tianyu Xiong, Xiaohan Yu</div><div style='padding-top: 10px; width: 80ex'>In the era of information overload, the value of recommender systems has been
profoundly recognized in academia and industry alike. Multi-interest sequential
recommendation, in particular, is a subfield that has been receiving increasing
attention in recent years. By generating multiple-user representations,
multi-interest learning models demonstrate superior expressiveness than
single-user representation models, both theoretically and empirically. Despite
major advancements in the field, three major issues continue to plague the
performance and adoptability of multi-interest learning methods, the difference
between training and deployment objectives, the inability to access item
information, and the difficulty of industrial adoption due to its single-tower
architecture. We address these challenges by proposing a novel multi-tower
multi-interest framework with user representation repel. Experimental results
across multiple large-scale industrial datasets proved the effectiveness and
generalizability of our proposed framework.</div><div><a href='http://arxiv.org/abs/2403.05122v1'>2403.05122v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04875v1")'>Aligning GPTRec with Beyond-Accuracy Goals with Reinforcement Learning</div>
<div id='2403.04875v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T19:47:48Z</div><div>Authors: Aleksandr Petrov, Craig Macdonald</div><div style='padding-top: 10px; width: 80ex'>Adaptations of Transformer models, such as BERT4Rec and SASRec, achieve
state-of-the-art performance in the sequential recommendation task according to
accuracy-based metrics, such as NDCG. These models treat items as tokens and
then utilise a score-and-rank approach (Top-K strategy), where the model first
computes item scores and then ranks them according to this score. While this
approach works well for accuracy-based metrics, it is hard to use it for
optimising more complex beyond-accuracy metrics such as diversity. Recently,
the GPTRec model, which uses a different Next-K strategy, has been proposed as
an alternative to the Top-K models. In contrast with traditional Top-K
recommendations, Next-K generates recommendations item-by-item and, therefore,
can account for complex item-to-item interdependencies important for the
beyond-accuracy measures. However, the original GPTRec paper focused only on
accuracy in experiments and needed to address how to optimise the model for
complex beyond-accuracy metrics. Indeed, training GPTRec for beyond-accuracy
goals is challenging because the interaction training data available for
training recommender systems typically needs to be aligned with beyond-accuracy
recommendation goals. To solve the misalignment problem, we train GPTRec using
a 2-stage approach: in the first stage, we use a teacher-student approach to
train GPTRec, mimicking the behaviour of traditional Top-K models; in the
second stage, we use Reinforcement Learning to align the model for
beyond-accuracy goals. In particular, we experiment with increasing
recommendation diversity and reducing popularity bias. Our experiments on two
datasets show that in 3 out of 4 cases, GPTRec's Next-K generation approach
offers a better tradeoff between accuracy and secondary metrics than classic
greedy re-ranking techniques.</div><div><a href='http://arxiv.org/abs/2403.04875v1'>2403.04875v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00844v1")'>Lower-Left Partial AUC: An Effective and Efficient Optimization Metric
  for Recommendation</div>
<div id='2403.00844v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T13:58:33Z</div><div>Authors: Wentao Shi, Chenxu Wang, Fuli Feng, Yang Zhang, Wenjie Wang, Junkang Wu, Xiangnan He</div><div style='padding-top: 10px; width: 80ex'>Optimization metrics are crucial for building recommendation systems at
scale. However, an effective and efficient metric for practical use remains
elusive. While Top-K ranking metrics are the gold standard for optimization,
they suffer from significant computational overhead. Alternatively, the more
efficient accuracy and AUC metrics often fall short of capturing the true
targets of recommendation tasks, leading to suboptimal performance. To overcome
this dilemma, we propose a new optimization metric, Lower-Left Partial AUC
(LLPAUC), which is computationally efficient like AUC but strongly correlates
with Top-K ranking metrics. Compared to AUC, LLPAUC considers only the partial
area under the ROC curve in the Lower-Left corner to push the optimization
focus on Top-K. We provide theoretical validation of the correlation between
LLPAUC and Top-K ranking metrics and demonstrate its robustness to noisy user
feedback. We further design an efficient point-wise recommendation loss to
maximize LLPAUC and evaluate it on three datasets, validating its effectiveness
and robustness.</div><div><a href='http://arxiv.org/abs/2403.00844v1'>2403.00844v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09766v1")'>From Variability to Stability: Advancing RecSys Benchmarking Practices</div>
<div id='2402.09766v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T07:35:52Z</div><div>Authors: Valeriy Shevchenko, Nikita Belousov, Alexey Vasilev, Vladimir Zholobov, Artyom Sosedka, Natalia Semenova, Anna Volodkevich, Andrey Savchenko, Alexey Zaytsev</div><div style='padding-top: 10px; width: 80ex'>In the rapidly evolving domain of Recommender Systems (RecSys), new
algorithms frequently claim state-of-the-art performance based on evaluations
over a limited set of arbitrarily selected datasets. However, this approach may
fail to holistically reflect their effectiveness due to the significant impact
of dataset characteristics on algorithm performance. Addressing this
deficiency, this paper introduces a novel benchmarking methodology to
facilitate a fair and robust comparison of RecSys algorithms, thereby advancing
evaluation practices. By utilizing a diverse set of $30$ open datasets,
including two introduced in this work, and evaluating $11$ collaborative
filtering algorithms across $9$ metrics, we critically examine the influence of
dataset characteristics on algorithm performance. We further investigate the
feasibility of aggregating outcomes from multiple datasets into a unified
ranking. Through rigorous experimental analysis, we validate the reliability of
our methodology under the variability of datasets, offering a benchmarking
strategy that balances quality and computational demands. This methodology
enables a fair yet effective means of evaluating RecSys algorithms, providing
valuable guidance for future research endeavors.</div><div><a href='http://arxiv.org/abs/2402.09766v1'>2402.09766v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11262v1")'>Mirror Gradient: Towards Robust Multimodal Recommender Systems via
  Exploring Flat Local Minima</div>
<div id='2402.11262v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-17T12:27:30Z</div><div>Authors: Shanshan Zhong, Zhongzhan Huang, Daifeng Li, Wushao Wen, Jinghui Qin, Liang Lin</div><div style='padding-top: 10px; width: 80ex'>Multimodal recommender systems utilize various types of information to model
user preferences and item features, helping users discover items aligned with
their interests. The integration of multimodal information mitigates the
inherent challenges in recommender systems, e.g., the data sparsity problem and
cold-start issues. However, it simultaneously magnifies certain risks from
multimodal information inputs, such as information adjustment risk and inherent
noise risk. These risks pose crucial challenges to the robustness of
recommendation models. In this paper, we analyze multimodal recommender systems
from the novel perspective of flat local minima and propose a concise yet
effective gradient strategy called Mirror Gradient (MG). This strategy can
implicitly enhance the model's robustness during the optimization process,
mitigating instability risks arising from multimodal information inputs. We
also provide strong theoretical evidence and conduct extensive empirical
experiments to show the superiority of MG across various multimodal
recommendation models and benchmarks. Furthermore, we find that the proposed MG
can complement existing robust training methods and be easily extended to
diverse advanced recommendation models, making it a promising new and
fundamental paradigm for training multimodal recommender systems. The code is
released at https://github.com/Qrange-group/Mirror-Gradient.</div><div><a href='http://arxiv.org/abs/2402.11262v1'>2402.11262v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12384v2")'>An Aligning and Training Framework for Multimodal Recommendations</div>
<div id='2403.12384v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T02:49:32Z</div><div>Authors: Yifan Liu, Kangning Zhang, Xiangyuan Ren, Yanhua Huang, Jiarui Jin, Yingjie Qin, Ruilong Su, Ruiwen Xu, Weinan Zhang</div><div style='padding-top: 10px; width: 80ex'>With the development of multimedia applications, multimodal recommendations
are playing an essential role, as they can leverage rich contexts beyond user
interactions. Existing methods mainly regard multimodal information as an
auxiliary, using them to help learn ID features; however, there exist semantic
gaps among multimodal content features and ID features, for which directly
using multimodal information as an auxiliary would lead to misalignment in
representations of users and items. In this paper, we first systematically
investigate the misalignment issue in multimodal recommendations, and propose a
solution named AlignRec. In AlignRec, the recommendation objective is
decomposed into three alignments, namely alignment within contents, alignment
between content and categorical ID, and alignment between users and items. Each
alignment is characterized by a specific objective function and is integrated
into our multimodal recommendation framework. To effectively train our
AlignRec, we propose starting from pre-training the first alignment to obtain
unified multimodal features and subsequently training the following two
alignments together with these features as input. As it is essential to analyze
whether each multimodal feature helps in training, we design three new classes
of metrics to evaluate intermediate performance. Our extensive experiments on
three real-world datasets consistently verify the superiority of AlignRec
compared to nine baselines. We also find that the multimodal features generated
by AlignRec are better than currently used ones, which are to be open-sourced.</div><div><a href='http://arxiv.org/abs/2403.12384v2'>2403.12384v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.02794v1")'>A Distance Metric Learning Model Based On Variational Information
  Bottleneck</div>
<div id='2403.02794v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T09:08:20Z</div><div>Authors: YaoDan Zhang, Zidong Wang, Ru Jia, Ru Li</div><div style='padding-top: 10px; width: 80ex'>In recent years, personalized recommendation technology has flourished and
become one of the hot research directions. The matrix factorization model and
the metric learning model which proposed successively have been widely studied
and applied. The latter uses the Euclidean distance instead of the dot product
used by the former to measure the latent space vector. While avoiding the
shortcomings of the dot product, the assumption of Euclidean distance is
neglected, resulting in limited recommendation quality of the model. In order
to solve this problem, this paper combines the Variationl Information
Bottleneck with metric learning model for the first time, and proposes a new
metric learning model VIB-DML (Variational Information Bottleneck Distance
Metric Learning) for rating prediction, which limits the mutual information of
the latent space feature vector to improve the robustness of the model and
satisfiy the assumption of Euclidean distance by decoupling the latent space
feature vector. In this paper, the experimental results are compared with the
root mean square error (RMSE) on the three public datasets. The results show
that the generalization ability of VIB-DML is excellent. Compared with the
general metric learning model MetricF, the prediction error is reduced by
7.29%. Finally, the paper proves the strong robustness of VIBDML through
experiments.</div><div><a href='http://arxiv.org/abs/2403.02794v1'>2403.02794v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.03481v1")'>FINEST: Stabilizing Recommendations by Rank-Preserving Fine-Tuning</div>
<div id='2402.03481v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T19:53:34Z</div><div>Authors: Sejoon Oh, Berk Ustun, Julian McAuley, Srijan Kumar</div><div style='padding-top: 10px; width: 80ex'>Modern recommender systems may output considerably different recommendations
due to small perturbations in the training data. Changes in the data from a
single user will alter the recommendations as well as the recommendations of
other users. In applications like healthcare, housing, and finance, this
sensitivity can have adverse effects on user experience. We propose a method to
stabilize a given recommender system against such perturbations. This is a
challenging task due to (1) the lack of a ``reference'' rank list that can be
used to anchor the outputs; and (2) the computational challenges in ensuring
the stability of rank lists with respect to all possible perturbations of
training data. Our method, FINEST, overcomes these challenges by obtaining
reference rank lists from a given recommendation model and then fine-tuning the
model under simulated perturbation scenarios with rank-preserving
regularization on sampled items. Our experiments on real-world datasets
demonstrate that FINEST can ensure that recommender models output stable
recommendations under a wide range of different perturbations without
compromising next-item prediction accuracy.</div><div><a href='http://arxiv.org/abs/2402.03481v1'>2402.03481v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.00803v1")'>LiMAML: Personalization of Deep Recommender Models via Meta Learning</div>
<div id='2403.00803v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T22:06:36Z</div><div>Authors: Ruofan Wang, Prakruthi Prabhakar, Gaurav Srivastava, Tianqi Wang, Zeinab S. Jalali, Varun Bharill, Yunbo Ouyang, Aastha Nigam, Divya Venugopalan, Aman Gupta, Fedor Borisyuk, Sathiya Keerthi, Ajith Muralidharan</div><div style='padding-top: 10px; width: 80ex'>In the realm of recommender systems, the ubiquitous adoption of deep neural
networks has emerged as a dominant paradigm for modeling diverse business
objectives. As user bases continue to expand, the necessity of personalization
and frequent model updates have assumed paramount significance to ensure the
delivery of relevant and refreshed experiences to a diverse array of members.
In this work, we introduce an innovative meta-learning solution tailored to the
personalization of models for individual members and other entities, coupled
with the frequent updates based on the latest user interaction signals.
Specifically, we leverage the Model-Agnostic Meta Learning (MAML) algorithm to
adapt per-task sub-networks using recent user interaction data. Given the near
infeasibility of productionizing original MAML-based models in online
recommendation systems, we propose an efficient strategy to operationalize
meta-learned sub-networks in production, which involves transforming them into
fixed-sized vectors, termed meta embeddings, thereby enabling the seamless
deployment of models with hundreds of billions of parameters for online
serving. Through extensive experimentation on production data drawn from
various applications at LinkedIn, we demonstrate that the proposed solution
consistently outperforms the baseline models of those applications, including
strong baselines such as using wide-and-deep ID based personalization approach.
Our approach has enabled the deployment of a range of highly personalized AI
models across diverse LinkedIn applications, leading to substantial
improvements in business metrics as well as refreshed experience for our
members.</div><div><a href='http://arxiv.org/abs/2403.00803v1'>2403.00803v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08109v1")'>From Data to Decisions: The Transformational Power of Machine Learning
  in Business Recommendations</div>
<div id='2402.08109v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T22:56:18Z</div><div>Authors: Kapilya Gangadharan, K. Malathi, Anoop Purandaran, Barathi Subramanian, Rathinaraja Jeyaraj</div><div style='padding-top: 10px; width: 80ex'>This research aims to explore the impact of Machine Learning (ML) on the
evolution and efficacy of Recommendation Systems (RS), particularly in the
context of their growing significance in commercial business environments.
Methodologically, the study delves into the role of ML in crafting and refining
these systems, focusing on aspects such as data sourcing, feature engineering,
and the importance of evaluation metrics, thereby highlighting the iterative
nature of enhancing recommendation algorithms. The deployment of Recommendation
Engines (RE), driven by advanced algorithms and data analytics, is explored
across various domains, showcasing their significant impact on user experience
and decision-making processes. These engines not only streamline information
discovery and enhance collaboration but also accelerate knowledge acquisition,
proving vital in navigating the digital landscape for businesses. They
contribute significantly to sales, revenue, and the competitive edge of
enterprises by offering improved recommendations that align with individual
customer needs. The research identifies the increasing expectation of users for
a seamless, intuitive online experience, where content is personalized and
dynamically adapted to changing preferences. Future research directions include
exploring advancements in deep learning models, ethical considerations in the
deployment of RS, and addressing scalability challenges. This study emphasizes
the indispensability of comprehending and leveraging ML in RS for researchers
and practitioners, to tap into the full potential of personalized
recommendation in commercial business prospects.</div><div><a href='http://arxiv.org/abs/2402.08109v1'>2402.08109v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02152v2")'>Position Paper: Why the Shooting in the Dark Method Dominates
  Recommender Systems Practice; A Call to Abandon Anti-Utopian Thinking</div>
<div id='2402.02152v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T13:46:28Z</div><div>Authors: David Rohde</div><div style='padding-top: 10px; width: 80ex'>Applied recommender systems research is in a curious position. While there is
a very rigorous protocol for measuring performance by A/B testing, best
practice for finding a `B' to test does not explicitly target performance but
rather targets a proxy measure. The success or failure of a given A/B test then
depends entirely on if the proposed proxy is better correlated to performance
than the previous proxy. No principle exists to identify if one proxy is better
than another offline, leaving the practitioners shooting in the dark. The
purpose of this position paper is to question this anti-Utopian thinking and
argue that a non-standard use of the deep learning stacks actually has the
potential to unlock reward optimizing recommendation.</div><div><a href='http://arxiv.org/abs/2402.02152v2'>2402.02152v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02855v1")'>Dynamic Sparse Learning: A Novel Paradigm for Efficient Recommendation</div>
<div id='2402.02855v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T10:16:20Z</div><div>Authors: Shuyao Wang, Yongduo Sui, Jiancan Wu, Zhi Zheng, Hui Xiong</div><div style='padding-top: 10px; width: 80ex'>In the realm of deep learning-based recommendation systems, the increasing
computational demands, driven by the growing number of users and items, pose a
significant challenge to practical deployment. This challenge is primarily
twofold: reducing the model size while effectively learning user and item
representations for efficient recommendations. Despite considerable
advancements in model compression and architecture search, prevalent approaches
face notable constraints. These include substantial additional computational
costs from pre-training/re-training in model compression and an extensive
search space in architecture design. Additionally, managing complexity and
adhering to memory constraints is problematic, especially in scenarios with
strict time or space limitations. Addressing these issues, this paper
introduces a novel learning paradigm, Dynamic Sparse Learning (DSL), tailored
for recommendation models. DSL innovatively trains a lightweight sparse model
from scratch, periodically evaluating and dynamically adjusting each weight's
significance and the model's sparsity distribution during the training. This
approach ensures a consistent and minimal parameter budget throughout the full
learning lifecycle, paving the way for "end-to-end" efficiency from training to
inference. Our extensive experimental results underline DSL's effectiveness,
significantly reducing training and inference costs while delivering comparable
recommendation performance.</div><div><a href='http://arxiv.org/abs/2402.02855v1'>2402.02855v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00877v2")'>Disaggregated Multi-Tower: Topology-aware Modeling Technique for
  Efficient Large-Scale Recommendation</div>
<div id='2403.00877v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T08:26:44Z</div><div>Authors: Liang Luo, Buyun Zhang, Michael Tsang, Yinbin Ma, Ching-Hsiang Chu, Yuxin Chen, Shen Li, Yuchen Hao, Yanli Zhao, Guna Lakshminarayanan, Ellie Dingqiao Wen, Jongsoo Park, Dheevatsa Mudigere, Maxim Naumov</div><div style='padding-top: 10px; width: 80ex'>We study a mismatch between the deep learning recommendation models' flat
architecture, common distributed training paradigm and hierarchical data center
topology. To address the associated inefficiencies, we propose Disaggregated
Multi-Tower (DMT), a modeling technique that consists of (1)
Semantic-preserving Tower Transform (SPTT), a novel training paradigm that
decomposes the monolithic global embedding lookup process into disjoint towers
to exploit data center locality; (2) Tower Module (TM), a synergistic dense
component attached to each tower to reduce model complexity and communication
volume through hierarchical feature interaction; and (3) Tower Partitioner
(TP), a feature partitioner to systematically create towers with meaningful
feature interactions and load balanced assignments to preserve model quality
and training throughput via learned embeddings. We show that DMT can achieve up
to 1.9x speedup compared to the state-of-the-art baselines without losing
accuracy across multiple generations of hardware at large data center scales.</div><div><a href='http://arxiv.org/abs/2403.00877v2'>2403.00877v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00793v1")'>Ad Recommendation in a Collapsed and Entangled World</div>
<div id='2403.00793v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T22:47:08Z</div><div>Authors: Junwei Pan, Wei Xue, Ximei Wang, Haibin Yu, Xun Liu, Shijie Quan, Xueming Qiu, Dapeng Liu, Lei Xiao, Jie Jiang</div><div style='padding-top: 10px; width: 80ex'>In this paper, we present an industry ad recommendation system, paying
attention to the challenges and practices of learning appropriate
representations. Our study begins by showcasing our approaches to preserving
priors when encoding features of diverse types into embedding representations.
Specifically, we address sequence features, numeric features, pre-trained
embedding features, as well as sparse ID features. Moreover, we delve into two
pivotal challenges associated with feature representation: the dimensional
collapse of embeddings and the interest entanglement across various tasks or
scenarios. Subsequently, we propose several practical approaches to effectively
tackle these two challenges. We then explore several training techniques to
facilitate model optimization, reduce bias, and enhance exploration.
Furthermore, we introduce three analysis tools that enable us to
comprehensively study feature correlation, dimensional collapse, and interest
entanglement. This work builds upon the continuous efforts of Tencent's ads
recommendation team in the last decade. It not only summarizes general design
principles but also presents a series of off-the-shelf solutions and analysis
tools. The reported performance is based on our online advertising platform,
which handles hundreds of billions of requests daily, serving millions of ads
to billions of users.</div><div><a href='http://arxiv.org/abs/2403.00793v1'>2403.00793v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07445v1")'>GACE: Learning Graph-Based Cross-Page Ads Embedding For Click-Through
  Rate Prediction</div>
<div id='2401.07445v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T03:12:21Z</div><div>Authors: Haowen Wang, Yuliang Du, Congyun Jin, Yujiao Li, Yingbo Wang, Tao Sun, Piqi Qin, Cong Fan</div><div style='padding-top: 10px; width: 80ex'>Predicting click-through rate (CTR) is the core task of many ads online
recommendation systems, which helps improve user experience and increase
platform revenue. In this type of recommendation system, we often encounter two
main problems: the joint usage of multi-page historical advertising data and
the cold start of new ads. In this paper, we proposed GACE, a graph-based
cross-page ads embedding generation method. It can warm up and generate the
representation embedding of cold-start and existing ads across various pages.
Specifically, we carefully build linkages and a weighted undirected graph model
considering semantic and page-type attributes to guide the direction of feature
fusion and generation. We designed a variational auto-encoding task as
pre-training module and generated embedding representations for new and old ads
based on this task. The results evaluated in the public dataset AliEC from
RecBole and the real-world industry dataset from Alipay show that our GACE
method is significantly superior to the SOTA method. In the online A/B test,
the click-through rate on three real-world pages from Alipay has increased by
3.6%, 2.13%, and 3.02%, respectively. Especially in the cold-start task, the
CTR increased by 9.96%, 7.51%, and 8.97%, respectively.</div><div><a href='http://arxiv.org/abs/2401.07445v1'>2401.07445v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16073v2")'>Pfeed: Generating near real-time personalized feeds using precomputed
  embedding similarities</div>
<div id='2402.16073v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-25T12:06:33Z</div><div>Authors: Binyam Gebre, Karoliina Ranta, Stef van den Elzen, Ernst Kuiper, Thijs Baars, Tom Heskes</div><div style='padding-top: 10px; width: 80ex'>In personalized recommender systems, embeddings are often used to encode
customer actions and items, and retrieval is then performed in the embedding
space using approximate nearest neighbor search. However, this approach can
lead to two challenges: 1) user embeddings can restrict the diversity of
interests captured and 2) the need to keep them up-to-date requires an
expensive, real-time infrastructure. In this paper, we propose a method that
overcomes these challenges in a practical, industrial setting. The method
dynamically updates customer profiles and composes a feed every two minutes,
employing precomputed embeddings and their respective similarities. We tested
and deployed this method to personalise promotional items at Bol, one of the
largest e-commerce platforms of the Netherlands and Belgium. The method
enhanced customer engagement and experience, leading to a significant 4.9%
uplift in conversions.</div><div><a href='http://arxiv.org/abs/2402.16073v2'>2402.16073v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13344v1")'>USE: Dynamic User Modeling with Stateful Sequence Models</div>
<div id='2403.13344v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T07:05:19Z</div><div>Authors: Zhihan Zhou, Qixiang Fang, Leonardo Neves, Francesco Barbieri, Yozen Liu, Han Liu, Maarten W. Bos, Ron Dotsch</div><div style='padding-top: 10px; width: 80ex'>User embeddings play a crucial role in user engagement forecasting and
personalized services. Recent advances in sequence modeling have sparked
interest in learning user embeddings from behavioral data. Yet behavior-based
user embedding learning faces the unique challenge of dynamic user modeling. As
users continuously interact with the apps, user embeddings should be
periodically updated to account for users' recent and long-term behavior
patterns. Existing methods highly rely on stateless sequence models that lack
memory of historical behavior. They have to either discard historical data and
use only the most recent data or reprocess the old and new data jointly. Both
cases incur substantial computational overhead. To address this limitation, we
introduce User Stateful Embedding (USE). USE generates user embeddings and
reflects users' evolving behaviors without the need for exhaustive reprocessing
by storing previous model states and revisiting them in the future.
Furthermore, we introduce a novel training objective named future W-behavior
prediction to transcend the limitations of next-token prediction by forecasting
a broader horizon of upcoming user behaviors. By combining it with the Same
User Prediction, a contrastive learning-based objective that predicts whether
different segments of behavior sequences belong to the same user, we further
improve the embeddings' distinctiveness and representativeness. We conducted
experiments on 8 downstream tasks using Snapchat users' behavioral logs in both
static (i.e., fixed user behavior sequences) and dynamic (i.e., periodically
updated user behavior sequences) settings. We demonstrate USE's superior
performance over established baselines. The results underscore USE's
effectiveness and efficiency in integrating historical and recent user behavior
sequences into user embeddings in dynamic user modeling.</div><div><a href='http://arxiv.org/abs/2403.13344v1'>2403.13344v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.10045v3")'>Short-Form Videos and Mental Health: A Knowledge-Guided Neural Topic
  Model</div>
<div id='2402.10045v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T03:36:47Z</div><div>Authors: Jiaheng Xie, Ruicheng Liang, Yidong Chai, Yang Liu, Daniel Zeng</div><div style='padding-top: 10px; width: 80ex'>While short-form videos head to reshape the entire social media landscape,
experts are exceedingly worried about their depressive impacts on viewers, as
evidenced by medical studies. To prevent widespread consequences, platforms are
eager to predict these videos' impact on viewers' mental health. Subsequently,
they can take intervention measures, such as revising recommendation algorithms
and displaying viewer discretion. Nevertheless, applicable predictive methods
lack relevance to well-established medical knowledge, which outlines clinically
proven external and environmental factors of depression. To account for such
medical knowledge, we resort to an emergent methodological discipline, seeded
Neural Topic Models (NTMs). However, existing seeded NTMs suffer from the
limitations of single-origin topics, unknown topic sources, unclear seed
supervision, and suboptimal convergence. To address those challenges, we
develop a novel Knowledge-guided Multimodal NTM to predict a short-form video's
depressive impact on viewers. Extensive empirical analyses using TikTok and
Douyin datasets prove that our method outperforms state-of-the-art benchmarks.
Our method also discovers medically relevant topics from videos that are linked
to depressive impact. We contribute to IS with a novel video analytics method
that is generalizable to other video classification problems. Practically, our
method can help platforms understand videos' mental impacts, thus adjusting
recommendations and video topic disclosure.</div><div><a href='http://arxiv.org/abs/2402.10045v3'>2402.10045v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00137v1")'>Multimodal Neurodegenerative Disease Subtyping Explained by ChatGPT</div>
<div id='2402.00137v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T19:30:04Z</div><div>Authors: Diego Machado Reyes, Hanqing Chao, Juergen Hahn, Li Shen, Pingkun Yan</div><div style='padding-top: 10px; width: 80ex'>Alzheimer's disease (AD) is the most prevalent neurodegenerative disease; yet
its currently available treatments are limited to stopping disease progression.
Moreover, effectiveness of these treatments is not guaranteed due to the
heterogenetiy of the disease. Therefore, it is essential to be able to identify
the disease subtypes at a very early stage. Current data driven approaches are
able to classify the subtypes at later stages of AD or related disorders, but
struggle when predicting at the asymptomatic or prodromal stage. Moreover, most
existing models either lack explainability behind the classification or only
use a single modality for the assessment, limiting scope of its analysis. Thus,
we propose a multimodal framework that uses early-stage indicators such as
imaging, genetics and clinical assessments to classify AD patients into
subtypes at early stages. Similarly, we build prompts and use large language
models, such as ChatGPT, to interpret the findings of our model. In our
framework, we propose a tri-modal co-attention mechanism (Tri-COAT) to
explicitly learn the cross-modal feature associations. Our proposed model
outperforms baseline models and provides insight into key cross-modal feature
associations supported by known biological mechanisms.</div><div><a href='http://arxiv.org/abs/2402.00137v1'>2402.00137v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.02609v1")'>Search Intenion Network for Personalized Query Auto-Completion in
  E-Commerce</div>
<div id='2403.02609v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T02:53:24Z</div><div>Authors: Wei Bao, Mi Zhang, Tao Zhang, Chengfu Huo</div><div style='padding-top: 10px; width: 80ex'>Query Auto-Completion(QAC), as an important part of the modern search engine,
plays a key role in complementing user queries and helping them refine their
search intentions.Today's QAC systems in real-world scenarios face two major
challenges:1)intention equivocality(IE): during the user's typing process,the
prefix often contains a combination of characters and subwords, which makes the
current intention ambiguous and difficult to model.2)intention transfer
(IT):previous works make personalized recommendations based on users'
historical sequences, but ignore the search intention transfer.However, the
current intention extracted from prefix may be contrary to the historical
preferences.</div><div><a href='http://arxiv.org/abs/2403.02609v1'>2403.02609v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.12313v1")'>Improving LoRA in Privacy-preserving Federated Learning</div>
<div id='2403.12313v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T23:20:08Z</div><div>Authors: Youbang Sun, Zitao Li, Yaliang Li, Bolin Ding</div><div style='padding-top: 10px; width: 80ex'>Low-rank adaptation (LoRA) is one of the most popular task-specific
parameter-efficient fine-tuning (PEFT) methods on pre-trained language models
for its good performance and computational efficiency. LoRA injects a product
of two trainable rank decomposition matrices over the top of each frozen
pre-trained model module. However, when applied in the setting of
privacy-preserving federated learning (FL), LoRA may become unstable due to the
following facts: 1) the effects of data heterogeneity and multi-step local
updates are non-negligible, 2) additive noise enforced on updating gradients to
guarantee differential privacy (DP) can be amplified and 3) the final
performance is susceptible to hyper-parameters. A key factor leading to these
phenomena is the discordance between jointly optimizing the two low-rank
matrices by local clients and separately aggregating them by the central
server. Thus, this paper proposes an efficient and effective version of LoRA,
Federated Freeze A LoRA (FFA-LoRA), to alleviate these challenges and further
halve the communication cost of federated fine-tuning LLMs. The core idea of
FFA-LoRA is to fix the randomly initialized non-zero matrices and only
fine-tune the zero-initialized matrices. Compared to LoRA, FFA-LoRA is
motivated by practical and theoretical benefits in privacy-preserved FL. Our
experiments demonstrate that FFA-LoRA provides more consistent performance with
better computational efficiency over vanilla LoRA in various FL tasks.</div><div><a href='http://arxiv.org/abs/2403.12313v1'>2403.12313v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07159v1")'>Quantized Side Tuning: Fast and Memory-Efficient Tuning of Quantized
  Large Language Models</div>
<div id='2401.07159v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-13T21:00:21Z</div><div>Authors: Zhengxin Zhang, Dan Zhao, Xupeng Miao, Gabriele Oliaro, Qing Li, Yong Jiang, Zhihao Jia</div><div style='padding-top: 10px; width: 80ex'>Finetuning large language models (LLMs) has been empirically effective on a
variety of downstream tasks. Existing approaches to finetuning an LLM either
focus on parameter-efficient finetuning, which only updates a small number of
trainable parameters, or attempt to reduce the memory footprint during the
training phase of the finetuning. Typically, the memory footprint during
finetuning stems from three contributors: model weights, optimizer states, and
intermediate activations. However, existing works still require considerable
memory and none can simultaneously mitigate memory footprint for all three
sources. In this paper, we present Quantized Side Tuing (QST), which enables
memory-efficient and fast finetuning of LLMs by operating through a dual-stage
process. First, QST quantizes an LLM's model weights into 4-bit to reduce the
memory footprint of the LLM's original weights; QST also introduces a side
network separated from the LLM, which utilizes the hidden states of the LLM to
make task-specific predictions. Using a separate side network avoids performing
backpropagation through the LLM, thus reducing the memory requirement of the
intermediate activations. Furthermore, QST leverages several low-rank adaptors
and gradient-free downsample modules to significantly reduce the trainable
parameters, so as to save the memory footprint of the optimizer states.
Experiments show that QST can reduce the total memory footprint by up to 2.3
$\times$ and speed up the finetuning process by up to 3 $\times$ while
achieving competent performance compared with the state-of-the-art. When it
comes to full finetuning, QST can reduce the total memory footprint up to 7
$\times$.</div><div><a href='http://arxiv.org/abs/2401.07159v1'>2401.07159v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01739v1")'>OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models</div>
<div id='2402.01739v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T12:05:02Z</div><div>Authors: Fuzhao Xue, Zian Zheng, Yao Fu, Jinjie Ni, Zangwei Zheng, Wangchunshu Zhou, Yang You</div><div style='padding-top: 10px; width: 80ex'>To help the open-source community have a better understanding of
Mixture-of-Experts (MoE) based large language models (LLMs), we train and
release OpenMoE, a series of fully open-sourced and reproducible decoder-only
MoE LLMs, ranging from 650M to 34B parameters and trained on up to over 1T
tokens. Our investigation confirms that MoE-based LLMs can offer a more
favorable cost-effectiveness trade-off than dense LLMs, highlighting the
potential effectiveness for future LLM development.
  One more important contribution of this study is an in-depth analysis of the
routing mechanisms within our OpenMoE models, leading to three significant
findings: Context-Independent Specialization, Early Routing Learning, and
Drop-towards-the-End. We discovered that routing decisions in MoE models are
predominantly based on token IDs, with minimal context relevance. The
token-to-expert assignments are determined early in the pre-training phase and
remain largely unchanged. This imperfect routing can result in performance
degradation, particularly in sequential tasks like multi-turn conversations,
where tokens appearing later in a sequence are more likely to be dropped.
Finally, we rethink our design based on the above-mentioned observations and
analysis. To facilitate future MoE LLM development, we propose potential
strategies for mitigating the issues we found and further improving
off-the-shelf MoE LLM designs.</div><div><a href='http://arxiv.org/abs/2402.01739v1'>2402.01739v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07871v1")'>Scaling Laws for Fine-Grained Mixture of Experts</div>
<div id='2402.07871v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T18:33:47Z</div><div>Authors: Jakub Krajewski, Jan Ludziejewski, Kamil Adamczewski, Maciej Pióro, Michał Krutul, Szymon Antoniak, Kamil Ciebiera, Krystian Król, Tomasz Odrzygóźdź, Piotr Sankowski, Marek Cygan, Sebastian Jaszczur</div><div style='padding-top: 10px; width: 80ex'>Mixture of Experts (MoE) models have emerged as a primary solution for
reducing the computational cost of Large Language Models. In this work, we
analyze their scaling properties, incorporating an expanded range of variables.
Specifically, we introduce a new hyperparameter, granularity, whose adjustment
enables precise control over the size of the experts. Building on this, we
establish scaling laws for fine-grained MoE, taking into account the number of
training tokens, model size, and granularity. Leveraging these laws, we derive
the optimal training configuration for a given computational budget. Our
findings not only show that MoE models consistently outperform dense
Transformers but also highlight that the efficiency gap between dense and MoE
models widens as we scale up the model size and training budget. Furthermore,
we demonstrate that the common practice of setting the size of experts in MoE
to mirror the feed-forward layer is not optimal at almost any computational
budget.</div><div><a href='http://arxiv.org/abs/2402.07871v1'>2402.07871v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14800v1")'>Not All Experts are Equal: Efficient Expert Pruning and Skipping for
  Mixture-of-Experts Large Language Models</div>
<div id='2402.14800v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T18:56:07Z</div><div>Authors: Xudong Lu, Qi Liu, Yuhui Xu, Aojun Zhou, Siyuan Huang, Bo Zhang, Junchi Yan, Hongsheng Li</div><div style='padding-top: 10px; width: 80ex'>A pivotal advancement in the progress of large language models (LLMs) is the
emergence of the Mixture-of-Experts (MoE) LLMs. Compared to traditional LLMs,
MoE LLMs can achieve higher performance with fewer parameters, but it is still
hard to deploy them due to their immense parameter sizes. Different from
previous weight pruning methods that rely on specifically designed hardware,
this paper mainly aims to enhance the deployment efficiency of MoE LLMs by
introducing plug-and-play expert-level sparsification techniques. Specifically,
we propose, for the first time to our best knowledge, post-training approaches
for task-agnostic and task-specific expert pruning and skipping of MoE LLMs,
tailored to improve deployment efficiency while maintaining model performance
across a wide range of tasks. Extensive experiments show that our proposed
methods can simultaneously reduce model sizes and increase the inference speed,
while maintaining satisfactory performance. Data and code will be available at
https://github.com/Lucky-Lance/Expert_Sparsity.</div><div><a href='http://arxiv.org/abs/2402.14800v1'>2402.14800v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07033v1")'>Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts
  Models</div>
<div id='2402.07033v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-10T19:54:08Z</div><div>Authors: Keisuke Kamahori, Yile Gu, Kan Zhu, Baris Kasikci</div><div style='padding-top: 10px; width: 80ex'>Large Language Models (LLMs) based on Mixture-of-Experts (MoE) architecture
are showing promising performance on various tasks. However, running them on
resource-constrained settings, where GPU memory resources are not abundant, is
challenging due to huge model sizes. Existing systems that offload model
weights to CPU memory suffer from the significant overhead of frequently moving
data between CPU and GPU. In this paper, we propose Fiddler, a
resource-efficient inference engine with CPU-GPU orchestration for MoE models.
The key idea of Fiddler is to use the computation ability of the CPU to
minimize the data movement between the CPU and GPU. Our evaluation shows that
Fiddler can run the uncompressed Mixtral-8x7B model, which exceeds 90GB in
parameters, to generate over $3$ tokens per second on a single GPU with 24GB
memory, showing an order of magnitude improvement over existing methods. The
code of Fiddler is publicly available at
\url{https://github.com/efeslab/fiddler}</div><div><a href='http://arxiv.org/abs/2402.07033v1'>2402.07033v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08219v1")'>BBox-Adapter: Lightweight Adapting for Black-Box Large Language Models</div>
<div id='2402.08219v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T05:15:46Z</div><div>Authors: Haotian Sun, Yuchen Zhuang, Wei Wei, Chao Zhang, Bo Dai</div><div style='padding-top: 10px; width: 80ex'>Adapting state-of-the-art Large Language Models (LLMs) like GPT-4 and Gemini
for specific tasks is challenging. Due to the opacity in their parameters,
embeddings, and even output probabilities, existing fine-tuning adaptation
methods are inapplicable. Consequently, adapting these black-box LLMs is only
possible through their API services, raising concerns about transparency,
privacy, and cost. To address these challenges, we introduce BBox-Adapter, a
novel lightweight adapter for black-box LLMs. BBox-Adapter distinguishes target
and source domain data by treating target data as positive and source data as
negative. It employs a ranking-based Noise Contrastive Estimation (NCE) loss to
promote the likelihood of target domain data while penalizing that of the
source domain. Furthermore, it features an online adaptation mechanism, which
incorporates real-time positive data sampling from ground-truth, human, or AI
feedback, coupled with negative data from previous adaptations. Extensive
experiments demonstrate BBox-Adapter's effectiveness and cost efficiency. It
improves model performance by up to 6.77% across diverse tasks and domains,
while reducing training and inference costs by 31.30x and 1.84x, respectively.</div><div><a href='http://arxiv.org/abs/2402.08219v1'>2402.08219v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15607v1")'>Training Nonlinear Transformers for Efficient In-Context Learning: A
  Theoretical Learning and Generalization Analysis</div>
<div id='2402.15607v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T21:07:20Z</div><div>Authors: Hongkang Li, Meng Wang, Songtao Lu, Xiaodong Cui, Pin-Yu Chen</div><div style='padding-top: 10px; width: 80ex'>Transformer-based large language models have displayed impressive in-context
learning capabilities, where a pre-trained model can handle new tasks without
fine-tuning by simply augmenting the query with some input-output examples from
that task. Despite the empirical success, the mechanics of how to train a
Transformer to achieve ICL and the corresponding ICL capacity is mostly elusive
due to the technical challenges of analyzing the nonconvex training problems
resulting from the nonlinear self-attention and nonlinear activation in
Transformers. To the best of our knowledge, this paper provides the first
theoretical analysis of the training dynamics of Transformers with nonlinear
self-attention and nonlinear MLP, together with the ICL generalization
capability of the resulting model. Focusing on a group of binary classification
tasks, we train Transformers using data from a subset of these tasks and
quantify the impact of various factors on the ICL generalization performance on
the remaining unseen tasks with and without data distribution shifts. We also
analyze how different components in the learned Transformers contribute to the
ICL performance. Furthermore, we provide the first theoretical analysis of how
model pruning affects the ICL performance and prove that proper magnitude-based
pruning can have a minimal impact on ICL while reducing inference costs. These
theoretical findings are justified through numerical experiments.</div><div><a href='http://arxiv.org/abs/2402.15607v1'>2402.15607v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04317v1")'>Online Adaptation of Language Models with a Memory of Amortized Contexts</div>
<div id='2403.04317v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T08:34:57Z</div><div>Authors: Jihoon Tack, Jaehyung Kim, Eric Mitchell, Jinwoo Shin, Yee Whye Teh, Jonathan Richard Schwarz</div><div style='padding-top: 10px; width: 80ex'>Due to the rapid generation and dissemination of information, large language
models (LLMs) quickly run out of date despite enormous development costs. Due
to this crucial need to keep models updated, online learning has emerged as a
critical necessity when utilizing LLMs for real-world applications. However,
given the ever-expanding corpus of unseen documents and the large parameter
space of modern LLMs, efficient adaptation is essential. To address these
challenges, we propose Memory of Amortized Contexts (MAC), an efficient and
effective online adaptation framework for LLMs with strong knowledge retention.
We propose an amortized feature extraction and memory-augmentation approach to
compress and extract information from new documents into compact modulations
stored in a memory bank. When answering questions, our model attends to and
extracts relevant knowledge from this memory bank. To learn informative
modulations in an efficient manner, we utilize amortization-based
meta-learning, which substitutes the optimization process with a single forward
pass of the encoder. Subsequently, we learn to choose from and aggregate
selected documents into a single modulation by conditioning on the question,
allowing us to adapt a frozen language model during test time without requiring
further gradient updates. Our experiment demonstrates the superiority of MAC in
multiple aspects, including online adaptation performance, time, and memory
efficiency. Code is available at: https://github.com/jihoontack/MAC.</div><div><a href='http://arxiv.org/abs/2403.04317v1'>2403.04317v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09997v1")'>LoraRetriever: Input-Aware LoRA Retrieval and Composition for Mixed
  Tasks in the Wild</div>
<div id='2402.09997v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T15:02:46Z</div><div>Authors: Ziyu Zhao, Leilei Gan, Guoyin Wang, Wangchunshu Zhou, Hongxia Yang, Kun Kuang, Fei Wu</div><div style='padding-top: 10px; width: 80ex'>Low-Rank Adaptation (LoRA) provides an effective yet efficient solution for
fine-tuning large language models (LLM). The modular and plug-and-play nature
of LoRA enables the integration of diverse domain-specific LoRAs to enhance the
capabilities of LLMs. Previous research on exploiting multiple LoRAs either
focuses on specific isolated downstream tasks or fixes the selection of LoRAs
during training. However, in real-world scenarios, LLMs receive diverse prompts
covering different tasks, and the pool of candidate LoRAs is often dynamically
updated. To bridge this gap, we propose LoraRetriever, a retrieve-then-compose
framework that adaptively retrieves and composes multiple LoRAs according to
the input prompts. LoraRetriever contains three main components: firstly,
identifying and retrieving LoRAs relevant to the given input; secondly,
formulating strategies for effectively integrating the retrieved LoRAs; and
thirdly, developing efficient batch inference to accommodate heterogeneous
requests. Experimental results indicate that LoraRetriever consistently
outperforms the baselines, highlighting its practical effectiveness and
versatility.</div><div><a href='http://arxiv.org/abs/2402.09997v1'>2402.09997v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14074v1")'>M3: A Multi-Task Mixed-Objective Learning Framework for Open-Domain
  Multi-Hop Dense Sentence Retrieval</div>
<div id='2403.14074v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T01:52:07Z</div><div>Authors: Yang Bai, Anthony Colas, Christan Grant, Daisy Zhe Wang</div><div style='padding-top: 10px; width: 80ex'>In recent research, contrastive learning has proven to be a highly effective
method for representation learning and is widely used for dense retrieval.
However, we identify that relying solely on contrastive learning can lead to
suboptimal retrieval performance. On the other hand, despite many retrieval
datasets supporting various learning objectives beyond contrastive learning,
combining them efficiently in multi-task learning scenarios can be challenging.
In this paper, we introduce M3, an advanced recursive Multi-hop dense sentence
retrieval system built upon a novel Multi-task Mixed-objective approach for
dense text representation learning, addressing the aforementioned challenges.
Our approach yields state-of-the-art performance on a large-scale open-domain
fact verification benchmark dataset, FEVER. Code and data are available at:
https://github.com/TonyBY/M3</div><div><a href='http://arxiv.org/abs/2403.14074v1'>2403.14074v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06683v2")'>DQNC2S: DQN-based Cross-stream Crisis event Summarizer</div>
<div id='2401.06683v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T16:43:28Z</div><div>Authors: Daniele Rege Cambrin, Luca Cagliero, Paolo Garza</div><div style='padding-top: 10px; width: 80ex'>Summarizing multiple disaster-relevant data streams simultaneously is
particularly challenging as existing Retrieve&amp;Re-ranking strategies suffer from
the inherent redundancy of multi-stream data and limited scalability in a
multi-query setting. This work proposes an online approach to crisis timeline
generation based on weak annotation with Deep Q-Networks. It selects on-the-fly
the relevant pieces of text without requiring neither human annotations nor
content re-ranking. This makes the inference time independent of the number of
input queries. The proposed approach also incorporates a redundancy filter into
the reward function to effectively handle cross-stream content overlaps. The
achieved ROUGE and BERTScore results are superior to those of best-performing
models on the CrisisFACTS 2022 benchmark.</div><div><a href='http://arxiv.org/abs/2401.06683v2'>2401.06683v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.13482v1")'>Retrieval-Augmented Data Augmentation for Low-Resource Domain Tasks</div>
<div id='2402.13482v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T02:45:46Z</div><div>Authors: Minju Seo, Jinheon Baek, James Thorne, Sung Ju Hwang</div><div style='padding-top: 10px; width: 80ex'>Despite large successes of recent language models on diverse tasks, they
suffer from severe performance degeneration in low-resource settings with
limited training data available. Many existing works tackle this problem by
generating synthetic data from the training data and then training models on
them, recently using Large Language Models (LLMs). However, in low-resource
settings, the amount of seed data samples to use for data augmentation is very
small, which makes generated samples suboptimal and less diverse. To tackle
this challenge, we propose a novel method that augments training data by
incorporating a wealth of examples from other datasets, along with the given
training data. Specifically, we first retrieve the relevant instances from
other datasets, such as their input-output pairs or contexts, based on their
similarities with the given seed data, and then prompt LLMs to generate new
samples with the contextual information within and across the original and
retrieved samples. This approach can ensure that the generated data is not only
relevant but also more diverse than what could be achieved using the limited
seed data alone. We validate our proposed Retrieval-Augmented Data Augmentation
(RADA) framework on multiple datasets under low-resource settings of training
and test-time data augmentation scenarios, on which it outperforms existing
LLM-powered data augmentation baselines.</div><div><a href='http://arxiv.org/abs/2402.13482v1'>2402.13482v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12177v4")'>Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-Tuning</div>
<div id='2402.12177v4' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T14:33:24Z</div><div>Authors: Mingtian Zhang, Shawn Lan, Peter Hayes, David Barber</div><div style='padding-top: 10px; width: 80ex'>Retrieval Augmented Generation (RAG) has emerged as an effective solution for
mitigating hallucinations in Large Language Models (LLMs). The retrieval stage
in RAG typically involves a pre-trained embedding model, which converts queries
and passages into vectors to capture their semantics. However, a standard
pre-trained embedding model may exhibit sub-optimal performance when applied to
specific domain knowledge, necessitating fine-tuning. This paper addresses
scenarios where the embeddings are only available from a black-box model. We
introduce Model augmented fine-tuning (Mafin) -- a novel approach for
fine-tuning a black-box embedding model by augmenting it with a trainable
embedding model. Our results demonstrate that Mafin significantly enhances the
performance of the black-box embeddings by only requiring the training of a
small augmented model. We validate the effectiveness of our method on both
labeled and unlabeled datasets, illustrating its broad applicability and
efficiency.</div><div><a href='http://arxiv.org/abs/2402.12177v4'>2402.12177v4</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16829v1")'>GISTEmbed: Guided In-sample Selection of Training Negatives for Text
  Embedding Fine-tuning</div>
<div id='2402.16829v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T18:55:15Z</div><div>Authors: Aivin V. Solatorio</div><div style='padding-top: 10px; width: 80ex'>Embedding models are integral to AI applications like semantic search,
personalized recommendations, and retrieval augmented generation for LLMs,
necessitating high-quality training data. However, the limited scalability of
manual data curation prompts the need for automated methods to ensure data
integrity. Traditional unsupervised triplet mining automates training data
generation, crucial for embedding model training, yet inadvertently injects
biases and noise, thereby degrading model performance. Addressing this, we
introduce GISTEmbed, a novel strategy that enhances in-batch negative selection
during contrastive training through a guide model. This approach departs from
reliance on random sampling and equal utility assumption of batch negatives,
significantly reducing noise from data quality issues and improving model
fine-tuning. Benchmarked against the Massive Text Embedding Benchmark (MTEB),
GISTEmbed showcases consistent performance improvements across various model
sizes and achieves state-of-the-art results in select categories. This
framework enables significant enhancements for smaller models by leveraging the
capabilities of powerful yet resource-intensive large models. GISTEmbed can
potentially revolutionize the creation of highly efficient, smaller models,
democratizing access to advanced AI technologies. Making these technologies
more accessible and cost-effective, especially for applications constrained by
resources, significantly expands the impact and accessibility of
state-of-the-art AI solutions across diverse sectors.</div><div><a href='http://arxiv.org/abs/2402.16829v1'>2402.16829v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03216v3")'>BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity
  Text Embeddings Through Self-Knowledge Distillation</div>
<div id='2402.03216v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T17:26:49Z</div><div>Authors: Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, Zheng Liu</div><div style='padding-top: 10px; width: 80ex'>In this paper, we present a new embedding model, called M3-Embedding, which
is distinguished for its versatility in Multi-Linguality, Multi-Functionality,
and Multi-Granularity. It can support more than 100 working languages, leading
to new state-of-the-art performances on multi-lingual and cross-lingual
retrieval tasks. It can simultaneously perform the three common retrieval
functionalities of embedding model: dense retrieval, multi-vector retrieval,
and sparse retrieval, which provides a unified model foundation for real-world
IR applications. It is able to process inputs of different granularities,
spanning from short sentences to long documents of up to 8192 tokens. The
effective training of M3-Embedding involves the following technical
contributions. We propose a novel self-knowledge distillation approach, where
the relevance scores from different retrieval functionalities can be integrated
as the teacher signal to enhance the training quality. We also optimize the
batching strategy, enabling a large batch size and high training throughput to
ensure the discriminativeness of embeddings. To the best of our knowledge,
M3-Embedding is the first embedding model which realizes such a strong
versatility. The model and code will be publicly available at
https://github.com/FlagOpen/FlagEmbedding.</div><div><a href='http://arxiv.org/abs/2402.03216v3'>2402.03216v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06583v1")'>Mapping Transformer Leveraged Embeddings for Cross-Lingual Document
  Representation</div>
<div id='2401.06583v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T14:01:15Z</div><div>Authors: Tsegaye Misikir Tashu, Eduard-Raul Kontos, Matthia Sabatelli, Matias Valdenegro-Toro</div><div style='padding-top: 10px; width: 80ex'>Recommendation systems, for documents, have become tools to find relevant
content on the Web. However, these systems have limitations when it comes to
recommending documents in languages different from the query language, which
means they might overlook resources in non-native languages. This research
focuses on representing documents across languages by using Transformer
Leveraged Document Representations (TLDRs) that are mapped to a cross-lingual
domain. Four multilingual pre-trained transformer models (mBERT, mT5 XLM
RoBERTa, ErnieM) were evaluated using three mapping methods across 20 language
pairs representing combinations of five selected languages of the European
Union. Metrics like Mate Retrieval Rate and Reciprocal Rank were used to
measure the effectiveness of mapped TLDRs compared to non-mapped ones. The
results highlight the power of cross-lingual representations achieved through
pre-trained transformers and mapping approaches suggesting a promising
direction for expanding beyond language connections, between two specific
languages.</div><div><a href='http://arxiv.org/abs/2401.06583v1'>2401.06583v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05792v1")'>Discovering Low-rank Subspaces for Language-agnostic Multilingual
  Representations</div>
<div id='2401.05792v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T09:54:11Z</div><div>Authors: Zhihui Xie, Handong Zhao, Tong Yu, Shuai Li</div><div style='padding-top: 10px; width: 80ex'>Large pretrained multilingual language models (ML-LMs) have shown remarkable
capabilities of zero-shot cross-lingual transfer, without direct cross-lingual
supervision. While these results are promising, follow-up works found that,
within the multilingual embedding spaces, there exists strong language identity
information which hinders the expression of linguistic factors shared across
languages. For semantic tasks like cross-lingual sentence retrieval, it is
desired to remove such language identity signals to fully leverage semantic
information. In this work, we provide a novel view of projecting away
language-specific factors from a multilingual embedding space. Specifically, we
discover that there exists a low-rank subspace that primarily encodes
information irrelevant to semantics (e.g., syntactic information). To identify
this subspace, we present a simple but effective unsupervised method based on
singular value decomposition with multiple monolingual corpora as input. Once
the subspace is found, we can directly project the original embeddings into the
null space to boost language agnosticism without finetuning. We systematically
evaluate our method on various tasks including the challenging
language-agnostic QA retrieval task. Empirical results show that applying our
method consistently leads to improvements over commonly used ML-LMs.</div><div><a href='http://arxiv.org/abs/2401.05792v1'>2401.05792v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12663v1")'>SoftQE: Learned Representations of Queries Expanded by LLMs</div>
<div id='2402.12663v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T02:23:15Z</div><div>Authors: Varad Pimpalkhute, John Heyer, Xusen Yin, Sameer Gupta</div><div style='padding-top: 10px; width: 80ex'>We investigate the integration of Large Language Models (LLMs) into query
encoders to improve dense retrieval without increasing latency and cost, by
circumventing the dependency on LLMs at inference time. SoftQE incorporates
knowledge from LLMs by mapping embeddings of input queries to those of the
LLM-expanded queries. While improvements over various strong baselines on
in-domain MS-MARCO metrics are marginal, SoftQE improves performance by 2.83
absolute percentage points on average on five out-of-domain BEIR tasks.</div><div><a href='http://arxiv.org/abs/2402.12663v1'>2402.12663v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.09891v2")'>Fisher Mask Nodes for Language Model Merging</div>
<div id='2403.09891v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T21:52:26Z</div><div>Authors: Thennal D K, Ganesh Nathan, Suchithra M S</div><div style='padding-top: 10px; width: 80ex'>Fine-tuning pre-trained models provides significant advantages in downstream
performance. The ubiquitous nature of pre-trained models such as BERT and its
derivatives in natural language processing has also led to a proliferation of
task-specific fine-tuned models. As these models typically only perform one
task well, additional training or ensembling is required in multi-task
scenarios. The growing field of model merging provides a solution, dealing with
the challenge of combining multiple task-specific models into a single
multi-task model. In this study, we introduce a novel model merging method for
Transformers, combining insights from previous work in Fisher-weighted
averaging and the use of Fisher information in model pruning. Utilizing the
Fisher information of mask nodes within the Transformer architecture, we devise
a computationally efficient weighted-averaging scheme. Our method exhibits a
regular and significant performance increase across various models in the BERT
family, outperforming full-scale Fisher-weighted averaging in a fraction of the
computational cost, with baseline performance improvements of up to +6.5 and a
speedup of 57.4x in the biggest model. Our results prove the potential of our
method in current multi-task learning environments and suggest its scalability
and adaptability to new model architectures and learning scenarios.</div><div><a href='http://arxiv.org/abs/2403.09891v2'>2403.09891v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12408v1")'>ModelGPT: Unleashing LLM's Capabilities for Tailored Model Generation</div>
<div id='2402.12408v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T11:24:34Z</div><div>Authors: Zihao Tang, Zheqi Lv, Shengyu Zhang, Fei Wu, Kun Kuang</div><div style='padding-top: 10px; width: 80ex'>The rapid advancement of Large Language Models (LLMs) has revolutionized
various sectors by automating routine tasks, marking a step toward the
realization of Artificial General Intelligence (AGI). However, they still
struggle to accommodate the diverse and specific needs of users and simplify
the utilization of AI models for the average user. In response, we propose
ModelGPT, a novel framework designed to determine and generate AI models
specifically tailored to the data or task descriptions provided by the user,
leveraging the capabilities of LLMs. Given user requirements, ModelGPT is able
to provide tailored models at most 270x faster than the previous paradigms
(e.g. all-parameter or LoRA finetuning). Comprehensive experiments on NLP, CV,
and Tabular datasets attest to the effectiveness of our framework in making AI
models more accessible and user-friendly. Our code is available at
https://github.com/IshiKura-a/ModelGPT.</div><div><a href='http://arxiv.org/abs/2402.12408v1'>2402.12408v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.01308v2")'>VBART: The Turkish LLM</div>
<div id='2403.01308v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T20:40:11Z</div><div>Authors: Meliksah Turker, Mehmet Erdi Ari, Aydin Han</div><div style='padding-top: 10px; width: 80ex'>We present VBART, the first Turkish sequence-to-sequence Large Language
Models (LLMs) pre-trained on a large corpus from scratch. VBART are compact
LLMs based on good ideas leveraged from BART and mBART models and come in two
sizes, Large and XLarge. Fine-tuned VBART models surpass the prior
state-of-the-art results in abstractive text summarization, title generation,
text paraphrasing, question answering and question generation tasks. They allow
fine-tuning for future text generation tasks and datasets, carving a new path
for Turkish Natural Language Processing (NLP) research. Our work shows that
having a pre-trained LLM for Turkish outperforms up to 3x multilingual models,
improving existing results and providing efficient models for training and
inference. Moreover, we show that our monolingual tokenizer is up to 11x more
efficient than multilingual tokenizers. Last but not least, we introduce a
method to enlarge an existing pre-trained LLM and question the relevancy of
Chinchilla Scaling Law to sequence-to-sequence masked language models. Our
fine-tuned models, tokenizer and cleaned vngrs-web-corpus of 135 GB are
publicly available at huggingface.co/vngrs-ai.</div><div><a href='http://arxiv.org/abs/2403.01308v2'>2403.01308v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16640v1")'>TeenyTinyLlama: open-source tiny language models trained in Brazilian
  Portuguese</div>
<div id='2401.16640v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T00:25:54Z</div><div>Authors: Nicholas Kluge Corrêa, Sophia Falk, Shiza Fatimah, Aniket Sen, Nythamar de Oliveira</div><div style='padding-top: 10px; width: 80ex'>Large language models (LLMs) have significantly advanced natural language
processing, but their progress has yet to be equal across languages. While most
LLMs are trained in high-resource languages like English, multilingual models
generally underperform monolingual ones. Additionally, aspects of their
multilingual foundation sometimes restrict the byproducts they produce, like
computational demands and licensing regimes. In this study, we document the
development of open-foundation models tailored for use in low-resource
settings, their limitations, and their benefits. This is the TeenyTinyLlama
pair: two compact models for Brazilian Portuguese text generation. We release
them under the permissive Apache 2.0 license on GitHub and Hugging Face for
community use and further development. See
https://github.com/Nkluge-correa/TeenyTinyLlama</div><div><a href='http://arxiv.org/abs/2401.16640v1'>2401.16640v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14373v1")'>TURNA: A Turkish Encoder-Decoder Language Model for Enhanced
  Understanding and Generation</div>
<div id='2401.14373v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T18:24:13Z</div><div>Authors: Gökçe Uludoğan, Zeynep Yirmibeşoğlu Balal, Furkan Akkurt, Melikşah Türker, Onur Güngör, Susan Üsküdarlı</div><div style='padding-top: 10px; width: 80ex'>The recent advances in natural language processing have predominantly favored
well-resourced English-centric models, resulting in a significant gap with
low-resource languages. In this work, we introduce the language model TURNA,
which is developed for the low-resource language Turkish and is capable of both
natural language understanding and generation tasks. TURNA is pretrained with
an encoder-decoder architecture based on the unified framework UL2 with a
diverse corpus that we specifically curated for this purpose. We evaluated
TURNA with three generation tasks and five understanding tasks for Turkish. The
results show that TURNA outperforms several multilingual models in both
understanding and generation tasks, and competes with monolingual Turkish
models in understanding tasks. TURNA is made available at
https://huggingface.co/boun-tabi-LMG/TURNA .</div><div><a href='http://arxiv.org/abs/2401.14373v1'>2401.14373v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00964v1")'>MALTO at SemEval-2024 Task 6: Leveraging Synthetic Data for LLM
  Hallucination Detection</div>
<div id='2403.00964v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T20:31:10Z</div><div>Authors: Federico Borra, Claudio Savelli, Giacomo Rosso, Alkis Koudounas, Flavio Giobergia</div><div style='padding-top: 10px; width: 80ex'>In Natural Language Generation (NLG), contemporary Large Language Models
(LLMs) face several challenges, such as generating fluent yet inaccurate
outputs and reliance on fluency-centric metrics. This often leads to neural
networks exhibiting "hallucinations". The SHROOM challenge focuses on
automatically identifying these hallucinations in the generated text. To tackle
these issues, we introduce two key components, a data augmentation pipeline
incorporating LLM-assisted pseudo-labelling and sentence rephrasing, and a
voting ensemble from three models pre-trained on Natural Language Inference
(NLI) tasks and fine-tuned on diverse datasets.</div><div><a href='http://arxiv.org/abs/2403.00964v1'>2403.00964v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00252v1")'>EUROPA: A Legal Multilingual Keyphrase Generation Dataset</div>
<div id='2403.00252v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T03:30:38Z</div><div>Authors: Olivier Salaün, Frédéric Piedboeuf, Guillaume Le Berre, David Alfonso Hermelo, Philippe Langlais</div><div style='padding-top: 10px; width: 80ex'>Keyphrase generation has primarily been explored within the context of
academic research articles, with a particular focus on scientific domains and
the English language. In this work, we present EUROPA, a dataset for
multilingual keyphrase generation in the legal domain. It is derived from legal
judgments from the Court of Justice of the European Union (EU), and contains
instances in all 24 EU official languages. We run multilingual models on our
corpus and analyze the results, showing room for improvement on a
domain-specific multilingual corpus such as the one we present.</div><div><a href='http://arxiv.org/abs/2403.00252v1'>2403.00252v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.06018v1")'>Few-Shot Cross-Lingual Transfer for Prompting Large Language Models in
  Low-Resource Languages</div>
<div id='2403.06018v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-09T21:36:13Z</div><div>Authors: Christopher Toukmaji</div><div style='padding-top: 10px; width: 80ex'>Large pre-trained language models (PLMs) are at the forefront of advances in
Natural Language Processing. One widespread use case of PLMs is "prompting" -
or in-context learning - where a user provides a description of a task and some
completed examples of the task to a PLM as context before prompting the PLM to
perform the task on a new example. Only the largest, most capable PLMs are able
to perform in-context learning effectively, and these models are typically
trained with a predominantly English corpus, leaving all other languages
behind. The data limitations in most languages preclude the training of
language-specific PLMs capable of prompting. Albeit the surge in work of
prompting settings, it is still unclear how PLMs should be adapted
cross-lingually specifically for prompting. We evaluate the possible methods to
adapt LLaMa, a 7B parameter open-source PLM mainly trained in English, for
prompting in low-resource languages, namely for Kinyarwanda, Hausa, and
Luganda. We consider three methods: few-shot prompting (prompt),
language-adaptive fine-tuning (LAFT), and neural machine translation
(translate), and evaluate on abstractive summarization, multi-class topic
classification, and named-entity recognition. Although LAFT carries the
greatest compute cost and intuitively should lead to the best results, our
experiments exhibit that LAFT is only occasionally the optimal choice for
adapting PLMs for prompting. Rather, the translate and prompt settings are a
compute-efficient and cost-effective method of few-shot prompting for the
selected low-resource languages. We find that the results are task and language
dependent but find that the prompting method is the best on average across all
tasks and languages. Results show that the prompt setting performs better than
both translating and LAFT with statistical significance for all shots when
aggregated across all tasks and languages.</div><div><a href='http://arxiv.org/abs/2403.06018v1'>2403.06018v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08429v1")'>Machine Translation with Large Language Models: Prompt Engineering for
  Persian, English, and Russian Directions</div>
<div id='2401.08429v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T15:16:34Z</div><div>Authors: Nooshin Pourkamali, Shler Ebrahim Sharifi</div><div style='padding-top: 10px; width: 80ex'>Generative large language models (LLMs) have demonstrated exceptional
proficiency in various natural language processing (NLP) tasks, including
machine translation, question answering, text summarization, and natural
language understanding.
  To further enhance the performance of LLMs in machine translation, we
conducted an investigation into two popular prompting methods and their
combination, focusing on cross-language combinations of Persian, English, and
Russian. We employed n-shot feeding and tailored prompting frameworks. Our
findings indicate that multilingual LLMs like PaLM exhibit human-like machine
translation outputs, enabling superior fine-tuning of desired translation
nuances in accordance with style guidelines and linguistic considerations.
These models also excel in processing and applying prompts. However, the choice
of language model, machine translation task, and the specific source and target
languages necessitate certain considerations when adopting prompting frameworks
and utilizing n-shot in-context learning.
  Furthermore, we identified errors and limitations inherent in popular LLMs as
machine translation tools and categorized them based on various linguistic
metrics. This typology of errors provides valuable insights for utilizing LLMs
effectively and offers methods for designing prompts for in-context learning.
Our report aims to contribute to the advancement of machine translation with
LLMs by improving both the accuracy and reliability of evaluation metrics.</div><div><a href='http://arxiv.org/abs/2401.08429v1'>2401.08429v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14837v1")'>An Empirical Categorization of Prompting Techniques for Large Language
  Models: A Practitioner's Guide</div>
<div id='2402.14837v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T23:03:56Z</div><div>Authors: Oluwole Fagbohun, Rachel M. Harrison, Anton Dereventsov</div><div style='padding-top: 10px; width: 80ex'>Due to rapid advancements in the development of Large Language Models (LLMs),
programming these models with prompts has recently gained significant
attention. However, the sheer number of available prompt engineering techniques
creates an overwhelming landscape for practitioners looking to utilize these
tools. For the most efficient and effective use of LLMs, it is important to
compile a comprehensive list of prompting techniques and establish a
standardized, interdisciplinary categorization framework. In this survey, we
examine some of the most well-known prompting techniques from both academic and
practical viewpoints and classify them into seven distinct categories. We
present an overview of each category, aiming to clarify their unique
contributions and showcase their practical applications in real-world examples
in order to equip fellow practitioners with a structured framework for
understanding and categorizing prompting techniques tailored to their specific
domains. We believe that this approach will help simplify the complex landscape
of prompt engineering and enable more effective utilization of LLMs in various
applications. By providing practitioners with a systematic approach to prompt
categorization, we aim to assist in navigating the intricacies of effective
prompt design for conversational pre-trained LLMs and inspire new possibilities
in their respective fields.</div><div><a href='http://arxiv.org/abs/2402.14837v1'>2402.14837v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14423v3")'>Prompt Design and Engineering: Introduction and Advanced Methods</div>
<div id='2401.14423v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T06:20:18Z</div><div>Authors: Xavier Amatriain</div><div style='padding-top: 10px; width: 80ex'>Prompt design and engineering has rapidly become essential for maximizing the
potential of large language models. In this paper, we introduce core concepts,
advanced techniques like Chain-of-Thought and Reflection, and the principles
behind building LLM-based agents. Finally, we provide a survey of tools for
prompt engineers.</div><div><a href='http://arxiv.org/abs/2401.14423v3'>2401.14423v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14447v1")'>Wordflow: Social Prompt Engineering for Large Language Models</div>
<div id='2401.14447v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T18:58:11Z</div><div>Authors: Zijie J. Wang, Aishwarya Chakravarthy, David Munechika, Duen Horng Chau</div><div style='padding-top: 10px; width: 80ex'>Large language models (LLMs) require well-crafted prompts for effective use.
Prompt engineering, the process of designing prompts, is challenging,
particularly for non-experts who are less familiar with AI technologies. While
researchers have proposed techniques and tools to assist LLM users in prompt
design, these works primarily target AI application developers rather than
non-experts. To address this research gap, we propose social prompt
engineering, a novel paradigm that leverages social computing techniques to
facilitate collaborative prompt design. To investigate social prompt
engineering, we introduce Wordflow, an open-source and social text editor that
enables everyday users to easily create, run, share, and discover LLM prompts.
Additionally, by leveraging modern web technologies, Wordflow allows users to
run LLMs locally and privately in their browsers. Two usage scenarios highlight
how social prompt engineering and our tool can enhance laypeople's interaction
with LLMs. Wordflow is publicly accessible at
https://poloclub.github.io/wordflow.</div><div><a href='http://arxiv.org/abs/2401.14447v1'>2401.14447v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02575v1")'>Large Language Models for Social Networks: Applications, Challenges, and
  Solutions</div>
<div id='2401.02575v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T23:37:48Z</div><div>Authors: Jingying Zeng, Richard Huang, Waleed Malik, Langxuan Yin, Bojan Babic, Danny Shacham, Xiao Yan, Jaewon Yang, Qi He</div><div style='padding-top: 10px; width: 80ex'>Large Language Models (LLMs) are transforming the way people generate,
explore, and engage with content. We study how we can develop LLM applications
for online social networks. Despite LLMs' successes in other domains, it is
challenging to develop LLM-based products for social networks for numerous
reasons, and it has been relatively under-reported in the research community.
We categorize LLM applications for social networks into three categories. First
is knowledge tasks where users want to find new knowledge and information, such
as search and question-answering. Second is entertainment tasks where users
want to consume interesting content, such as getting entertaining notification
content. Third is foundational tasks that need to be done to moderate and
operate the social networks, such as content annotation and LLM monitoring. For
each task, we share the challenges we found, solutions we developed, and
lessons we learned. To the best of our knowledge, this is the first
comprehensive paper about developing LLM applications for social networks.</div><div><a href='http://arxiv.org/abs/2401.02575v1'>2401.02575v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10707v1")'>Uncovering Latent Themes of Messaging on Social Media by Integrating
  LLMs: A Case Study on Climate Campaigns</div>
<div id='2403.10707v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T21:54:00Z</div><div>Authors: Tunazzina Islam, Dan Goldwasser</div><div style='padding-top: 10px; width: 80ex'>This paper introduces a novel approach to uncovering and analyzing themes in
social media messaging. Recognizing the limitations of traditional topic-level
analysis, which tends to capture only the overarching patterns, this study
emphasizes the need for a finer-grained, theme-focused exploration.
Conventional methods of theme discovery, involving manual processes and a
human-in-the-loop approach, are valuable but face challenges in scalability,
consistency, and resource intensity in terms of time and cost. To address these
challenges, we propose a machine-in-the-loop approach that leverages the
advanced capabilities of Large Language Models (LLMs). This approach allows for
a deeper investigation into the thematic aspects of social media discourse,
enabling us to uncover a diverse array of themes, each with unique
characteristics and relevance, thereby offering a comprehensive understanding
of the nuances present within broader topics. Furthermore, this method
efficiently maps the text and the newly discovered themes, enhancing our
understanding of the thematic nuances in social media messaging. We employ
climate campaigns as a case study and demonstrate that our methodology yields
more accurate and interpretable results compared to traditional topic models.
Our results not only demonstrate the effectiveness of our approach in
uncovering latent themes but also illuminate how these themes are tailored for
demographic targeting in social media contexts. Additionally, our work sheds
light on the dynamic nature of social media, revealing the shifts in the
thematic focus of messaging in response to real-world events.</div><div><a href='http://arxiv.org/abs/2403.10707v1'>2403.10707v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13528v2")'>Infrastructure Ombudsman: Mining Future Failure Concerns from Structural
  Disaster Response</div>
<div id='2402.13528v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T04:55:03Z</div><div>Authors: Md Towhidul Absar Chowdhury, Soumyajit Datta, Naveen Sharma, Ashiqur R. KhudaBukhsh</div><div style='padding-top: 10px; width: 80ex'>Current research concentrates on studying discussions on social media related
to structural failures to improve disaster response strategies. However,
detecting social web posts discussing concerns about anticipatory failures is
under-explored. If such concerns are channeled to the appropriate authorities,
it can aid in the prevention and mitigation of potential infrastructural
failures. In this paper, we develop an infrastructure ombudsman -- that
automatically detects specific infrastructure concerns. Our work considers
several recent structural failures in the US. We present a first-of-its-kind
dataset of 2,662 social web instances for this novel task mined from Reddit and
YouTube.</div><div><a href='http://arxiv.org/abs/2402.13528v2'>2402.13528v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.02987v1")'>Conversation Reconstruction Attack Against GPT Models</div>
<div id='2402.02987v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T13:18:42Z</div><div>Authors: Junjie Chu, Zeyang Sha, Michael Backes, Yang Zhang</div><div style='padding-top: 10px; width: 80ex'>In recent times, significant advancements have been made in the field of
large language models (LLMs), represented by GPT series models. To optimize
task execution, users often engage in multi-round conversations with GPT models
hosted in cloud environments. These multi-round conversations, potentially
replete with private information, require transmission and storage within the
cloud. However, this operational paradigm introduces additional attack
surfaces. In this paper, we first introduce a specific Conversation
Reconstruction Attack targeting GPT models. Our introduced Conversation
Reconstruction Attack is composed of two steps: hijacking a session and
reconstructing the conversations. Subsequently, we offer an exhaustive
evaluation of the privacy risks inherent in conversations when GPT models are
subjected to the proposed attack. However, GPT-4 demonstrates certain
robustness to the proposed attacks. We then introduce two advanced attacks
aimed at better reconstructing previous conversations, specifically the UNR
attack and the PBU attack. Our experimental findings indicate that the PBU
attack yields substantial performance across all models, achieving semantic
similarity scores exceeding 0.60, while the UNR attack is effective solely on
GPT-3.5. Our results reveal the concern about privacy risks associated with
conversations involving GPT models and aim to draw the community's attention to
prevent the potential misuse of these models' remarkable capabilities. We will
responsibly disclose our findings to the suppliers of related large language
models.</div><div><a href='http://arxiv.org/abs/2402.02987v1'>2402.02987v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.02675v1")'>LMaaS: Exploring Pricing Strategy of Large Model as a Service for
  Communication</div>
<div id='2401.02675v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T07:19:19Z</div><div>Authors: Panlong Wu, Qi Liu, Yanjie Dong, Fangxin Wang</div><div style='padding-top: 10px; width: 80ex'>The next generation of communication is envisioned to be intelligent
communication, that can replace traditional symbolic communication, where
highly condensed semantic information considering both source and channel will
be extracted and transmitted with high efficiency. The recent popular large
models such as GPT4 and the boosting learning techniques lay a solid foundation
for the intelligent communication, and prompt the practical deployment of it in
the near future. Given the characteristics of "training once and widely use" of
those multimodal large language models, we argue that a pay-as-you-go service
mode will be suitable in this context, referred to as Large Model as a Service
(LMaaS). However, the trading and pricing problem is quite complex with
heterogeneous and dynamic customer environments, making the pricing
optimization problem challenging in seeking on-hand solutions. In this paper,
we aim to fill this gap and formulate the LMaaS market trading as a Stackelberg
game with two steps. In the first step, we optimize the seller's pricing
decision and propose an Iterative Model Pricing (IMP) algorithm that optimizes
the prices of large models iteratively by reasoning customers' future rental
decisions, which is able to achieve a near-optimal pricing solution. In the
second step, we optimize customers' selection decisions by designing a robust
selecting and renting (RSR) algorithm, which is guaranteed to be optimal with
rigorous theoretical proof. Extensive experiments confirm the effectiveness and
robustness of our algorithms.</div><div><a href='http://arxiv.org/abs/2401.02675v1'>2401.02675v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.10408v1")'>SocialGenPod: Privacy-Friendly Generative AI Social Web Applications
  with Decentralised Personal Data Stores</div>
<div id='2403.10408v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T15:43:02Z</div><div>Authors: Vidminas Vizgirda, Rui Zhao, Naman Goel</div><div style='padding-top: 10px; width: 80ex'>We present SocialGenPod, a decentralised and privacy-friendly way of
deploying generative AI Web applications. Unlike centralised Web and data
architectures that keep user data tied to application and service providers, we
show how one can use Solid -- a decentralised Web specification -- to decouple
user data from generative AI applications. We demonstrate SocialGenPod using a
prototype that allows users to converse with different Large Language Models,
optionally leveraging Retrieval Augmented Generation to generate answers
grounded in private documents stored in any Solid Pod that the user is allowed
to access, directly or indirectly. SocialGenPod makes use of Solid access
control mechanisms to give users full control of determining who has access to
data stored in their Pods. SocialGenPod keeps all user data (chat history, app
configuration, personal documents, etc) securely in the user's personal Pod;
separate from specific model or application providers. Besides better privacy
controls, this approach also enables portability across different services and
applications. Finally, we discuss challenges, posed by the large compute
requirements of state-of-the-art models, that future research in this area
should address. Our prototype is open-source and available at:
https://github.com/Vidminas/socialgenpod/.</div><div><a href='http://arxiv.org/abs/2403.10408v1'>2403.10408v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.02713v1")'>Android in the Zoo: Chain-of-Action-Thought for GUI Agents</div>
<div id='2403.02713v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T07:09:35Z</div><div>Authors: Jiwen Zhang, Jihao Wu, Yihua Teng, Minghui Liao, Nuo Xu, Xiao Xiao, Zhongyu Wei, Duyu Tang</div><div style='padding-top: 10px; width: 80ex'>Large language model (LLM) leads to a surge of autonomous GUI agents for
smartphone, which completes a task triggered by natural language through
predicting a sequence of actions of API. Even though the task highly relies on
past actions and visual observations, existing studies typical consider little
semantic information carried out by intermediate screenshots and screen
operations. To address this, this work presents Chain-of-Action-Thought (dubbed
CoAT), which takes the description of the previous actions, the current screen,
and more importantly the action thinking of what actions should be performed
and the outcomes led by the chosen action. We demonstrate that, in a zero-shot
setting upon an off-the-shell LLM, CoAT significantly improves the goal
progress compared to standard context modeling. To further facilitate the
research in this line, we construct a benchmark Android-In-The-Zoo (AitZ),
which contains 18,643 screen-action pairs together with chain-of-action-thought
annotations. Experiments show that fine-tuning a 200M model on our AitZ dataset
achieves on par performance with CogAgent-Chat-18B.</div><div><a href='http://arxiv.org/abs/2403.02713v1'>2403.02713v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15350v1")'>Farsight: Fostering Responsible AI Awareness During AI Application
  Prototyping</div>
<div id='2402.15350v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T14:38:05Z</div><div>Authors: Zijie J. Wang, Chinmay Kulkarni, Lauren Wilcox, Michael Terry, Michael Madaio</div><div style='padding-top: 10px; width: 80ex'>Prompt-based interfaces for Large Language Models (LLMs) have made
prototyping and building AI-powered applications easier than ever before.
However, identifying potential harms that may arise from AI applications
remains a challenge, particularly during prompt-based prototyping. To address
this, we present Farsight, a novel in situ interactive tool that helps people
identify potential harms from the AI applications they are prototyping. Based
on a user's prompt, Farsight highlights news articles about relevant AI
incidents and allows users to explore and edit LLM-generated use cases,
stakeholders, and harms. We report design insights from a co-design study with
10 AI prototypers and findings from a user study with 42 AI prototypers. After
using Farsight, AI prototypers in our user study are better able to
independently identify potential harms associated with a prompt and find our
tool more useful and usable than existing resources. Their qualitative feedback
also highlights that Farsight encourages them to focus on end-users and think
beyond immediate harms. We discuss these findings and reflect on their
implications for designing AI prototyping experiences that meaningfully engage
with AI harms. Farsight is publicly accessible at:
https://PAIR-code.github.io/farsight.</div><div><a href='http://arxiv.org/abs/2402.15350v1'>2402.15350v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12611v1")'>Prompt Smells: An Omen for Undesirable Generative AI Outputs</div>
<div id='2401.12611v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T10:10:01Z</div><div>Authors: Krishna Ronanki, Beatriz Cabrero-Daniel, Christian Berger</div><div style='padding-top: 10px; width: 80ex'>Recent Generative Artificial Intelligence (GenAI) trends focus on various
applications, including creating stories, illustrations, poems, articles,
computer code, music compositions, and videos. Extrinsic hallucinations are a
critical limitation of such GenAI, which can lead to significant challenges in
achieving and maintaining the trustworthiness of GenAI. In this paper, we
propose two new concepts that we believe will aid the research community in
addressing limitations associated with the application of GenAI models. First,
we propose a definition for the "desirability" of GenAI outputs and three
factors which are observed to influence it. Second, drawing inspiration from
Martin Fowler's code smells, we propose the concept of "prompt smells" and the
adverse effects they are observed to have on the desirability of GenAI outputs.
We expect our work will contribute to the ongoing conversation about the
desirability of GenAI outputs and help advance the field in a meaningful way.</div><div><a href='http://arxiv.org/abs/2401.12611v1'>2401.12611v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08030v1")'>Why and When LLM-Based Assistants Can Go Wrong: Investigating the
  Effectiveness of Prompt-Based Interactions for Software Help-Seeking</div>
<div id='2402.08030v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T19:49:58Z</div><div>Authors: Anjali Khurana, Hari Subramonyam, Parmit K Chilana</div><div style='padding-top: 10px; width: 80ex'>Large Language Model (LLM) assistants, such as ChatGPT, have emerged as
potential alternatives to search methods for helping users navigate complex,
feature-rich software. LLMs use vast training data from domain-specific texts,
software manuals, and code repositories to mimic human-like interactions,
offering tailored assistance, including step-by-step instructions. In this
work, we investigated LLM-generated software guidance through a within-subject
experiment with 16 participants and follow-up interviews. We compared a
baseline LLM assistant with an LLM optimized for particular software contexts,
SoftAIBot, which also offered guidelines for constructing appropriate prompts.
We assessed task completion, perceived accuracy, relevance, and trust.
Surprisingly, although SoftAIBot outperformed the baseline LLM, our results
revealed no significant difference in LLM usage and user perceptions with or
without prompt guidelines and the integration of domain context. Most users
struggled to understand how the prompt's text related to the LLM's responses
and often followed the LLM's suggestions verbatim, even if they were incorrect.
This resulted in difficulties when using the LLM's advice for software tasks,
leading to low task completion rates. Our detailed analysis also revealed that
users remained unaware of inaccuracies in the LLM's responses, indicating a gap
between their lack of software expertise and their ability to evaluate the
LLM's assistance. With the growing push for designing domain-specific LLM
assistants, we emphasize the importance of incorporating explainable,
context-aware cues into LLMs to help users understand prompt-based
interactions, identify biases, and maximize the utility of LLM assistants.</div><div><a href='http://arxiv.org/abs/2402.08030v1'>2402.08030v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07158v1")'>Effort and Size Estimation in Software Projects with Large Language
  Model-based Intelligent Interfaces</div>
<div id='2402.07158v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T11:03:08Z</div><div>Authors: Claudionor N. Coelho Jr, Hanchen Xiong, Tushar Karayil, Sree Koratala, Rex Shang, Jacob Bollinger, Mohamed Shabar, Syam Nair</div><div style='padding-top: 10px; width: 80ex'>The advancement of Large Language Models (LLM) has also resulted in an
equivalent proliferation in its applications. Software design, being one, has
gained tremendous benefits in using LLMs as an interface component that extends
fixed user stories. However, inclusion of LLM-based AI agents in software
design often poses unexpected challenges, especially in the estimation of
development efforts. Through the example of UI-based user stories, we provide a
comparison against traditional methods and propose a new way to enhance
specifications of natural language-based questions that allows for the
estimation of development effort by taking into account data sources,
interfaces and algorithms.</div><div><a href='http://arxiv.org/abs/2402.07158v1'>2402.07158v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.03099v1")'>Intent-based Prompt Calibration: Enhancing prompt optimization with
  synthetic boundary cases</div>
<div id='2402.03099v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T15:28:43Z</div><div>Authors: Elad Levi, Eli Brosh, Matan Friedmann</div><div style='padding-top: 10px; width: 80ex'>Prompt engineering is a challenging and important task due to the high
sensitivity of Large Language Models (LLMs) to the given prompt and the
inherent ambiguity of a textual task instruction. Automatic prompt engineering
is essential to achieve optimized performance from LLMs. Recent studies have
demonstrated the capabilities of LLMs to automatically conduct prompt
engineering by employing a meta-prompt that incorporates the outcomes of the
last trials and proposes an improved prompt. However, this requires a
high-quality benchmark to compare different prompts, which is difficult and
expensive to acquire in many real-world use cases. In this work, we introduce a
new method for automatic prompt engineering, using a calibration process that
iteratively refines the prompt to the user intent. During the optimization
process, the system jointly generates synthetic data of boundary use cases and
optimizes the prompt according to the generated dataset. We demonstrate the
effectiveness of our method with respect to strong proprietary models on
real-world tasks such as moderation and generation. Our method outperforms
state-of-the-art methods with a limited number of annotated samples.
Furthermore, we validate the advantages of each one of the system's key
components. Our system is built in a modular way, facilitating easy adaptation
to other tasks. The code is available
$\href{https://github.com/Eladlev/AutoPrompt}{here}$.</div><div><a href='http://arxiv.org/abs/2402.03099v1'>2402.03099v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02408v1")'>GLaPE: Gold Label-agnostic Prompt Evaluation and Optimization for Large
  Language Model</div>
<div id='2402.02408v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T08:57:54Z</div><div>Authors: Xuanchang Zhang, Zhuosheng Zhang, Hai Zhao</div><div style='padding-top: 10px; width: 80ex'>Despite the rapid progress of large language models (LLMs), their task
performance remains sensitive to prompt design. Recent studies have explored
leveraging the LLM itself as an optimizer to identify optimal prompts that
maximize task accuracy. However, when evaluating prompts, such approaches
heavily rely on elusive manually annotated gold labels to calculate task
accuracy for each candidate prompt, which hinders the widespread implementation
and generality. To overcome the limitation, this work proposes a gold
label-agnostic prompt evaluation (GLaPE) to alleviate dependence on gold
labels. Motivated by the observed correlation between self-consistency and the
accuracy of the answer, we adopt self-consistency as the initial evaluation
score. Subsequently, we refine the scores of prompts producing identical
answers to be mutually consistent. Experimental results show that GLaPE
provides reliable evaluations uniform with accuracy, even in the absence of
gold labels. Moreover, on six popular reasoning tasks, our GLaPE-based prompt
optimization yields effective prompts comparable to accuracy-based ones. The
code is publicly available at https://github.com/thunderous77/GLaPE.</div><div><a href='http://arxiv.org/abs/2402.02408v1'>2402.02408v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14833v1")'>CliqueParcel: An Approach For Batching LLM Prompts That Jointly
  Optimizes Efficiency And Faithfulness</div>
<div id='2402.14833v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-17T22:37:17Z</div><div>Authors: Jiayi Liu, Tinghan Yang, Jennifer Neville</div><div style='padding-top: 10px; width: 80ex'>Large language models (LLMs) have become pivotal in recent research. However,
during the inference process, LLMs still require substantial resources. In this
paper, we propose CliqueParcel, a method designed to improve the efficiency of
LLMs via prompt batching. Existing strategies to optimize inference efficiency
often compromise on output quality, leading to a discounted output problem.
This issue might result in reduced accuracy or outputs that are less detailed.
CliqueParcel is our answer to this challenge. While ensuring accuracy and
minimizing deviations from the original outputs (i.e., faithfulness), our
method significantly improves efficiency during inference.
  To lay the groundwork, we first redefine efficiency measurements by excluding
the reduction in running time due to shorter lengths. Then, we provide a
comprehensive trade-off between efficiency and faithfulness to clarify the
nature of the 'discounted output' problem. Within the CliqueParcel framework,
we suggest multiple batching sub-methods and discuss the specific scenarios in
which they can be applied. During evaluation, CliqueParcel is tested on eight
widely recognized datasets, which can be classified into three types: reading
comprehension, open-source question-answering, and reasoning. Our experiments
explore the performance of CliqueParcel, including efficiency, faithfulness,
and the trade-off between them. This work provides novel insights into
inference efficiency and demonstrates promising performance.</div><div><a href='http://arxiv.org/abs/2402.14833v1'>2402.14833v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08273v2")'>Large Language Models are Null-Shot Learners</div>
<div id='2401.08273v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T10:53:11Z</div><div>Authors: Pittawat Taveekitworachai, Febri Abdullah, Ruck Thawonmas</div><div style='padding-top: 10px; width: 80ex'>This paper presents null-shot prompting. Null-shot prompting exploits
hallucination in large language models (LLMs) by instructing LLMs to utilize
information from the "Examples" section that never exists within the provided
context to perform a task. While reducing hallucination is crucial and
non-negligible for daily and critical uses of LLMs, we propose that in the
current landscape in which these LLMs still hallucinate, it is possible, in
fact, to exploit hallucination to increase performance in performing tasks
compared to standard zero-shot prompting. Experiments with eight LLMs show
improvements in performance across the majority of eight datasets, including
reading comprehension, arithmetic reasoning, and closed-book question
answering. The observed inconsistency in increased relative performance across
the LLMs also potentially indicates a different degree of inherent
hallucination in each model. These differences show that it is possible to
utilize null-shot prompting as a way to detect degrees of hallucination in LLMs
using existing benchmarking datasets. We also perform ablation studies,
including experimenting with a modified version of null-shot prompting that
incorporates ideas from zero-shot chain-of-thought prompting, which shows
different trends of results.</div><div><a href='http://arxiv.org/abs/2401.08273v2'>2401.08273v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14578v1")'>RAmBLA: A Framework for Evaluating the Reliability of LLMs as Assistants
  in the Biomedical Domain</div>
<div id='2403.14578v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T17:30:59Z</div><div>Authors: William James Bolton, Rafael Poyiadzi, Edward R. Morrell, Gabriela van Bergen Gonzalez Bueno, Lea Goetz</div><div style='padding-top: 10px; width: 80ex'>Large Language Models (LLMs) increasingly support applications in a wide
range of domains, some with potential high societal impact such as biomedicine,
yet their reliability in realistic use cases is under-researched. In this work
we introduce the Reliability AssesMent for Biomedical LLM Assistants (RAmBLA)
framework and evaluate whether four state-of-the-art foundation LLMs can serve
as reliable assistants in the biomedical domain. We identify prompt robustness,
high recall, and a lack of hallucinations as necessary criteria for this use
case. We design shortform tasks and tasks requiring LLM freeform responses
mimicking real-world user interactions. We evaluate LLM performance using
semantic similarity with a ground truth response, through an evaluator LLM.</div><div><a href='http://arxiv.org/abs/2403.14578v1'>2403.14578v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14493v1")'>K-QA: A Real-World Medical Q&amp;A Benchmark</div>
<div id='2401.14493v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T20:11:04Z</div><div>Authors: Itay Manes, Naama Ronn, David Cohen, Ran Ilan Ber, Zehavi Horowitz-Kugler, Gabriel Stanovsky</div><div style='padding-top: 10px; width: 80ex'>Ensuring the accuracy of responses provided by large language models (LLMs)
is crucial, particularly in clinical settings where incorrect information may
directly impact patient health. To address this challenge, we construct K-QA, a
dataset containing 1,212 patient questions originating from real-world
conversations held on K Health (an AI-driven clinical platform). We employ a
panel of in-house physicians to answer and manually decompose a subset of K-QA
into self-contained statements. Additionally, we formulate two NLI-based
evaluation metrics approximating recall and precision: (1) comprehensiveness,
measuring the percentage of essential clinical information in the generated
answer and (2) hallucination rate, measuring the number of statements from the
physician-curated response contradicted by the LLM answer. Finally, we use K-QA
along with these metrics to evaluate several state-of-the-art models, as well
as the effect of in-context learning and medically-oriented augmented retrieval
schemes developed by the authors. Our findings indicate that in-context
learning improves the comprehensiveness of the models, and augmented retrieval
is effective in reducing hallucinations. We make K-QA available to to the
community to spur research into medically accurate NLP applications.</div><div><a href='http://arxiv.org/abs/2401.14493v1'>2401.14493v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07023v1")'>Gemini Goes to Med School: Exploring the Capabilities of Multimodal
  Large Language Models on Medical Challenge Problems &amp; Hallucinations</div>
<div id='2402.07023v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-10T19:08:28Z</div><div>Authors: Ankit Pal, Malaikannan Sankarasubbu</div><div style='padding-top: 10px; width: 80ex'>Large language models have the potential to be valuable in the healthcare
industry, but it's crucial to verify their safety and effectiveness through
rigorous evaluation. For this purpose, we comprehensively evaluated both
open-source LLMs and Google's new multimodal LLM called Gemini across Medical
reasoning, hallucination detection, and Medical Visual Question Answering
tasks. While Gemini showed competence, it lagged behind state-of-the-art models
like MedPaLM 2 and GPT-4 in diagnostic accuracy. Additionally, Gemini achieved
an accuracy of 61.45\% on the medical VQA dataset, significantly lower than
GPT-4V's score of 88\%. Our analysis revealed that Gemini is highly susceptible
to hallucinations, overconfidence, and knowledge gaps, which indicate risks if
deployed uncritically. We also performed a detailed analysis by medical subject
and test type, providing actionable feedback for developers and clinicians. To
mitigate risks, we applied prompting strategies that improved performance.
Additionally, we facilitated future research and development by releasing a
Python module for medical LLM evaluation and establishing a dedicated
leaderboard on Hugging Face for medical domain LLMs. Python module can be found
at https://github.com/promptslab/RosettaEval</div><div><a href='http://arxiv.org/abs/2402.07023v1'>2402.07023v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.10024v1")'>Self-Augmented In-Context Learning for Unsupervised Word Translation</div>
<div id='2402.10024v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T15:43:05Z</div><div>Authors: Yaoyiran Li, Anna Korhonen, Ivan Vulić</div><div style='padding-top: 10px; width: 80ex'>Recent work has shown that, while large language models (LLMs) demonstrate
strong word translation or bilingual lexicon induction (BLI) capabilities in
few-shot setups, they still cannot match the performance of 'traditional'
mapping-based approaches in the unsupervised scenario where no seed translation
pairs are available, especially for lower-resource languages. To address this
challenge with LLMs, we propose self-augmented in-context learning (SAIL) for
unsupervised BLI: starting from a zero-shot prompt, SAIL iteratively induces a
set of high-confidence word translation pairs for in-context learning (ICL)
from an LLM, which it then reapplies to the same LLM in the ICL fashion. Our
method shows substantial gains over zero-shot prompting of LLMs on two
established BLI benchmarks spanning a wide range of language pairs, also
outperforming mapping-based baselines across the board. In addition to
achieving state-of-the-art unsupervised BLI performance, we also conduct
comprehensive analyses on SAIL and discuss its limitations.</div><div><a href='http://arxiv.org/abs/2402.10024v1'>2402.10024v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01416v1")'>Sequence Shortening for Context-Aware Machine Translation</div>
<div id='2402.01416v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T13:55:37Z</div><div>Authors: Paweł Mąka, Yusuf Can Semerci, Jan Scholtes, Gerasimos Spanakis</div><div style='padding-top: 10px; width: 80ex'>Context-aware Machine Translation aims to improve translations of sentences
by incorporating surrounding sentences as context. Towards this task, two main
architectures have been applied, namely single-encoder (based on concatenation)
and multi-encoder models. In this study, we show that a special case of
multi-encoder architecture, where the latent representation of the source
sentence is cached and reused as the context in the next step, achieves higher
accuracy on the contrastive datasets (where the models have to rank the correct
translation among the provided sentences) and comparable BLEU and COMET scores
as the single- and multi-encoder approaches. Furthermore, we investigate the
application of Sequence Shortening to the cached representations. We test three
pooling-based shortening techniques and introduce two novel methods - Latent
Grouping and Latent Selecting, where the network learns to group tokens or
selects the tokens to be cached as context. Our experiments show that the two
methods achieve competitive BLEU and COMET scores and accuracies on the
contrastive datasets to the other tested methods while potentially allowing for
higher interpretability and reducing the growth of memory requirements with
increased context size.</div><div><a href='http://arxiv.org/abs/2402.01416v1'>2402.01416v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14849v1")'>Asynchronous and Segmented Bidirectional Encoding for NMT</div>
<div id='2402.14849v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T19:48:02Z</div><div>Authors: Jingpu Yang, Zehua Han, Mengyu Xiang, Helin Wang, Yuxiao Huang, Miao Fang</div><div style='padding-top: 10px; width: 80ex'>With the rapid advancement of Neural Machine Translation (NMT), enhancing
translation efficiency and quality has become a focal point of research.
Despite the commendable performance of general models such as the Transformer
in various aspects, they still fall short in processing long sentences and
fully leveraging bidirectional contextual information. This paper introduces an
improved model based on the Transformer, implementing an asynchronous and
segmented bidirectional decoding strategy aimed at elevating translation
efficiency and accuracy. Compared to traditional unidirectional translations
from left-to-right or right-to-left, our method demonstrates heightened
efficiency and improved translation quality, particularly in handling long
sentences. Experimental results on the IWSLT2017 dataset confirm the
effectiveness of our approach in accelerating translation and increasing
accuracy, especially surpassing traditional unidirectional strategies in long
sentence translation. Furthermore, this study analyzes the impact of sentence
length on decoding outcomes and explores the model's performance in various
scenarios. The findings of this research not only provide an effective encoding
strategy for the NMT field but also pave new avenues and directions for future
studies.</div><div><a href='http://arxiv.org/abs/2402.14849v1'>2402.14849v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15061v1")'>Fine-tuning Large Language Models for Domain-specific Machine
  Translation</div>
<div id='2402.15061v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T02:24:15Z</div><div>Authors: Jiawei Zheng, Hanghai Hong, Xiaoli Wang, Jingsong Su, Yonggui Liang, Shikai Wu</div><div style='padding-top: 10px; width: 80ex'>Large language models (LLMs) have made significant progress in machine
translation (MT). However, their potential in domain-specific MT remains
under-explored. Current LLM-based MT systems still face several challenges.
First, for LLMs with in-context learning, their effectiveness is highly
sensitive to input translation examples, and processing them can increase
inference costs. They often require extra post-processing due to
over-generation. Second, LLMs with fine-tuning on domain-specific data often
require high training costs for domain adaptation, and may weaken the zero-shot
MT capabilities of LLMs due to over-specialization. The aforementioned methods
can struggle to translate rare words in domain transfer scenarios. To address
these challenges, this paper proposes a prompt-oriented fine-tuning method,
denoted as LlamaIT, to effectively and efficiently fine-tune a general-purpose
LLM for domain-specific MT tasks. First, we construct a task-specific
mix-domain dataset, which is then used to fine-tune the LLM with LoRA. This can
eliminate the need for input translation examples, post-processing, or
over-specialization. By zero-shot prompting with instructions, we adapt the MT
tasks to the target domain at inference time. To further elicit the MT
capability for rare words, we construct new prompts by incorporating
domain-specific bilingual vocabulary. We also conduct extensive experiments on
both publicly available and self-constructed datasets. The results show that
our LlamaIT can significantly enhance the domain-specific MT capabilities of
the LLM, meanwhile preserving its zero-shot MT capabilities.</div><div><a href='http://arxiv.org/abs/2402.15061v1'>2402.15061v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03861v2")'>Designing Informative Metrics for Few-Shot Example Selection</div>
<div id='2403.03861v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T17:11:38Z</div><div>Authors: Rishabh Adiga, Lakshminarayanan Subramanian, Varun Chandrasekaran</div><div style='padding-top: 10px; width: 80ex'>Pretrained language models (PLMs) have shown remarkable few-shot learning
capabilities when provided with properly formatted examples. However, selecting
the "best" examples remains an open challenge. We propose a complexity-based
prompt selection approach for sequence tagging tasks. This approach avoids the
training of a dedicated model for selection of examples, and instead uses
certain metrics to align the syntactico-semantic complexity of test sentences
and examples. We use both sentence- and word-level metrics to match the
complexity of examples to the (test) sentence being considered. Our results
demonstrate that our approach extracts greater performance from PLMs: it
achieves state-of-the-art performance on few-shot NER, achieving a 5% absolute
improvement in F1 score on the CoNLL2003 dataset for GPT-4. We also see large
gains of upto 28.85 points (F1/Acc.) in smaller models like GPT-j-6B.</div><div><a href='http://arxiv.org/abs/2403.03861v2'>2403.03861v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14522v1")'>Towards Unified Task Embeddings Across Multiple Models: Bridging the Gap
  for Prompt-Based Large Language Models and Beyond</div>
<div id='2402.14522v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T13:13:31Z</div><div>Authors: Xinyu Wang, Hainiu Xu, Lin Gui, Yulan He</div><div style='padding-top: 10px; width: 80ex'>Task embedding, a meta-learning technique that captures task-specific
information, has become prevalent, especially in areas such as multi-task
learning, model editing, and interpretability. However, it faces challenges
with the emergence of prompt-guided Large Language Models (LLMs) operating in a
gradientfree manner. Existing task embedding methods rely on fine-tuned,
task-specific language models, which hinders the adaptability of task
embeddings across diverse models, especially prompt-based LLMs. To unleash the
power of task embedding in the era of LLMs, we propose a framework for unified
task embeddings (FUTE), harmonizing task embeddings from various models,
including smaller language models and LLMs with varied prompts, within a single
vector space. Such uniformity enables the comparison and analysis of
similarities amongst different models, extending the scope and utility of
existing task embedding methods in addressing multi-model scenarios, whilst
maintaining their performance to be comparable to architecture-specific
methods.</div><div><a href='http://arxiv.org/abs/2402.14522v1'>2402.14522v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13598v1")'>User-LLM: Efficient LLM Contextualization with User Embeddings</div>
<div id='2402.13598v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T08:03:27Z</div><div>Authors: Lin Ning, Luyang Liu, Jiaxing Wu, Neo Wu, Devora Berlowitz, Sushant Prakash, Bradley Green, Shawn O'Banion, Jun Xie</div><div style='padding-top: 10px; width: 80ex'>Large language models (LLMs) have revolutionized natural language processing.
However, effectively incorporating complex and potentially noisy user
interaction data remains a challenge. To address this, we propose User-LLM, a
novel framework that leverages user embeddings to contextualize LLMs. These
embeddings, distilled from diverse user interactions using self-supervised
pretraining, capture latent user preferences and their evolution over time. We
integrate these user embeddings with LLMs through cross-attention and
soft-prompting, enabling LLMs to dynamically adapt to user context. Our
comprehensive experiments on MovieLens, Amazon Review, and Google Local Review
datasets demonstrate significant performance gains across various tasks.
Notably, our approach outperforms text-prompt-based contextualization on long
sequence tasks and tasks that require deep user understanding while being
computationally efficient. We further incorporate Perceiver layers to
streamline the integration between user encoders and LLMs, reducing
computational demands.</div><div><a href='http://arxiv.org/abs/2402.13598v1'>2402.13598v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.04858v1")'>User Embedding Model for Personalized Language Prompting</div>
<div id='2401.04858v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T00:35:52Z</div><div>Authors: Sumanth Doddapaneni, Krishna Sayana, Ambarish Jash, Sukhdeep Sodhi, Dima Kuzmin</div><div style='padding-top: 10px; width: 80ex'>Modeling long histories plays a pivotal role in enhancing recommendation
systems, allowing to capture user's evolving preferences, resulting in more
precise and personalized recommendations. In this study we tackle the
challenges of modeling long user histories for preference understanding in
natural language. Specifically, we introduce a new User Embedding Module (UEM)
that efficiently processes user history in free-form text by compressing and
representing them as embeddings, to use them as soft prompts to a LM. Our
experiments demonstrate the superior capability of this approach in handling
significantly longer histories compared to conventional text based prompting
methods, yielding substantial improvements in predictive performance. The main
contribution of this research is to demonstrate the ability to bias language
models with user signals represented as embeddings.</div><div><a href='http://arxiv.org/abs/2401.04858v1'>2401.04858v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01173v1")'>Efficient Prompt Caching via Embedding Similarity</div>
<div id='2402.01173v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T06:34:11Z</div><div>Authors: Hanlin Zhu, Banghua Zhu, Jiantao Jiao</div><div style='padding-top: 10px; width: 80ex'>Large language models (LLMs) have achieved huge success in numerous natural
language process (NLP) tasks. However, it faces the challenge of significant
resource consumption during inference. In this paper, we aim to improve the
inference efficiency of LLMs by prompt caching, i.e., if the current prompt can
be answered by the same response of a previous prompt, one can directly utilize
that previous response without calling the LLM. Specifically, we focus on the
prediction accuracy of prompt caching for single-round question-answering tasks
via embedding similarity. The existing embeddings of prompts mostly focus on
whether two prompts are semantically similar, which is not necessarily
equivalent to whether the same response can answer them. Therefore, we propose
a distillation-based method to fine-tune the existing embeddings for better
caching prediction. Theoretically, we provide finite-sample guarantees for the
convergence of our method under different types of loss functions. Empirically,
we carefully construct a hard dataset based on Kwiatkowski et al. (2019) where
the existing embedding model (Wang et al., 2022) only achieves an AUC of 0.51.
We then fine-tune the above embedding model, which significantly improves the
AUC of caching prediction from 0.51 to 0.81. We also conduct simulations
demonstrating that our trained models achieve better caching efficiency than
the previous embedding model.</div><div><a href='http://arxiv.org/abs/2402.01173v1'>2402.01173v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.03131v1")'>Constrained Decoding for Cross-lingual Label Projection</div>
<div id='2402.03131v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T15:57:32Z</div><div>Authors: Duong Minh Le, Yang Chen, Alan Ritter, Wei Xu</div><div style='padding-top: 10px; width: 80ex'>Zero-shot cross-lingual transfer utilizing multilingual LLMs has become a
popular learning paradigm for low-resource languages with no labeled training
data. However, for NLP tasks that involve fine-grained predictions on words and
phrases, the performance of zero-shot cross-lingual transfer learning lags far
behind supervised fine-tuning methods. Therefore, it is common to exploit
translation and label projection to further improve the performance by (1)
translating training data that is available in a high-resource language (e.g.,
English) together with the gold labels into low-resource languages, and/or (2)
translating test data in low-resource languages to a high-source language to
run inference on, then projecting the predicted span-level labels back onto the
original test data. However, state-of-the-art marker-based label projection
methods suffer from translation quality degradation due to the extra label
markers injected in the input to the translation model. In this work, we
explore a new direction that leverages constrained decoding for label
projection to overcome the aforementioned issues. Our new method not only can
preserve the quality of translated texts but also has the versatility of being
applicable to both translating training and translating test data strategies.
This versatility is crucial as our experiments reveal that translating test
data can lead to a considerable boost in performance compared to translating
only training data. We evaluate on two cross-lingual transfer tasks, namely
Named Entity Recognition and Event Argument Extraction, spanning 20 languages.
The results demonstrate that our approach outperforms the state-of-the-art
marker-based method by a large margin and also shows better performance than
other label projection methods that rely on external word alignment.</div><div><a href='http://arxiv.org/abs/2402.03131v1'>2402.03131v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13835v1")'>SMART: Automatically Scaling Down Language Models with Accuracy
  Guarantees for Reduced Processing Fees</div>
<div id='2403.13835v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T17:45:47Z</div><div>Authors: Saehan Jo, Immanuel Trummer</div><div style='padding-top: 10px; width: 80ex'>The advancement of Large Language Models (LLMs) has significantly boosted
performance in natural language processing (NLP) tasks. However, the deployment
of high-performance LLMs incurs substantial costs, primarily due to the
increased number of parameters aimed at enhancing model performance. This has
made the use of state-of-the-art LLMs more expensive for end-users. AI service
providers, such as OpenAI and Anthropic, often offer multiple versions of LLMs
with varying prices and performance. However, end-users still face challenges
in choosing the appropriate LLM for their tasks that balance result quality
with cost.
  We introduce SMART, Scaling Models Adaptively for Reduced Token Fees, a novel
LLM framework designed to minimize the inference costs of NLP tasks while
ensuring sufficient result quality. It enables users to specify an accuracy
constraint in terms of the equivalence of outputs to those of the most powerful
LLM. SMART then generates results that deviate from the outputs of this LLM
only with a probability below a user-defined threshold. SMART employs a
profiling phase that evaluates the performance of multiple LLMs to identify
those that meet the user-defined accuracy level. SMART optimizes the tradeoff
between profiling overheads and the anticipated cost savings resulting from
profiling. Moreover, our approach significantly reduces inference costs by
strategically leveraging a mix of LLMs. Our experiments on three real-world
datasets show that, based on OpenAI models, SMART achieves significant cost
savings, up to 25.6x in comparison to GPT-4.</div><div><a href='http://arxiv.org/abs/2403.13835v1'>2403.13835v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14021v1")'>Accelerating Retrieval-Augmented Language Model Serving with Speculation</div>
<div id='2401.14021v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T09:06:44Z</div><div>Authors: Zhihao Zhang, Alan Zhu, Lijie Yang, Yihua Xu, Lanting Li, Phitchaya Mangpo Phothilimthana, Zhihao Jia</div><div style='padding-top: 10px; width: 80ex'>Retrieval-augmented language models (RaLM) have demonstrated the potential to
solve knowledge-intensive natural language processing (NLP) tasks by combining
a non-parametric knowledge base with a parametric language model. Instead of
fine-tuning a fully parametric model, RaLM excels at its low-cost adaptation to
the latest data and better source attribution mechanisms. Among various RaLM
approaches, iterative RaLM delivers a better generation quality due to a more
frequent interaction between the retriever and the language model. Despite the
benefits, iterative RaLM usually encounters high overheads due to the frequent
retrieval step. To this end, we propose RaLMSpec, a speculation-inspired
framework that provides generic speed-up over iterative RaLM while preserving
the same model outputs through speculative retrieval and batched verification.
By further incorporating prefetching, optimal speculation stride scheduler, and
asynchronous verification, RaLMSpec can automatically exploit the acceleration
potential to the fullest. For naive iterative RaLM serving, extensive
evaluations over three language models on four downstream QA datasets
demonstrate that RaLMSpec can achieve a speed-up ratio of 1.75-2.39x,
1.04-1.39x, and 1.31-1.77x when the retriever is an exact dense retriever,
approximate dense retriever, and sparse retriever respectively compared with
the baseline. For KNN-LM serving, RaLMSpec can achieve a speed-up ratio up to
7.59x and 2.45x when the retriever is an exact dense retriever and approximate
dense retriever, respectively, compared with the baseline.</div><div><a href='http://arxiv.org/abs/2401.14021v1'>2401.14021v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18700v1")'>Learning to Compress Prompt in Natural Language Formats</div>
<div id='2402.18700v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T20:41:21Z</div><div>Authors: Yu-Neng Chuang, Tianwei Xing, Chia-Yuan Chang, Zirui Liu, Xun Chen, Xia Hu</div><div style='padding-top: 10px; width: 80ex'>Large language models (LLMs) are great at processing multiple natural
language processing tasks, but their abilities are constrained by inferior
performance with long context, slow inference speed, and the high cost of
computing the results. Deploying LLMs with precise and informative context
helps users process large-scale datasets more effectively and cost-efficiently.
Existing works rely on compressing long prompt contexts into soft prompts.
However, soft prompt compression encounters limitations in transferability
across different LLMs, especially API-based LLMs. To this end, this work aims
to compress lengthy prompts in the form of natural language with LLM
transferability. This poses two challenges: (i) Natural Language (NL) prompts
are incompatible with back-propagation, and (ii) NL prompts lack flexibility in
imposing length constraints. In this work, we propose a Natural Language Prompt
Encapsulation (Nano-Capsulator) framework compressing original prompts into NL
formatted Capsule Prompt while maintaining the prompt utility and
transferability. Specifically, to tackle the first challenge, the
Nano-Capsulator is optimized by a reward function that interacts with the
proposed semantics preserving loss. To address the second question, the
Nano-Capsulator is optimized by a reward function featuring length constraints.
Experimental results demonstrate that the Capsule Prompt can reduce 81.4% of
the original length, decrease inference latency up to 4.5x, and save 80.1% of
budget overheads while providing transferability across diverse LLMs and
different datasets.</div><div><a href='http://arxiv.org/abs/2402.18700v1'>2402.18700v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01742v1")'>Towards Optimizing the Costs of LLM Usage</div>
<div id='2402.01742v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T16:36:31Z</div><div>Authors: Shivanshu Shekhar, Tanishq Dubey, Koyel Mukherjee, Apoorv Saxena, Atharv Tyagi, Nishanth Kotla</div><div style='padding-top: 10px; width: 80ex'>Generative AI and LLMs in particular are heavily used nowadays for various
document processing tasks such as question answering and summarization.
However, different LLMs come with different capabilities for different tasks as
well as with different costs, tokenization, and latency. In fact, enterprises
are already incurring huge costs of operating or using LLMs for their
respective use cases.
  In this work, we propose optimizing the usage costs of LLMs by estimating
their output quality (without actually invoking the LLMs), and then solving an
optimization routine for the LLM selection to either keep costs under a budget,
or minimize the costs, in a quality and latency aware manner. We propose a
model to predict the output quality of LLMs on document processing tasks like
summarization, followed by an LP rounding algorithm to optimize the selection
of LLMs. We study optimization problems trading off the quality and costs, both
theoretically and empirically. We further propose a sentence simplification
model for reducing the number of tokens in a controlled manner. Additionally,
we propose several deterministic heuristics for reducing tokens in a quality
aware manner, and study the related optimization problem of applying the
heuristics optimizing the quality and cost trade-off. We perform extensive
empirical validation of our methods on not only enterprise datasets but also on
open-source datasets, annotated by us, and show that we perform much better
compared to closest baselines. Our methods reduce costs by 40%- 90% while
improving quality by 4%-7%. We will release the annotated open source datasets
to the community for further research and exploration.</div><div><a href='http://arxiv.org/abs/2402.01742v1'>2402.01742v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07283v1")'>A Framework for Cost-Effective and Self-Adaptive LLM Shaking and
  Recovery Mechanism</div>
<div id='2403.07283v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T03:30:04Z</div><div>Authors: Zhiyu Chen, Yu Li, Suochao Zhang, Jingbo Zhou, Jiwen Zhou, Chenfu Bao, Dianhai Yu</div><div style='padding-top: 10px; width: 80ex'>As Large Language Models (LLMs) gain great success in real-world
applications, an increasing number of users are seeking to develop and deploy
their customized LLMs through cloud services. Nonetheless, in some specific
domains, there are still concerns regarding cost and trade-offs between privacy
issues and accuracy. In this study, we introduce a cost-effective and
self-adaptive LLM shaking tuning and recovery mechanism, named CypherTalk. With
carefully designed horizontal and vertical shaking operators, we can achieve
comparable accuracy results with SOTA privacy-preserving LLM schemes using
Cryptography-based or Differential Privacy-based methods. Experiments also show
that with the CypherTalk framework, users can achieve reliable accuracy when
using optimized shaking operator settings. To our best knowledge, this is the
first work that considers cost, and trade-off between model utility and privacy
in LLM scenarios.</div><div><a href='http://arxiv.org/abs/2403.07283v1'>2403.07283v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.05004v1")'>Can't Remember Details in Long Documents? You Need Some R&amp;R</div>
<div id='2403.05004v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T03:03:20Z</div><div>Authors: Devanshu Agrawal, Shang Gao, Martin Gajek</div><div style='padding-top: 10px; width: 80ex'>Long-context large language models (LLMs) hold promise for tasks such as
question-answering (QA) over long documents, but they tend to miss important
information in the middle of context documents (arXiv:2307.03172v3). Here, we
introduce $\textit{R&amp;R}$ -- a combination of two novel prompt-based methods
called $\textit{reprompting}$ and $\textit{in-context retrieval}$ (ICR) -- to
alleviate this effect in document-based QA. In reprompting, we repeat the
prompt instructions periodically throughout the context document to remind the
LLM of its original task. In ICR, rather than instructing the LLM to answer the
question directly, we instruct it to retrieve the top $k$ passage numbers most
relevant to the given question, which are then used as an abbreviated context
in a second QA prompt. We test R&amp;R with GPT-4 Turbo and Claude-2.1 on documents
up to 80k tokens in length and observe a 16-point boost in QA accuracy on
average. Our further analysis suggests that R&amp;R improves performance on long
document-based QA because it reduces the distance between relevant context and
the instructions. Finally, we show that compared to short-context chunkwise
methods, R&amp;R enables the use of larger chunks that cost fewer LLM calls and
output tokens, while minimizing the drop in accuracy.</div><div><a href='http://arxiv.org/abs/2403.05004v1'>2403.05004v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12968v1")'>LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic
  Prompt Compression</div>
<div id='2403.12968v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T17:59:56Z</div><div>Authors: Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo, Jue Zhang, Qingwei Lin, Victor Rühle, Yuqing Yang, Chin-Yew Lin, H. Vicky Zhao, Lili Qiu, Dongmei Zhang</div><div style='padding-top: 10px; width: 80ex'>This paper focuses on task-agnostic prompt compression for better
generalizability and efficiency. Considering the redundancy in natural
language, existing approaches compress prompts by removing tokens or lexical
units according to their information entropy obtained from a causal language
model such as LLaMa-7B. The challenge is that information entropy may be a
suboptimal compression metric: (i) it only leverages unidirectional context and
may fail to capture all essential information needed for prompt compression;
(ii) it is not aligned with the prompt compression objective.
  To address these issues, we propose a data distillation procedure to derive
knowledge from an LLM to compress prompts without losing crucial information,
and meantime, introduce an extractive text compression dataset. We formulate
prompt compression as a token classification problem to guarantee the
faithfulness of the compressed prompt to the original one, and use a
Transformer encoder as the base architecture to capture all essential
information for prompt compression from the full bidirectional context. Our
approach leads to lower latency by explicitly learning the compression
objective with smaller models such as XLM-RoBERTa-large and mBERT.
  We evaluate our method on both in-domain and out-of-domain datasets,
including MeetingBank, LongBench, ZeroScrolls, GSM8K, and BBH. Despite its
small size, our model shows significant performance gains over strong baselines
and demonstrates robust generalization ability across different LLMs.
Additionally, our model is 3x-6x faster than existing prompt compression
methods, while accelerating the end-to-end latency by 1.6x-2.9x with
compression ratios of 2x-5x.</div><div><a href='http://arxiv.org/abs/2403.12968v1'>2403.12968v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09841v1")'>LAPDoc: Layout-Aware Prompting for Documents</div>
<div id='2402.09841v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T10:00:49Z</div><div>Authors: Marcel Lamott, Yves-Noel Weweler, Adrian Ulges, Faisal Shafait, Dirk Krechel, Darko Obradovic</div><div style='padding-top: 10px; width: 80ex'>Recent advances in training large language models (LLMs) using massive
amounts of solely textual data lead to strong generalization across many
domains and tasks, including document-specific tasks. Opposed to that there is
a trend to train multi-modal transformer architectures tailored for document
understanding that are designed specifically to fuse textual inputs with the
corresponding document layout. This involves a separate fine-tuning step for
which additional training data is required. At present, no document
transformers with comparable generalization to LLMs are available That raises
the question which type of model is to be preferred for document understanding
tasks. In this paper we investigate the possibility to use purely text-based
LLMs for document-specific tasks by using layout enrichment. We explore drop-in
modifications and rule-based methods to enrich purely textual LLM prompts with
layout information. In our experiments we investigate the effects on the
commercial ChatGPT model and the open-source LLM Solar. We demonstrate that
using our approach both LLMs show improved performance on various standard
document benchmarks. In addition, we study the impact of noisy OCR and layout
errors, as well as the limitations of LLMs when it comes to utilizing document
layout. Our results indicate that layout enrichment can improve the performance
of purely text-based LLMs for document understanding by up to 15% compared to
just using plain document text. In conclusion, this approach should be
considered for the best model choice between text-based LLM or multi-modal
document transformers.</div><div><a href='http://arxiv.org/abs/2402.09841v1'>2402.09841v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01714v1")'>TrICy: Trigger-guided Data-to-text Generation with Intent aware
  Attention-Copy</div>
<div id='2402.01714v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T20:17:06Z</div><div>Authors: Vibhav Agarwal, Sourav Ghosh, Harichandana BSS, Himanshu Arora, Barath Raj Kandur Raja</div><div style='padding-top: 10px; width: 80ex'>Data-to-text (D2T) generation is a crucial task in many natural language
understanding (NLU) applications and forms the foundation of task-oriented
dialog systems. In the context of conversational AI solutions that can work
directly with local data on the user's device, architectures utilizing large
pre-trained language models (PLMs) are impractical for on-device deployment due
to a high memory footprint. To this end, we propose TrICy, a novel lightweight
framework for an enhanced D2T task that generates text sequences based on the
intent in context and may further be guided by user-provided triggers. We
leverage an attention-copy mechanism to predict out-of-vocabulary (OOV) words
accurately. Performance analyses on E2E NLG dataset (BLEU: 66.43%, ROUGE-L:
70.14%), WebNLG dataset (BLEU: Seen 64.08%, Unseen 52.35%), and our Custom
dataset related to text messaging applications, showcase our architecture's
effectiveness. Moreover, we show that by leveraging an optional trigger input,
data-to-text generation quality increases significantly and achieves the new
SOTA score of 69.29% BLEU for E2E NLG. Furthermore, our analyses show that
TrICy achieves at least 24% and 3% improvement in BLEU and METEOR respectively
over LLMs like GPT-3, ChatGPT, and Llama 2. We also demonstrate that in some
scenarios, performance improvement due to triggers is observed even when they
are absent in training.</div><div><a href='http://arxiv.org/abs/2402.01714v1'>2402.01714v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.15313v2")'>ArabianGPT: Native Arabic GPT-based Large Language Model</div>
<div id='2402.15313v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T13:32:47Z</div><div>Authors: Anis Koubaa, Adel Ammar, Lahouari Ghouti, Omar Najar, Serry Sibaee</div><div style='padding-top: 10px; width: 80ex'>The predominance of English and Latin-based large language models (LLMs) has
led to a notable deficit in native Arabic LLMs. This discrepancy is accentuated
by the prevalent inclusion of English tokens in existing Arabic models,
detracting from their efficacy in processing native Arabic's intricate
morphology and syntax. Consequently, there is a theoretical and practical
imperative for developing LLMs predominantly focused on Arabic linguistic
elements. To address this gap, this paper proposes ArabianGPT, a series of
transformer-based models within the ArabianLLM suite designed explicitly for
Arabic. These models, including ArabianGPT-0.1B and ArabianGPT-0.3B, vary in
size and complexity, aligning with the nuanced linguistic characteristics of
Arabic. The AraNizer tokenizer, integral to these models, addresses the unique
morphological aspects of Arabic script, ensuring more accurate text processing.
Empirical results from fine-tuning the models on tasks like sentiment analysis
and summarization demonstrate significant improvements. For sentiment analysis,
the fine-tuned ArabianGPT-0.1B model achieved a remarkable accuracy of 95%, a
substantial increase from the base model's 56%. Similarly, in summarization
tasks, fine-tuned models showed enhanced F1 scores, indicating improved
precision and recall in generating concise summaries. Comparative analysis of
fine-tuned ArabianGPT models against their base versions across various
benchmarks reveals nuanced differences in performance, with fine-tuning
positively impacting specific tasks like question answering and summarization.
These findings underscore the efficacy of fine-tuning in aligning ArabianGPT
models more closely with specific NLP tasks, highlighting the potential of
tailored transformer architectures in advancing Arabic NLP.</div><div><a href='http://arxiv.org/abs/2402.15313v2'>2402.15313v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07448v1")'>AraSpider: Democratizing Arabic-to-SQL</div>
<div id='2402.07448v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T07:11:13Z</div><div>Authors: Ahmed Heakl, Youssef Mohamed, Ahmed B. Zaky</div><div style='padding-top: 10px; width: 80ex'>This study presents AraSpider, the first Arabic version of the Spider
dataset, aimed at improving natural language processing (NLP) in the
Arabic-speaking community. Four multilingual translation models were tested for
their effectiveness in translating English to Arabic. Additionally, two models
were assessed for their ability to generate SQL queries from Arabic text. The
results showed that using back translation significantly improved the
performance of both ChatGPT 3.5 and SQLCoder models, which are considered top
performers on the Spider dataset. Notably, ChatGPT 3.5 demonstrated
high-quality translation, while SQLCoder excelled in text-to-SQL tasks. The
study underscores the importance of incorporating contextual schema and
employing back translation strategies to enhance model performance in Arabic
NLP tasks. Moreover, the provision of detailed methodologies for
reproducibility and translation of the dataset into other languages highlights
the research's commitment to promoting transparency and collaborative knowledge
sharing in the field. Overall, these contributions advance NLP research,
empower Arabic-speaking researchers, and enrich the global discourse on
language comprehension and database interrogation.</div><div><a href='http://arxiv.org/abs/2402.07448v1'>2402.07448v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.09727v1")'>Investigating the performance of Retrieval-Augmented Generation and
  fine-tuning for the development of AI-driven knowledge-based systems</div>
<div id='2403.09727v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T21:06:31Z</div><div>Authors: Robert Lakatos, Peter Pollner, Andras Hajdu, Tamas Joo</div><div style='padding-top: 10px; width: 80ex'>The development of generative large language models (G-LLM) opened up new
opportunities for the development of new types of knowledge-based systems
similar to ChatGPT, Bing, or Gemini. Fine-tuning (FN) and Retrieval-Augmented
Generation (RAG) are the techniques that can be used to implement domain
adaptation for the development of G-LLM-based knowledge systems. In our study,
using ROUGE, BLEU, METEOR scores, and cosine similarity, we compare and examine
the performance of RAG and FN for the GPT-J-6B, OPT-6.7B, LlaMA, LlaMA-2
language models. Based on measurements shown on different datasets, we
demonstrate that RAG-based constructions are more efficient than models
produced with FN. We point out that connecting RAG and FN is not trivial,
because connecting FN models with RAG can cause a decrease in performance.
Furthermore, we outline a simple RAG-based architecture which, on average,
outperforms the FN models by 16% in terms of the ROGUE score, 15% in the case
of the BLEU score, and 53% based on the cosine similarity. This shows the
significant advantage of RAG over FN in terms of hallucination, which is not
offset by the fact that the average 8% better METEOR score of FN models
indicates greater creativity compared to RAG.</div><div><a href='http://arxiv.org/abs/2403.09727v1'>2403.09727v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07883v1")'>The Chronicles of RAG: The Retriever, the Chunk and the Generator</div>
<div id='2401.07883v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T18:25:18Z</div><div>Authors: Paulo Finardi, Leonardo Avila, Rodrigo Castaldoni, Pedro Gengo, Celio Larcher, Marcos Piau, Pablo Costa, Vinicius Caridá</div><div style='padding-top: 10px; width: 80ex'>Retrieval Augmented Generation (RAG) has become one of the most popular
paradigms for enabling LLMs to access external data, and also as a mechanism
for grounding to mitigate against hallucinations. When implementing RAG you can
face several challenges like effective integration of retrieval models,
efficient representation learning, data diversity, computational efficiency
optimization, evaluation, and quality of text generation. Given all these
challenges, every day a new technique to improve RAG appears, making it
unfeasible to experiment with all combinations for your problem. In this
context, this paper presents good practices to implement, optimize, and
evaluate RAG for the Brazilian Portuguese language, focusing on the
establishment of a simple pipeline for inference and experiments. We explored a
diverse set of methods to answer questions about the first Harry Potter book.
To generate the answers we used the OpenAI's gpt-4, gpt-4-1106-preview,
gpt-3.5-turbo-1106, and Google's Gemini Pro. Focusing on the quality of the
retriever, our approach achieved an improvement of MRR@10 by 35.4% compared to
the baseline. When optimizing the input size in the application, we observed
that it is possible to further enhance it by 2.4%. Finally, we present the
complete architecture of the RAG with our recommendations. As result, we moved
from a baseline of 57.88% to a maximum relative score of 98.61%.</div><div><a href='http://arxiv.org/abs/2401.07883v1'>2401.07883v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.10693v2")'>Exploring Precision and Recall to assess the quality and diversity of
  LLMs</div>
<div id='2402.10693v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T13:53:26Z</div><div>Authors: Florian Le Bronnec, Alexandre Verine, Benjamin Negrevergne, Yann Chevaleyre, Alexandre Allauzen</div><div style='padding-top: 10px; width: 80ex'>This paper introduces a novel evaluation framework for Large Language Models
(LLMs) such as Llama-2 and Mistral, focusing on the adaptation of Precision and
Recall metrics from image generation to text generation. This approach allows
for a nuanced assessment of the quality and diversity of generated text without
the need for aligned corpora. By conducting a comprehensive evaluation of
state-of-the-art language models, the study reveals significant insights into
their performance on open-ended generation tasks, which are not adequately
captured by traditional benchmarks. The findings highlight a trade-off between
the quality and diversity of generated samples, particularly when models are
fine-tuned with human feedback. This work extends the toolkit for
distribution-based NLP evaluation, offering insights into the practical
capabilities and challenges faced by current LLMs in generating diverse and
high-quality text.</div><div><a href='http://arxiv.org/abs/2402.10693v2'>2402.10693v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13726v1")'>Supporting Sensemaking of Large Language Model Outputs at Scale</div>
<div id='2401.13726v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T18:45:34Z</div><div>Authors: Katy Ilonka Gero, Chelse Swoopes, Ziwei Gu, Jonathan K. Kummerfeld, Elena L. Glassman</div><div style='padding-top: 10px; width: 80ex'>Large language models (LLMs) are capable of generating multiple responses to
a single prompt, yet little effort has been expended to help end-users or
system designers make use of this capability. In this paper, we explore how to
present many LLM responses at once. We design five features, which include both
pre-existing and novel methods for computing similarities and differences
across textual documents, as well as how to render their outputs. We report on
a controlled user study (n=24) and eight case studies evaluating these features
and how they support users in different tasks. We find that the features
support a wide variety of sensemaking tasks and even make tasks previously
considered to be too difficult by our participants now tractable. Finally, we
present design guidelines to inform future explorations of new LLM interfaces.</div><div><a href='http://arxiv.org/abs/2401.13726v1'>2401.13726v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07891v1")'>Label-Efficient Model Selection for Text Generation</div>
<div id='2402.07891v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T18:54:02Z</div><div>Authors: Shir Ashury-Tahan, Benjamin Sznajder, Leshem Choshen, Liat Ein-Dor, Eyal Shnarch, Ariel Gera</div><div style='padding-top: 10px; width: 80ex'>Model selection for a given target task can be costly, as it may entail
extensive annotation of the quality of outputs of different models. We
introduce DiffUse, an efficient method to make an informed decision between
candidate text generation models. DiffUse reduces the required amount of
preference annotations, thus saving valuable time and resources in performing
evaluation. DiffUse intelligently selects instances by clustering embeddings
that represent the semantic differences between model outputs. Thus, it is able
to identify a subset of examples that are more informative for preference
decisions. Our method is model-agnostic, and can be applied to any text
generation model. Moreover, we propose a practical iterative approach for
dynamically determining how many instances to annotate. In a series of
experiments over hundreds of model pairs, we demonstrate that DiffUse can
dramatically reduce the required number of annotations -- by up to 75% -- while
maintaining high evaluation reliability.</div><div><a href='http://arxiv.org/abs/2402.07891v1'>2402.07891v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08496v3")'>A Systematic Review of Data-to-Text NLG</div>
<div id='2402.08496v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T14:51:45Z</div><div>Authors: Chinonso Cynthia Osuji, Thiago Castro Ferreira, Brian Davis</div><div style='padding-top: 10px; width: 80ex'>This systematic review undertakes a comprehensive analysis of current
research on data-to-text generation, identifying gaps, challenges, and future
directions within the field. Relevant literature in this field on datasets,
evaluation metrics, application areas, multilingualism, language models, and
hallucination mitigation methods is reviewed. Various methods for producing
high-quality text are explored, addressing the challenge of hallucinations in
data-to-text generation. These methods include re-ranking, traditional and
neural pipeline architecture, planning architectures, data cleaning, controlled
generation, and modification of models and training techniques. Their
effectiveness and limitations are assessed, highlighting the need for
universally applicable strategies to mitigate hallucinations. The review also
examines the usage, popularity, and impact of datasets, alongside evaluation
metrics, with an emphasis on both automatic and human assessment. Additionally,
the evolution of data-to-text models, particularly the widespread adoption of
transformer models, is discussed. Despite advancements in text quality, the
review emphasizes the importance of research in low-resourced languages and the
engineering of datasets in these languages to promote inclusivity. Finally,
several application domains of data-to-text are highlighted, emphasizing their
relevance in such domains. Overall, this review serves as a guiding framework
for fostering innovation and advancing data-to-text generation.</div><div><a href='http://arxiv.org/abs/2402.08496v3'>2402.08496v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17268v1")'>Weaver: Foundation Models for Creative Writing</div>
<div id='2401.17268v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T18:58:43Z</div><div>Authors: Tiannan Wang, Jiamin Chen, Qingrui Jia, Shuai Wang, Ruoyu Fang, Huilin Wang, Zhaowei Gao, Chunzhao Xie, Chuou Xu, Jihong Dai, Yibin Liu, Jialong Wu, Shengwei Ding, Long Li, Zhiwei Huang, Xinle Deng, Teng Yu, Gangan Ma, Han Xiao, Zixin Chen, Danjun Xiang, Yunxia Wang, Yuanyuan Zhu, Yi Xiao, Jing Wang, Yiru Wang, Siran Ding, Jiayang Huang, Jiayi Xu, Yilihamu Tayier, Zhenyu Hu, Yuan Gao, Chengfeng Zheng, Yueshu Ye, Yihang Li, Lei Wan, Xinyue Jiang, Yujie Wang, Siyu Cheng, Zhule Song, Xiangru Tang, Xiaohua Xu, Ningyu Zhang, Huajun Chen, Yuchen Eleanor Jiang, Wangchunshu Zhou</div><div style='padding-top: 10px; width: 80ex'>This work introduces Weaver, our first family of large language models (LLMs)
dedicated to content creation. Weaver is pre-trained on a carefully selected
corpus that focuses on improving the writing capabilities of large language
models. We then fine-tune Weaver for creative and professional writing purposes
and align it to the preference of professional writers using a suit of novel
methods for instruction data synthesis and LLM alignment, making it able to
produce more human-like texts and follow more diverse instructions for content
creation. The Weaver family consists of models of Weaver Mini (1.8B), Weaver
Base (6B), Weaver Pro (14B), and Weaver Ultra (34B) sizes, suitable for
different applications and can be dynamically dispatched by a routing agent
according to query complexity to balance response quality and computation cost.
Evaluation on a carefully curated benchmark for assessing the writing
capabilities of LLMs shows Weaver models of all sizes outperform generalist
LLMs several times larger than them. Notably, our most-capable Weaver Ultra
model surpasses GPT-4, a state-of-the-art generalist LLM, on various writing
scenarios, demonstrating the advantage of training specialized LLMs for writing
purposes. Moreover, Weaver natively supports retrieval-augmented generation
(RAG) and function calling (tool usage). We present various use cases of these
abilities for improving AI-assisted writing systems, including integration of
external knowledge bases, tools, or APIs, and providing personalized writing
assistance. Furthermore, we discuss and summarize a guideline and best
practices for pre-training and fine-tuning domain-specific LLMs.</div><div><a href='http://arxiv.org/abs/2401.17268v1'>2401.17268v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13923v2")'>Towards 3D Molecule-Text Interpretation in Language Models</div>
<div id='2401.13923v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T03:42:00Z</div><div>Authors: Sihang Li, Zhiyuan Liu, Yanchen Luo, Xiang Wang, Xiangnan He, Kenji Kawaguchi, Tat-Seng Chua, Qi Tian</div><div style='padding-top: 10px; width: 80ex'>Language Models (LMs) have greatly influenced diverse domains. However, their
inherent limitation in comprehending 3D molecular structures has considerably
constrained their potential in the biomolecular domain. To bridge this gap, we
focus on 3D molecule-text interpretation, and propose 3D-MoLM: 3D-Molecular
Language Modeling. Specifically, 3D-MoLM enables an LM to interpret and analyze
3D molecules by equipping the LM with a 3D molecular encoder. This integration
is achieved by a 3D molecule-text projector, bridging the 3D molecular
encoder's representation space and the LM's input space. Moreover, to enhance
3D-MoLM's ability of cross-modal molecular understanding and instruction
following, we meticulously curated a 3D molecule-centric instruction tuning
dataset -- 3D-MoIT. Through 3D molecule-text alignment and 3D molecule-centric
instruction tuning, 3D-MoLM establishes an integration of 3D molecular encoder
and LM. It significantly surpasses existing baselines on downstream tasks,
including molecule-text retrieval, molecule captioning, and more challenging
open-text molecular QA tasks, especially focusing on 3D-dependent properties.
We release our codes and datasets at https://github.com/lsh0520/3D-MoLM.</div><div><a href='http://arxiv.org/abs/2401.13923v2'>2401.13923v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.07627v1")'>generAItor: Tree-in-the-Loop Text Generation for Language Model
  Explainability and Adaptation</div>
<div id='2403.07627v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T13:09:15Z</div><div>Authors: Thilo Spinner, Rebecca Kehlbeck, Rita Sevastjanova, Tobias Stähle, Daniel A. Keim, Oliver Deussen, Mennatallah El-Assady</div><div style='padding-top: 10px; width: 80ex'>Large language models (LLMs) are widely deployed in various downstream tasks,
e.g., auto-completion, aided writing, or chat-based text generation. However,
the considered output candidates of the underlying search algorithm are
under-explored and under-explained. We tackle this shortcoming by proposing a
tree-in-the-loop approach, where a visual representation of the beam search
tree is the central component for analyzing, explaining, and adapting the
generated outputs. To support these tasks, we present generAItor, a visual
analytics technique, augmenting the central beam search tree with various
task-specific widgets, providing targeted visualizations and interaction
possibilities. Our approach allows interactions on multiple levels and offers
an iterative pipeline that encompasses generating, exploring, and comparing
output candidates, as well as fine-tuning the model based on adapted data. Our
case study shows that our tool generates new insights in gender bias analysis
beyond state-of-the-art template-based methods. Additionally, we demonstrate
the applicability of our approach in a qualitative user study. Finally, we
quantitatively evaluate the adaptability of the model to few samples, as
occurring in text-generation use cases.</div><div><a href='http://arxiv.org/abs/2403.07627v1'>2403.07627v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15589v1")'>Prompting LLMs to Compose Meta-Review Drafts from Peer-Review Narratives
  of Scholarly Manuscripts</div>
<div id='2402.15589v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T20:14:16Z</div><div>Authors: Shubhra Kanti Karmaker Santu, Sanjeev Kumar Sinha, Naman Bansal, Alex Knipper, Souvika Sarkar, John Salvador, Yash Mahajan, Sri Guttikonda, Mousumi Akter, Matthew Freestone, Matthew C. Williams Jr</div><div style='padding-top: 10px; width: 80ex'>One of the most important yet onerous tasks in the academic peer-reviewing
process is composing meta-reviews, which involves understanding the core
contributions, strengths, and weaknesses of a scholarly manuscript based on
peer-review narratives from multiple experts and then summarizing those
multiple experts' perspectives into a concise holistic overview. Given the
latest major developments in generative AI, especially Large Language Models
(LLMs), it is very compelling to rigorously study the utility of LLMs in
generating such meta-reviews in an academic peer-review setting. In this paper,
we perform a case study with three popular LLMs, i.e., GPT-3.5, LLaMA2, and
PaLM2, to automatically generate meta-reviews by prompting them with different
types/levels of prompts based on the recently proposed TELeR taxonomy. Finally,
we perform a detailed qualitative study of the meta-reviews generated by the
LLMs and summarize our findings and recommendations for prompting LLMs for this
complex task.</div><div><a href='http://arxiv.org/abs/2402.15589v1'>2402.15589v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07183v1")'>Monitoring AI-Modified Content at Scale: A Case Study on the Impact of
  ChatGPT on AI Conference Peer Reviews</div>
<div id='2403.07183v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T21:51:39Z</div><div>Authors: Weixin Liang, Zachary Izzo, Yaohui Zhang, Haley Lepp, Hancheng Cao, Xuandong Zhao, Lingjiao Chen, Haotian Ye, Sheng Liu, Zhi Huang, Daniel A. McFarland, James Y. Zou</div><div style='padding-top: 10px; width: 80ex'>We present an approach for estimating the fraction of text in a large corpus
which is likely to be substantially modified or produced by a large language
model (LLM). Our maximum likelihood model leverages expert-written and
AI-generated reference texts to accurately and efficiently examine real-world
LLM-use at the corpus level. We apply this approach to a case study of
scientific peer review in AI conferences that took place after the release of
ChatGPT: ICLR 2024, NeurIPS 2023, CoRL 2023 and EMNLP 2023. Our results suggest
that between 6.5% and 16.9% of text submitted as peer reviews to these
conferences could have been substantially modified by LLMs, i.e. beyond
spell-checking or minor writing updates. The circumstances in which generated
text occurs offer insight into user behavior: the estimated fraction of
LLM-generated text is higher in reviews which report lower confidence, were
submitted close to the deadline, and from reviewers who are less likely to
respond to author rebuttals. We also observe corpus-level trends in generated
text which may be too subtle to detect at the individual level, and discuss the
implications of such trends on peer review. We call for future
interdisciplinary work to examine how LLM use is changing our information and
knowledge practices.</div><div><a href='http://arxiv.org/abs/2403.07183v1'>2403.07183v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04760v1")'>iScore: Visual Analytics for Interpreting How Language Models
  Automatically Score Summaries</div>
<div id='2403.04760v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T18:56:39Z</div><div>Authors: Adam Coscia, Langdon Holmes, Wesley Morris, Joon Suh Choi, Scott Crossley, Alex Endert</div><div style='padding-top: 10px; width: 80ex'>The recent explosion in popularity of large language models (LLMs) has
inspired learning engineers to incorporate them into adaptive educational tools
that automatically score summary writing. Understanding and evaluating LLMs is
vital before deploying them in critical learning environments, yet their
unprecedented size and expanding number of parameters inhibits transparency and
impedes trust when they underperform. Through a collaborative user-centered
design process with several learning engineers building and deploying summary
scoring LLMs, we characterized fundamental design challenges and goals around
interpreting their models, including aggregating large text inputs, tracking
score provenance, and scaling LLM interpretability methods. To address their
concerns, we developed iScore, an interactive visual analytics tool for
learning engineers to upload, score, and compare multiple summaries
simultaneously. Tightly integrated views allow users to iteratively revise the
language in summaries, track changes in the resulting LLM scores, and visualize
model weights at multiple levels of abstraction. To validate our approach, we
deployed iScore with three learning engineers over the course of a month. We
present a case study where interacting with iScore led a learning engineer to
improve their LLM's score accuracy by three percentage points. Finally, we
conducted qualitative interviews with the learning engineers that revealed how
iScore enabled them to understand, evaluate, and build trust in their LLMs
during deployment.</div><div><a href='http://arxiv.org/abs/2403.04760v1'>2403.04760v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02930v1")'>A Second Look on BASS -- Boosting Abstractive Summarization with Unified
  Semantic Graphs -- A Replication Study</div>
<div id='2403.02930v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T12:48:29Z</div><div>Authors: Osman Alperen Koraş, Jörg Schlötterer, Christin Seifert</div><div style='padding-top: 10px; width: 80ex'>We present a detailed replication study of the BASS framework, an abstractive
summarization system based on the notion of Unified Semantic Graphs. Our
investigation includes challenges in replicating key components and an ablation
study to systematically isolate error sources rooted in replicating novel
components. Our findings reveal discrepancies in performance compared to the
original work. We highlight the significance of paying careful attention even
to reasonably omitted details for replicating advanced frameworks like BASS,
and emphasize key practices for writing replicable papers.</div><div><a href='http://arxiv.org/abs/2403.02930v1'>2403.02930v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.02333v3")'>Beyond Extraction: Contextualising Tabular Data for Efficient
  Summarisation by Language Models</div>
<div id='2401.02333v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T16:16:14Z</div><div>Authors: Uday Allu, Biddwan Ahmed, Vishesh Tripathi</div><div style='padding-top: 10px; width: 80ex'>The conventional use of the Retrieval-Augmented Generation (RAG) architecture
has proven effective for retrieving information from diverse documents.
However, challenges arise in handling complex table queries, especially within
PDF documents containing intricate tabular structures.This research introduces
an innovative approach to enhance the accuracy of complex table queries in
RAG-based systems. Our methodology involves storing PDFs in the retrieval
database and extracting tabular content separately. The extracted tables
undergo a process of context enrichment, concatenating headers with
corresponding values. To ensure a comprehensive understanding of the enriched
data, we employ a fine-tuned version of the Llama-2-chat language model for
summarisation within the RAG architecture. Furthermore, we augment the tabular
data with contextual sense using the ChatGPT 3.5 API through a one-shot prompt.
This enriched data is then fed into the retrieval database alongside other
PDFs. Our approach aims to significantly improve the precision of complex table
queries, offering a promising solution to a longstanding challenge in
information retrieval.</div><div><a href='http://arxiv.org/abs/2401.02333v3'>2401.02333v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.07625v1")'>AutoMathText: Autonomous Data Selection with Language Models for
  Mathematical Texts</div>
<div id='2402.07625v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T13:09:21Z</div><div>Authors: Yifan Zhang, Yifan Luo, Yang Yuan, Andrew Chi-Chih Yao</div><div style='padding-top: 10px; width: 80ex'>To improve language models' proficiency in mathematical reasoning via
continual pretraining, we introduce a novel strategy that leverages base
language models for autonomous data selection. Departing from conventional
supervised fine-tuning or trained classifiers with human-annotated data, our
approach utilizes meta-prompted language models as zero-shot verifiers to
autonomously evaluate and select high-quality mathematical content, and we
release the curated open-source AutoMathText dataset encompassing over 200GB of
data. To demonstrate the efficacy of our method, we continuously pretrained a
7B-parameter Mistral language model on the AutoMathText dataset, achieving
substantial improvements in downstream performance on the MATH dataset with a
token amount reduced by orders of magnitude compared to previous continuous
pretraining works. Our method showcases a 2 times increase in pretraining token
efficiency compared to baselines, underscoring the potential of our approach in
enhancing models' mathematical reasoning capabilities. The AutoMathText dataset
is available at https://huggingface.co/datasets/math-ai/AutoMathText. The code
is available at https://github.com/yifanzhang-pro/AutoMathText.</div><div><a href='http://arxiv.org/abs/2402.07625v1'>2402.07625v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10326v1")'>CDGP: Automatic Cloze Distractor Generation based on Pre-trained
  Language Model</div>
<div id='2403.10326v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T14:14:26Z</div><div>Authors: Shang-Hsuan Chiang, Ssu-Cheng Wang, Yao-Chung Fan</div><div style='padding-top: 10px; width: 80ex'>Manually designing cloze test consumes enormous time and efforts. The major
challenge lies in wrong option (distractor) selection. Having carefully-design
distractors improves the effectiveness of learner ability assessment. As a
result, the idea of automatically generating cloze distractor is motivated. In
this paper, we investigate cloze distractor generation by exploring the
employment of pre-trained language models (PLMs) as an alternative for
candidate distractor generation. Experiments show that the PLM-enhanced model
brings a substantial performance improvement. Our best performing model
advances the state-of-the-art result from 14.94 to 34.17 (NDCG@10 score). Our
code and dataset is available at https://github.com/AndyChiangSH/CDGP.</div><div><a href='http://arxiv.org/abs/2403.10326v1'>2403.10326v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.15043v1")'>Health Text Simplification: An Annotated Corpus for Digestive Cancer
  Education and Novel Strategies for Reinforcement Learning</div>
<div id='2401.15043v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T18:13:57Z</div><div>Authors: Md Mushfiqur Rahman, Mohammad Sabik Irbaz, Kai North, Michelle S. Williams, Marcos Zampieri, Kevin Lybarger</div><div style='padding-top: 10px; width: 80ex'>Objective: The reading level of health educational materials significantly
influences information understandability and accessibility, particularly for
minoritized populations. Many patient educational resources surpass the reading
level and complexity of widely accepted standards. There is a critical need for
high-performing text simplification models in health information to enhance
dissemination and literacy. This need is particularly acute in cancer
education, where effective prevention and screening education can substantially
reduce morbidity and mortality.
  Methods: We introduce Simplified Digestive Cancer (SimpleDC), a parallel
corpus of cancer education materials tailored for health text simplification
research. Utilizing SimpleDC alongside the existing Med-EASi corpus, we explore
Large Language Model (LLM)-based simplification methods, including fine-tuning,
reinforcement learning (RL), reinforcement learning with human feedback (RLHF),
domain adaptation, and prompt-based approaches. Our experimentation encompasses
Llama 2 and GPT-4. A novel RLHF reward function is introduced, featuring a
lightweight model adept at distinguishing between original and simplified
texts, thereby enhancing the model's effectiveness with unlabeled data.
  Results: Fine-tuned Llama 2 models demonstrated high performance across
various metrics. Our innovative RLHF reward function surpassed existing RL text
simplification reward functions in effectiveness. The results underscore that
RL/RLHF can augment fine-tuning, facilitating model training on unlabeled text
and improving performance. Additionally, these methods effectively adapt
out-of-domain text simplification models to targeted domains.</div><div><a href='http://arxiv.org/abs/2401.15043v1'>2401.15043v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.02954v1")'>DeepSeek LLM: Scaling Open-Source Language Models with Longtermism</div>
<div id='2401.02954v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T18:59:13Z</div><div>Authors: DeepSeek-AI, :, Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He, Wenjie Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi Li, Yao Li, Y. K. Li, Wenfeng Liang, Fangyun Lin, A. X. Liu, Bo Liu, Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren, Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Junxiao Song, Xuecheng Su, Jingxiang Sun, Yaofeng Sun, Minghui Tang, Bingxuan Wang, Peiyi Wang, Shiyu Wang, Yaohui Wang, Yongji Wang, Tong Wu, Y. Wu, Xin Xie, Zhenda Xie, Ziwei Xie, Yiliang Xiong, Hanwei Xu, R. X. Xu, Yanhong Xu, Dejian Yang, Yuxiang You, Shuiping Yu, Xingkai Yu, B. Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang, Mingchuan Zhang, Minghua Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao, Shangyan Zhou, Shunfeng Zhou, Qihao Zhu, Yuheng Zou</div><div style='padding-top: 10px; width: 80ex'>The rapid development of open-source large language models (LLMs) has been
truly remarkable. However, the scaling law described in previous literature
presents varying conclusions, which casts a dark cloud over scaling LLMs. We
delve into the study of scaling laws and present our distinctive findings that
facilitate scaling of large scale models in two commonly used open-source
configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek
LLM, a project dedicated to advancing open-source language models with a
long-term perspective. To support the pre-training phase, we have developed a
dataset that currently consists of 2 trillion tokens and is continuously
expanding. We further conduct supervised fine-tuning (SFT) and Direct
Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the
creation of DeepSeek Chat models. Our evaluation results demonstrate that
DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in
the domains of code, mathematics, and reasoning. Furthermore, open-ended
evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance
compared to GPT-3.5.</div><div><a href='http://arxiv.org/abs/2401.02954v1'>2401.02954v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03300v2")'>DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open
  Language Models</div>
<div id='2402.03300v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T18:55:32Z</div><div>Authors: Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, Daya Guo</div><div style='padding-top: 10px; width: 80ex'>Mathematical reasoning poses a significant challenge for language models due
to its complex and structured nature. In this paper, we introduce DeepSeekMath
7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B
math-related tokens sourced from Common Crawl, together with natural language
and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the
competition-level MATH benchmark without relying on external toolkits and
voting techniques, approaching the performance level of Gemini-Ultra and GPT-4.
Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH.
The mathematical reasoning capability of DeepSeekMath is attributed to two key
factors: First, we harness the significant potential of publicly available web
data through a meticulously engineered data selection pipeline. Second, we
introduce Group Relative Policy Optimization (GRPO), a variant of Proximal
Policy Optimization (PPO), that enhances mathematical reasoning abilities while
concurrently optimizing the memory usage of PPO.</div><div><a href='http://arxiv.org/abs/2402.03300v2'>2402.03300v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17453v2")'>DS-Agent: Automated Data Science by Empowering Large Language Models
  with Case-Based Reasoning</div>
<div id='2402.17453v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T12:26:07Z</div><div>Authors: Siyuan Guo, Cheng Deng, Ying Wen, Hechang Chen, Yi Chang, Jun Wang</div><div style='padding-top: 10px; width: 80ex'>In this work, we investigate the potential of large language models (LLMs)
based agents to automate data science tasks, with the goal of comprehending
task requirements, then building and training the best-fit machine learning
models. Despite their widespread success, existing LLM agents are hindered by
generating unreasonable experiment plans within this scenario. To this end, we
present DS-Agent, a novel automatic framework that harnesses LLM agent and
case-based reasoning (CBR). In the development stage, DS-Agent follows the CBR
framework to structure an automatic iteration pipeline, which can flexibly
capitalize on the expert knowledge from Kaggle, and facilitate consistent
performance improvement through the feedback mechanism. Moreover, DS-Agent
implements a low-resource deployment stage with a simplified CBR paradigm to
adapt past successful solutions from the development stage for direct code
generation, significantly reducing the demand on foundational capabilities of
LLMs. Empirically, DS-Agent with GPT-4 achieves an unprecedented 100% success
rate in the development stage, while attaining 36% improvement on average one
pass rate across alternative LLMs in the deployment stage. In both stages,
DS-Agent achieves the best rank in performance, costing \$1.60 and \$0.13 per
run with GPT-4, respectively. Our code is open-sourced at
https://github.com/guosyjlu/DS-Agent.</div><div><a href='http://arxiv.org/abs/2402.17453v2'>2402.17453v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18679v3")'>Data Interpreter: An LLM Agent For Data Science</div>
<div id='2402.18679v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T19:49:55Z</div><div>Authors: Sirui Hong, Yizhang Lin, Bang Liu, Bangbang Liu, Binhao Wu, Danyang Li, Jiaqi Chen, Jiayi Zhang, Jinlin Wang, Li Zhang, Lingyao Zhang, Min Yang, Mingchen Zhuge, Taicheng Guo, Tuo Zhou, Wei Tao, Wenyi Wang, Xiangru Tang, Xiangtao Lu, Xiawu Zheng, Xinbing Liang, Yaying Fei, Yuheng Cheng, Zongze Xu, Chenglin Wu</div><div style='padding-top: 10px; width: 80ex'>Large Language Model (LLM)-based agents have demonstrated remarkable
effectiveness. However, their performance can be compromised in data science
scenarios that require real-time data adjustment, expertise in optimization due
to complex dependencies among various tasks, and the ability to identify
logical errors for precise reasoning. In this study, we introduce the Data
Interpreter, a solution designed to solve with code that emphasizes three
pivotal techniques to augment problem-solving in data science: 1) dynamic
planning with hierarchical graph structures for real-time data adaptability;2)
tool integration dynamically to enhance code proficiency during execution,
enriching the requisite expertise;3) logical inconsistency identification in
feedback, and efficiency enhancement through experience recording. We evaluate
the Data Interpreter on various data science and real-world tasks. Compared to
open-source baselines, it demonstrated superior performance, exhibiting
significant improvements in machine learning tasks, increasing from 0.86 to
0.95. Additionally, it showed a 26% increase in the MATH dataset and a
remarkable 112% improvement in open-ended tasks. The solution will be released
at https://github.com/geekan/MetaGPT.</div><div><a href='http://arxiv.org/abs/2402.18679v3'>2402.18679v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08073v2")'>Grounding Data Science Code Generation with Input-Output Specifications</div>
<div id='2402.08073v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T21:32:49Z</div><div>Authors: Yeming Wen, Pengcheng Yin, Kensen Shi, Henryk Michalewski, Swarat Chaudhuri, Alex Polozov</div><div style='padding-top: 10px; width: 80ex'>Large language models (LLMs) have recently demonstrated a remarkable ability
to generate code from natural language (NL) prompts. However, in the real
world, NL is often too ambiguous to capture the true intent behind programming
problems, requiring additional input-output (I/O) specifications.
Unfortunately, LLMs can have difficulty aligning their outputs with both the NL
prompt and the I/O specification. In this paper, we give a way to mitigate this
issue in the context of data science programming, where tasks require explicit
I/O specifications for clarity. Specifically, we propose GIFT4Code, a novel
approach for the instruction fine-tuning of LLMs with respect to I/O
specifications. Our method leverages synthetic data produced by the LLM itself
and utilizes execution-derived feedback as a key learning signal. This
feedback, in the form of program I/O specifications, is provided to the LLM to
facilitate instruction fine-tuning. We evaluated our approach on two
challenging data science benchmarks, Arcade and DS-1000. The results
demonstrate a significant improvement in the LLM's ability to generate code
that is not only executable but also accurately aligned with user
specifications, substantially improving the quality of code generation for
complex data science tasks.</div><div><a href='http://arxiv.org/abs/2402.08073v2'>2402.08073v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16445v1")'>OMPGPT: A Generative Pre-trained Transformer Model for OpenMP</div>
<div id='2401.16445v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-28T06:06:59Z</div><div>Authors: Le Chen, Arijit Bhattacharjee, Nesreen Ahmed, Niranjan Hasabnis, Gal Oren, Vy Vo, Ali Jannesari</div><div style='padding-top: 10px; width: 80ex'>Large language models (LLMs), as epitomized by models like ChatGPT, have
revolutionized the field of natural language processing (NLP). Along with this
trend, code-based large language models such as StarCoder, WizardCoder, and
CodeLlama have emerged, trained extensively on vast repositories of code data.
Yet, inherent in their design, these models primarily focus on generative tasks
like code generation, code completion, and comment generation, and general
support for multiple programming languages. While the generic abilities of code
LLMs are useful for many programmers, the area of high-performance computing
(HPC) has a narrower set of requirements that make a smaller and more
domain-specific LM a smarter choice. This paper introduces OMPGPT, a novel
model meticulously designed to harness the inherent strengths of language
models for OpenMP pragma generation. Furthermore, we adopt and adapt prompt
engineering techniques from the NLP domain to create chain-of-OMP, an
innovative strategy designed to enhance OMPGPT's effectiveness. Our extensive
evaluations demonstrate that OMPGPT outperforms existing large language models
specialized in OpenMP tasks and maintains a notably smaller size, aligning it
more closely with the typical hardware constraints of HPC environments. We
consider our contribution as a pivotal bridge, connecting the advantage of
language models with the specific demands of HPC tasks. The success of OMPGPT
lays a solid foundation, suggesting its potential applicability and
adaptability to a wider range of HPC tasks, thereby opening new avenues in the
field of computational efficiency and effectiveness.</div><div><a href='http://arxiv.org/abs/2401.16445v1'>2401.16445v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14196v2")'>DeepSeek-Coder: When the Large Language Model Meets Programming -- The
  Rise of Code Intelligence</div>
<div id='2401.14196v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T14:17:53Z</div><div>Authors: Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, Wenfeng Liang</div><div style='padding-top: 10px; width: 80ex'>The rapid development of large language models has revolutionized code
intelligence in software development. However, the predominance of
closed-source models has restricted extensive research and development. To
address this, we introduce the DeepSeek-Coder series, a range of open-source
code models with sizes from 1.3B to 33B, trained from scratch on 2 trillion
tokens. These models are pre-trained on a high-quality project-level code
corpus and employ a fill-in-the-blank task with a 16K window to enhance code
generation and infilling. Our extensive evaluations demonstrate that
DeepSeek-Coder not only achieves state-of-the-art performance among open-source
code models across multiple benchmarks but also surpasses existing
closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models
are under a permissive license that allows for both research and unrestricted
commercial use.</div><div><a href='http://arxiv.org/abs/2401.14196v2'>2401.14196v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03065v1")'>CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution</div>
<div id='2401.03065v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T20:53:51Z</div><div>Authors: Alex Gu, Baptiste Rozière, Hugh Leather, Armando Solar-Lezama, Gabriel Synnaeve, Sida I. Wang</div><div style='padding-top: 10px; width: 80ex'>We present CRUXEval (Code Reasoning, Understanding, and eXecution
Evaluation), a benchmark consisting of 800 Python functions (3-13 lines). Each
function comes with an input-output pair, leading to two natural tasks: input
prediction and output prediction. First, we propose a generic recipe for
generating our execution benchmark which can be used to create future variation
of the benchmark. Second, we evaluate twenty code models on our benchmark and
discover that many recent high-scoring models on HumanEval do not show the same
improvements on our benchmark. Third, we show that simple CoT and fine-tuning
schemes can improve performance on our benchmark but remain far from solving
it. The best setup, GPT-4 with chain of thought (CoT), achieves a pass@1 of 75%
and 81% on input and output prediction, respectively. In contrast, Code Llama
34B achieves a pass@1 of 50% and 46% on input and output prediction,
highlighting the gap between open and closed source models. As no model is
close to acing CRUXEval, we provide examples of consistent GPT-4 failures on
simple programs as a lens into its code reasoning capabilities and areas for
improvement.</div><div><a href='http://arxiv.org/abs/2401.03065v1'>2401.03065v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07974v1")'>LiveCodeBench: Holistic and Contamination Free Evaluation of Large
  Language Models for Code</div>
<div id='2403.07974v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T17:58:04Z</div><div>Authors: Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, Ion Stoica</div><div style='padding-top: 10px; width: 80ex'>Large Language Models (LLMs) applied to code-related applications have
emerged as a prominent field, attracting significant interest from both
academia and industry. However, as new and improved LLMs are developed,
existing evaluation benchmarks (e.g., HumanEval, MBPP) are no longer sufficient
for assessing their capabilities. In this work, we propose LiveCodeBench, a
comprehensive and contamination-free evaluation of LLMs for code, which
continuously collects new problems over time from contests across three
competition platforms, namely LeetCode, AtCoder, and CodeForces. Notably, our
benchmark also focuses on a broader range of code related capabilities, such as
self-repair, code execution, and test output prediction, beyond just code
generation. Currently, LiveCodeBench hosts four hundred high-quality coding
problems that were published between May 2023 and February 2024. We have
evaluated 9 base LLMs and 20 instruction-tuned LLMs on LiveCodeBench. We
present empirical findings on contamination, holistic performance comparisons,
potential overfitting in existing benchmarks as well as individual model
comparisons. We will release all prompts and model completions for further
community analysis, along with a general toolkit for adding new scenarios and
model</div><div><a href='http://arxiv.org/abs/2403.07974v1'>2403.07974v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18734v1")'>Priority Sampling of Large Language Models for Compilers</div>
<div id='2402.18734v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T22:27:49Z</div><div>Authors: Dejan Grubisic, Chris Cummins, Volker Seeker, Hugh Leather</div><div style='padding-top: 10px; width: 80ex'>Large language models show great potential in generating and optimizing code.
Widely used sampling methods such as Nucleus Sampling increase the diversity of
generation but often produce repeated samples for low temperatures and
incoherent samples for high temperatures. Furthermore, the temperature
coefficient has to be tuned for each task, limiting its usability. We present
Priority Sampling, a simple and deterministic sampling technique that produces
unique samples ordered by the model's confidence. Each new sample expands the
unexpanded token with the highest probability in the augmented search tree.
Additionally, Priority Sampling supports generation based on regular expression
that provides a controllable and structured exploration process. Priority
Sampling outperforms Nucleus Sampling for any number of samples, boosting the
performance of the original model from 2.87% to 5% improvement over -Oz.
Moreover, it outperforms the autotuner used for the generation of labels for
the training of the original model in just 30 samples.</div><div><a href='http://arxiv.org/abs/2402.18734v1'>2402.18734v1</a></div>
</div></div>
    <div><a href="arxiv_17.html">Prev (17)</a></div>
    <div><a href="arxiv_19.html">Next (19)</a></div>
    