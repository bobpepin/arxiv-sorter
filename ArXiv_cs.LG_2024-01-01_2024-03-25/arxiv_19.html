
<!doctype html>
<meta charset="utf-8">
<style>
body { margin: 20px; }
</style>
<script>
function toggle(arxiv) {
  let elt = document.getElementById(arxiv);
  console.log(elt, elt.style.display);
  if(elt.style.display == "block") {
    elt.style.display = "none";
  } else {
    elt.style.display = "block";
  }
}
</script>
<div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14714v1")'>Compiler generated feedback for Large Language Models</div>
<div id='2403.14714v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T23:25:13Z</div><div>Authors: Dejan Grubisic, Chris Cummins, Volker Seeker, Hugh Leather</div><div style='padding-top: 10px; width: 80ex'>We introduce a novel paradigm in compiler optimization powered by Large
Language Models with compiler feedback to optimize the code size of LLVM
assembly. The model takes unoptimized LLVM IR as input and produces optimized
IR, the best optimization passes, and instruction counts of both unoptimized
and optimized IRs. Then we compile the input with generated optimization passes
and evaluate if the predicted instruction count is correct, generated IR is
compilable, and corresponds to compiled code. We provide this feedback back to
LLM and give it another chance to optimize code. This approach adds an extra
0.53% improvement over -Oz to the original model. Even though, adding more
information with feedback seems intuitive, simple sampling techniques achieve
much higher performance given 10 or more samples.</div><div><a href='http://arxiv.org/abs/2403.14714v1'>2403.14714v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.10754v1")'>When Dataflow Analysis Meets Large Language Models</div>
<div id='2402.10754v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T15:21:35Z</div><div>Authors: Chengpeng Wang, Wuqi Zhang, Zian Su, Xiangzhe Xu, Xiaoheng Xie, Xiangyu Zhang</div><div style='padding-top: 10px; width: 80ex'>Dataflow analysis is a powerful code analysis technique that reasons
dependencies between program values, offering support for code optimization,
program comprehension, and bug detection. Existing approaches require the
successful compilation of the subject program and customizations for downstream
applications. This paper introduces LLMDFA, an LLM-powered dataflow analysis
framework that analyzes arbitrary code snippets without requiring a compilation
infrastructure and automatically synthesizes downstream applications. Inspired
by summary-based dataflow analysis, LLMDFA decomposes the problem into three
sub-problems, which are effectively resolved by several essential strategies,
including few-shot chain-of-thought prompting and tool synthesis. Our
evaluation has shown that the design can mitigate the hallucination and improve
the reasoning ability, obtaining high precision and recall in detecting
dataflow-related bugs upon benchmark programs, outperforming state-of-the-art
(classic) tools, including a very recent industrial analyzer.</div><div><a href='http://arxiv.org/abs/2402.10754v1'>2402.10754v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07294v1")'>On the Effectiveness of Machine Learning-based Call Graph Pruning: An
  Empirical Study</div>
<div id='2402.07294v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T20:15:44Z</div><div>Authors: Amir M. Mir, Mehdi Keshani, Sebastian Proksch</div><div style='padding-top: 10px; width: 80ex'>Static call graph (CG) construction often over-approximates call relations,
leading to sound, but imprecise results. Recent research has explored machine
learning (ML)-based CG pruning as a means to enhance precision by eliminating
false edges. However, current methods suffer from a limited evaluation dataset,
imbalanced training data, and reduced recall, which affects practical
downstream analyses. Prior results were also not compared with advanced static
CG construction techniques yet. This study tackles these issues. We introduce
the NYXCorpus, a dataset of real-world Java programs with high test coverage
and we collect traces from test executions and build a ground truth of dynamic
CGs. We leverage these CGs to explore conservative pruning strategies during
the training and inference of ML-based CG pruners. We conduct a comparative
analysis of static CGs generated using zero control flow analysis (0-CFA) and
those produced by a context-sensitive 1-CFA algorithm, evaluating both with and
without pruning. We find that CG pruning is a difficult task for real-world
Java projects and substantial improvements in the CG precision (+25%) meet
reduced recall (-9%). However, our experiments show promising results: even
when we favor recall over precision by using an F2 metric in our experiments,
we can show that pruned CGs have comparable quality to a context-sensitive
1-CFA analysis while being computationally less demanding. Resulting CGs are
much smaller (69%), and substantially faster (3.5x speed-up), with virtually
unchanged results in our downstream analysis.</div><div><a href='http://arxiv.org/abs/2402.07294v1'>2402.07294v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.09497v1")'>Instruction Tuning for Secure Code Generation</div>
<div id='2402.09497v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T15:47:46Z</div><div>Authors: Jingxuan He, Mark Vero, Gabriela Krasnopolska, Martin Vechev</div><div style='padding-top: 10px; width: 80ex'>Modern language models (LMs) have gained widespread acceptance in everyday
and professional contexts, particularly in programming. An essential procedure
enabling this adoption is instruction tuning, which substantially enhances LMs'
practical utility by training them to follow user instructions and human
preferences. However, existing instruction tuning schemes overlook a crucial
aspect: the security of generated code. As a result, even the state-of-the-art
instruction-tuned LMs frequently produce unsafe code, posing significant
security risks. In this work, we introduce SafeCoder to address this gap.
SafeCoder performs security-centric fine-tuning using a diverse and
high-quality dataset that we collected using an automated pipeline. We
integrate the security fine-tuning with standard instruction tuning, to
facilitate a joint optimization of both security and utility. Despite its
simplicity, we show that SafeCoder is effective across a variety of popular LMs
and datasets. It is able to drastically improve security (by about 30%), while
preserving utility.</div><div><a href='http://arxiv.org/abs/2402.09497v1'>2402.09497v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.15627v1")'>MegaScale: Scaling Large Language Model Training to More Than 10,000
  GPUs</div>
<div id='2402.15627v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T22:10:59Z</div><div>Authors: Ziheng Jiang, Haibin Lin, Yinmin Zhong, Qi Huang, Yangrui Chen, Zhi Zhang, Yanghua Peng, Xiang Li, Cong Xie, Shibiao Nong, Yulu Jia, Sun He, Hongmin Chen, Zhihao Bai, Qi Hou, Shipeng Yan, Ding Zhou, Yiyao Sheng, Zhuo Jiang, Haohan Xu, Haoran Wei, Zhang Zhang, Pengfei Nie, Leqi Zou, Sida Zhao, Liang Xiang, Zherui Liu, Zhe Li, Xiaoying Jia, Jianxi Ye, Xin Jin, Xin Liu</div><div style='padding-top: 10px; width: 80ex'>We present the design, implementation and engineering experience in building
and deploying MegaScale, a production system for training large language models
(LLMs) at the scale of more than 10,000 GPUs. Training LLMs at this scale
brings unprecedented challenges to training efficiency and stability. We take a
full-stack approach that co-designs the algorithmic and system components
across model block and optimizer design, computation and communication
overlapping, operator optimization, data pipeline, and network performance
tuning. Maintaining high efficiency throughout the training process (i.e.,
stability) is an important consideration in production given the long extent of
LLM training jobs. Many hard stability issues only emerge at large scale, and
in-depth observability is the key to address them. We develop a set of
diagnosis tools to monitor system components and events deep in the stack,
identify root causes, and derive effective techniques to achieve fault
tolerance and mitigate stragglers. MegaScale achieves 55.2% Model FLOPs
Utilization (MFU) when training a 175B LLM model on 12,288 GPUs, improving the
MFU by 1.34x compared to Megatron-LM. We share our operational experience in
identifying and fixing failures and stragglers. We hope by articulating the
problems and sharing our experience from a systems perspective, this work can
inspire future LLM systems research.</div><div><a href='http://arxiv.org/abs/2402.15627v1'>2402.15627v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16467v1")'>ReGAL: Refactoring Programs to Discover Generalizable Abstractions</div>
<div id='2401.16467v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T18:45:30Z</div><div>Authors: Elias Stengel-Eskin, Archiki Prasad, Mohit Bansal</div><div style='padding-top: 10px; width: 80ex'>While large language models (LLMs) are increasingly being used for program
synthesis, they lack the global view needed to develop useful abstractions;
they generally predict programs one at a time, often repeating the same
functionality. Generating redundant code from scratch is both inefficient and
error-prone. To address this, we propose Refactoring for Generalizable
Abstraction Learning (ReGAL), a gradient-free method for learning a library of
reusable functions via code refactorization, i.e. restructuring code without
changing its execution output. ReGAL learns from a small set of existing
programs, iteratively verifying and refining its abstractions via execution. We
find that the shared function libraries discovered by ReGAL make programs
easier to predict across diverse domains. On three datasets (LOGO graphics
generation, Date reasoning, and TextCraft, a Minecraft-based text game), both
open-source and proprietary LLMs improve in accuracy when predicting programs
with ReGAL functions. For CodeLlama-13B, ReGAL results in absolute accuracy
increases of 11.5% on graphics, 26.1% on date understanding, and 8.1% on
TextCraft, outperforming GPT-3.5 in two of three domains. Our analysis reveals
ReGAL's abstractions encapsulate frequently-used subroutines as well as
environment dynamics.</div><div><a href='http://arxiv.org/abs/2401.16467v1'>2401.16467v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06503v1")'>Automatic Generation of Python Programs Using Context-Free Grammars</div>
<div id='2403.06503v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T08:25:52Z</div><div>Authors: Kamel Yamani, Marwa Naïr, Riyadh Baghdadi</div><div style='padding-top: 10px; width: 80ex'>In recent years, data has emerged as the new gold, serving as a powerful tool
for creating intelligent systems. However, procuring high-quality data remains
challenging, especially for code. To address this, we developed TinyPy
Generator, a tool that generates random Python programs using a context-free
grammar. The generated programs are guaranteed to be correct by construction.
Our system uses custom production rules (in the Backus-Naur Form (BNF) format)
to recursively generate code. This allows us to generate code with different
levels of complexity, ranging from code containing only assignments to more
complex code containing conditionals and loops. Our proposed tool enables
effortless large-scale Python code generation, beneficial for a wide range of
applications. TinyPy Generator is particularly useful in the field of machine
learning, where it can generate substantial amounts of Python code for training
Python language models. Additionally, researchers who are studying programming
languages can utilize this tool to create datasets for their experiments, which
can help validate the robustness of code interpreters or compilers. Unlike
existing research, we have open-sourced our implementation. This allows
customization according to user needs and extends potential usage to other
languages.</div><div><a href='http://arxiv.org/abs/2403.06503v1'>2403.06503v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17988v1")'>Constrained Decoding for Code Language Models via Efficient Left and
  Right Quotienting of Context-Sensitive Grammars</div>
<div id='2402.17988v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T02:12:47Z</div><div>Authors: Daniel Melcer, Nathan Fulton, Sanjay Krishna Gouda, Haifeng Qian</div><div style='padding-top: 10px; width: 80ex'>Large Language Models are powerful tools for program synthesis and advanced
auto-completion, but come with no guarantee that their output code is
syntactically correct. This paper contributes an incremental parser that allows
early rejection of syntactically incorrect code, as well as efficient detection
of complete programs for fill-in-the-middle (FItM) tasks. We develop
Earley-style parsers that operate over left and right quotients of arbitrary
context-free grammars, and we extend our incremental parsing and quotient
operations to several context-sensitive features present in the grammars of
many common programming languages. The result of these contributions is an
efficient, general, and well-grounded method for left and right quotient
parsing.
  To validate our theoretical contributions -- and the practical effectiveness
of certain design decisions -- we evaluate our method on the particularly
difficult case of FItM completion for Python 3. Our results demonstrate that
constrained generation can significantly reduce the incidence of syntax errors
in recommended code.</div><div><a href='http://arxiv.org/abs/2402.17988v1'>2402.17988v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.06990v1")'>Guided Sketch-Based Program Induction by Search Gradients</div>
<div id='2402.06990v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-10T16:47:53Z</div><div>Authors: Ahmad Ayaz Amin</div><div style='padding-top: 10px; width: 80ex'>Many tasks can be easily solved using machine learning techniques. However,
some tasks cannot readily be solved using statistical models, requiring a
symbolic approach instead. Program induction is one of the ways that such tasks
can be solved by means of capturing an interpretable and generalizable
algorithm through training. However, contemporary approaches to program
induction are not sophisticated enough to readily be applied to various types
of tasks as they tend to be formulated as a single, all-encompassing model,
usually parameterized by neural networks. In an attempt to make program
induction a viable solution for many scenarios, we propose a framework for
learning parameterized programs via search gradients using evolution
strategies. This formulation departs from traditional program induction as it
allows for the programmer to impart task-specific code to the program 'sketch',
while also enjoying the benefits of accelerated learning through end-to-end
gradient-based optimization.</div><div><a href='http://arxiv.org/abs/2402.06990v1'>2402.06990v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14606v1")'>The Elements of Differentiable Programming</div>
<div id='2403.14606v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T17:55:16Z</div><div>Authors: Mathieu Blondel, Vincent Roulet</div><div style='padding-top: 10px; width: 80ex'>Artificial intelligence has recently experienced remarkable advances, fueled
by large models, vast datasets, accelerated hardware, and, last but not least,
the transformative power of differentiable programming. This new programming
paradigm enables end-to-end differentiation of complex computer programs
(including those with control flows and data structures), making gradient-based
optimization of program parameters possible. As an emerging paradigm,
differentiable programming builds upon several areas of computer science and
applied mathematics, including automatic differentiation, graphical models,
optimization and statistics. This book presents a comprehensive review of the
fundamental concepts useful for differentiable programming. We adopt two main
perspectives, that of optimization and that of probability, with clear
analogies between the two. Differentiable programming is not merely the
differentiation of programs, but also the thoughtful design of programs
intended for differentiation. By making programs differentiable, we inherently
introduce probability distributions over their execution, providing a means to
quantify the uncertainty associated with program outputs.</div><div><a href='http://arxiv.org/abs/2403.14606v1'>2403.14606v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06834v1")'>Optimization of Discrete Parameters Using the Adaptive Gradient Method
  and Directed Evolution</div>
<div id='2401.06834v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T15:45:56Z</div><div>Authors: Andrei Beinarovich, Sergey Stepanov, Alexander Zaslavsky</div><div style='padding-top: 10px; width: 80ex'>The problem is considered of optimizing discrete parameters in the presence
of constraints. We use the stochastic sigmoid with temperature and put forward
the new adaptive gradient method CONGA. The search for an optimal solution is
carried out by a population of individuals. Each of them varies according to
gradients of the 'environment' and is characterized by two temperature
parameters with different annealing schedules. Unadapted individuals die, and
optimal ones interbreed, the result is directed evolutionary dynamics. The
proposed method is illustrated using the well-known combinatorial problem for
optimal packing of a backpack (0-1 KP).</div><div><a href='http://arxiv.org/abs/2401.06834v1'>2401.06834v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.13321v1")'>Rigor with Machine Learning from Field Theory to the Poincaré
  Conjecture</div>
<div id='2402.13321v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T19:00:59Z</div><div>Authors: Sergei Gukov, James Halverson, Fabian Ruehle</div><div style='padding-top: 10px; width: 80ex'>Machine learning techniques are increasingly powerful, leading to many
breakthroughs in the natural sciences, but they are often stochastic,
error-prone, and blackbox. How, then, should they be utilized in fields such as
theoretical physics and pure mathematics that place a premium on rigor and
understanding? In this Perspective we discuss techniques for obtaining rigor in
the natural sciences with machine learning. Non-rigorous methods may lead to
rigorous results via conjecture generation or verification by reinforcement
learning. We survey applications of these techniques-for-rigor ranging from
string theory to the smooth $4$d Poincar\'e conjecture in low-dimensional
topology. One can also imagine building direct bridges between machine learning
theory and either mathematics or theoretical physics. As examples, we describe
a new approach to field theory motivated by neural network theory, and a theory
of Riemannian metric flows induced by neural network gradient descent, which
encompasses Perelman's formulation of the Ricci flow that was utilized to
resolve the $3$d Poincar\'e conjecture.</div><div><a href='http://arxiv.org/abs/2402.13321v1'>2402.13321v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.11650v1")'>Theoretical foundations for programmatic reinforcement learning</div>
<div id='2402.11650v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T17:02:39Z</div><div>Authors: Guruprerana Shabadi, Nathanaël Fijalkow, Théo Matricon</div><div style='padding-top: 10px; width: 80ex'>The field of Reinforcement Learning (RL) is concerned with algorithms for
learning optimal policies in unknown stochastic environments. Programmatic RL
studies representations of policies as programs, meaning involving higher order
constructs such as control loops. Despite attracting a lot of attention at the
intersection of the machine learning and formal methods communities, very
little is known on the theoretical front about programmatic RL: what are good
classes of programmatic policies? How large are optimal programmatic policies?
How can we learn them? The goal of this paper is to give first answers to these
questions, initiating a theoretical study of programmatic RL.</div><div><a href='http://arxiv.org/abs/2402.11650v1'>2402.11650v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11446v1")'>LLM Guided Evolution -- The Automation of Models Advancing Models</div>
<div id='2403.11446v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T03:44:55Z</div><div>Authors: Clint Morris, Michael Jurado, Jason Zutty</div><div style='padding-top: 10px; width: 80ex'>In the realm of machine learning, traditional model development and automated
approaches like AutoML typically rely on layers of abstraction, such as
tree-based or Cartesian genetic programming. Our study introduces "Guided
Evolution" (GE), a novel framework that diverges from these methods by
utilizing Large Language Models (LLMs) to directly modify code. GE leverages
LLMs for a more intelligent, supervised evolutionary process, guiding mutations
and crossovers. Our unique "Evolution of Thought" (EoT) technique further
enhances GE by enabling LLMs to reflect on and learn from the outcomes of
previous mutations. This results in a self-sustaining feedback loop that
augments decision-making in model evolution. GE maintains genetic diversity,
crucial for evolutionary algorithms, by leveraging LLMs' capability to generate
diverse responses from expertly crafted prompts and modulate model temperature.
This not only accelerates the evolution process but also injects expert like
creativity and insight into the process. Our application of GE in evolving the
ExquisiteNetV2 model demonstrates its efficacy: the LLM-driven GE autonomously
produced variants with improved accuracy, increasing from 92.52% to 93.34%,
without compromising model compactness. This underscores the potential of LLMs
to accelerate the traditional model design pipeline, enabling models to
autonomously evolve and enhance their own designs.</div><div><a href='http://arxiv.org/abs/2403.11446v1'>2403.11446v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.11522v2")'>LOOPer: A Learned Automatic Code Optimizer For Polyhedral Compilers</div>
<div id='2403.11522v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T07:22:31Z</div><div>Authors: Massinissa Merouani, Khaled Afif Boudaoud, Iheb Nassim Aouadj, Nassim Tchoulak, Islem Kara Bernou, Hamza Benyamina, Fatima Benbouzid-Si Tayeb, Karima Benatchba, Hugh Leather, Riyadh Baghdadi</div><div style='padding-top: 10px; width: 80ex'>While polyhedral compilers have shown success in implementing advanced code
transformations, they still have challenges in selecting the most profitable
transformations that lead to the best speedups. This has motivated the use of
machine learning to build cost models to guide the search for polyhedral
optimizations. State-of-the-art polyhedral compilers have demonstrated a viable
proof-of-concept of this approach. While such a proof-of-concept has shown
promise, it still has significant limitations. State-of-the-art polyhedral
compilers that use a deep-learning cost model only support a small subset of
affine transformations, limiting their ability to apply complex code
transformations. They also only support simple programs that have a single loop
nest and a rectangular iteration domain, limiting their applicability to many
programs. These limitations significantly impact the generality of such
compilers and autoschedulers and put into question the whole approach. In this
paper, we introduce LOOPer, the first polyhedral autoscheduler that uses a
deep-learning based cost model and covers a large set of affine transformations
and programs. It supports the exploration of a large set of affine
transformations, allowing the application of complex sequences of polyhedral
transformations. It also supports the optimization of programs with multiple
loop nests and with rectangular and non-rectangular iteration domains, allowing
the optimization of an extensive set of programs. We implement and evaluate
LOOPer and show that it achieves speedups over the state-of-the-art. On the
Polybench benchmark, LOOPer achieves a geometric mean speedup of 1.59x over
Tiramisu. LOOPer also achieves competitive speedups with a geometric mean
speedup of 1.34x over Pluto, a state-of-the-art polyhedral compiler that does
not use a machine-learning based cost model.</div><div><a href='http://arxiv.org/abs/2403.11522v2'>2403.11522v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10652v2")'>AutoChunk: Automated Activation Chunk for Memory-Efficient Long Sequence
  Inference</div>
<div id='2401.10652v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T11:58:13Z</div><div>Authors: Xuanlei Zhao, Shenggan Cheng, Guangyang Lu, Jiarui Fang, Haotian Zhou, Bin Jia, Ziming Liu, Yang You</div><div style='padding-top: 10px; width: 80ex'>Large deep learning models have achieved impressive performance across a
range of applications. However, their large memory requirements, including
parameter memory and activation memory, have become a significant challenge for
their practical serving. While existing methods mainly address parameter
memory, the importance of activation memory has been overlooked. Especially for
long input sequences, activation memory is expected to experience a significant
exponential growth as the length of sequences increases. In this approach, we
propose AutoChunk, an automatic and adaptive compiler system that efficiently
reduces activation memory for long sequence inference by chunk strategies. The
proposed system generates chunk plans by optimizing through multiple stages. In
each stage, the chunk search pass explores all possible chunk candidates and
the chunk selection pass identifies the optimal one. At runtime, AutoChunk
employs code generation to automatically apply chunk strategies. The
experiments demonstrate that AutoChunk can reduce over 80\% of activation
memory while maintaining speed loss within 10%, extend max sequence length by
3.2x to 11.7x, and outperform state-of-the-art methods by a large margin.</div><div><a href='http://arxiv.org/abs/2401.10652v2'>2401.10652v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00176v1")'>SoD$^2$: Statically Optimizing Dynamic Deep Neural Network</div>
<div id='2403.00176v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T23:04:01Z</div><div>Authors: Wei Niu, Gagan Agrawal, Bin Ren</div><div style='padding-top: 10px; width: 80ex'>Though many compilation and runtime systems have been developed for DNNs in
recent years, the focus has largely been on static DNNs. Dynamic DNNs, where
tensor shapes and sizes and even the set of operators used are dependent upon
the input and/or execution, are becoming common. This paper presents SoD$^2$, a
comprehensive framework for optimizing Dynamic DNNs. The basis of our approach
is a classification of common operators that form DNNs, and the use of this
classification towards a Rank and Dimension Propagation (RDP) method. This
framework statically determines the shapes of operators as known constants,
symbolic constants, or operations on these. Next, using RDP we enable a series
of optimizations, like fused code generation, execution (order) planning, and
even runtime memory allocation plan generation. By evaluating the framework on
10 emerging Dynamic DNNs and comparing it against several existing systems, we
demonstrate both reductions in execution latency and memory requirements, with
RDP-enabled key optimizations responsible for much of the gains. Our evaluation
results show that SoD$^2$ runs up to $3.9\times$ faster than these systems
while saving up to $88\%$ peak memory consumption.</div><div><a href='http://arxiv.org/abs/2403.00176v1'>2403.00176v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.19475v1")'>The Counterfeit Conundrum: Can Code Language Models Grasp the Nuances of
  Their Incorrect Generations?</div>
<div id='2402.19475v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T18:59:25Z</div><div>Authors: Alex Gu, Wen-Ding Li, Naman Jain, Theo X. Olausson, Celine Lee, Koushik Sen, Armando Solar-Lezama</div><div style='padding-top: 10px; width: 80ex'>While language models are increasingly more proficient at code generation,
they still frequently generate incorrect programs. Many of these programs are
obviously wrong, but others are more subtle and pass weaker correctness checks
such as being able to compile. In this work, we focus on these counterfeit
samples: programs sampled from a language model that 1) have a high enough
log-probability to be generated at a moderate temperature and 2) pass weak
correctness checks. Overall, we discover that most models have a very shallow
understanding of counterfeits through three clear failure modes. First, models
mistakenly classify them as correct. Second, models are worse at reasoning
about the execution behaviour of counterfeits and often predict their execution
results as if they were correct. Third, when asking models to fix counterfeits,
the likelihood of a model successfully repairing a counterfeit is often even
lower than that of sampling a correct program from scratch. Counterfeits also
have very unexpected properties: first, counterfeit programs for problems that
are easier for a model to solve are not necessarily easier to detect and only
slightly easier to execute and repair. Second, counterfeits from a given model
are just as confusing to the model itself as they are to other models. Finally,
both strong and weak models are able to generate counterfeit samples that
equally challenge all models. In light of our findings, we recommend that care
and caution be taken when relying on models to understand their own samples,
especially when no external feedback is incorporated.</div><div><a href='http://arxiv.org/abs/2402.19475v1'>2402.19475v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.09074v2")'>Code Simulation Challenges for Large Language Models</div>
<div id='2401.09074v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T09:23:59Z</div><div>Authors: Emanuele La Malfa, Christoph Weinhuber, Orazio Torre, Fangru Lin, Anthony Cohn, Nigel Shadbolt, Michael Wooldridge</div><div style='padding-top: 10px; width: 80ex'>We investigate the extent to which Large Language Models (LLMs) can simulate
the execution of computer code and algorithms. We begin by looking at straight
line programs, and show that current LLMs demonstrate poor performance even
with such simple programs -- performance rapidly degrades with the length of
code. We then investigate the ability of LLMs to simulate programs that contain
critical paths and redundant instructions. We also go beyond straight line
program simulation with sorting algorithms and nested loops, and we show the
computational complexity of a routine directly affects the ability of an LLM to
simulate its execution. We observe that LLMs execute instructions sequentially
and with a low error margin only for short programs or standard procedures.
LLMs' code simulation is in tension with their pattern recognition and
memorisation capabilities: on tasks where memorisation is detrimental, we
propose a novel prompting method to simulate code execution line by line.
Empirically, our new Chain of Simulation (CoSm) method improves on the standard
Chain of Thought prompting approach by avoiding the pitfalls of memorisation.</div><div><a href='http://arxiv.org/abs/2401.09074v2'>2401.09074v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02167v1")'>Vi(E)va LLM! A Conceptual Stack for Evaluating and Interpreting
  Generative AI-based Visualizations</div>
<div id='2402.02167v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T14:28:55Z</div><div>Authors: Luca Podo, Muhammad Ishmal, Marco Angelini</div><div style='padding-top: 10px; width: 80ex'>The automatic generation of visualizations is an old task that, through the
years, has shown more and more interest from the research and practitioner
communities. Recently, large language models (LLM) have become an interesting
option for supporting generative tasks related to visualization, demonstrating
initial promising results. At the same time, several pitfalls, like the
multiple ways of instructing an LLM to generate the desired result, the
different perspectives leading the generation (code-based, image-based,
grammar-based), and the presence of hallucinations even for the visualization
generation task, make their usage less affordable than expected. Following
similar initiatives for benchmarking LLMs, this paper copes with the problem of
modeling the evaluation of a generated visualization through an LLM. We propose
a theoretical evaluation stack, EvaLLM, that decomposes the evaluation effort
in its atomic components, characterizes their nature, and provides an overview
of how to implement and interpret them. We also designed and implemented an
evaluation platform that provides a benchmarking resource for the visualization
generation task. The platform supports automatic and manual scoring conducted
by multiple assessors to support a fine-grained and semantic evaluation based
on the EvaLLM stack. Two case studies on GPT3.5-turbo with Code Interpreter and
Llama2-70-b models show the benefits of EvaLLM and illustrate interesting
results on the current state-of-the-art LLM-generated visualizations.</div><div><a href='http://arxiv.org/abs/2402.02167v1'>2402.02167v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09230v1")'>Context Composing for Full Line Code Completion</div>
<div id='2402.09230v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T15:17:37Z</div><div>Authors: Anton Semenkin, Yaroslav Sokolov, Evgeniia Vu</div><div style='padding-top: 10px; width: 80ex'>Code Completion is one of the most used Integrated Development Environment
(IDE) features, which affects the everyday life of a software developer. Modern
code completion approaches moved from the composition of several static
analysis-based contributors to pipelines that involve neural networks. This
change allows the proposal of longer code suggestions while maintaining the
relatively short time spent on generation itself. At JetBrains, we put a lot of
effort into perfecting the code completion workflow so it can be both helpful
and non-distracting for a programmer. We managed to ship the Full Line Code
Completion feature to PyCharm Pro IDE and proved its usefulness in A/B testing
on hundreds of real Python users. The paper describes our approach to context
composing for the Transformer model that is a core of the feature's
implementation. In addition to that, we share our next steps to improve the
feature and emphasize the importance of several research aspects in the area.</div><div><a href='http://arxiv.org/abs/2402.09230v1'>2402.09230v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17967v1")'>CONCORD: Towards a DSL for Configurable Graph Code Representation</div>
<div id='2401.17967v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T16:16:48Z</div><div>Authors: Mootez Saad, Tushar Sharma</div><div style='padding-top: 10px; width: 80ex'>Deep learning is widely used to uncover hidden patterns in large code
corpora. To achieve this, constructing a format that captures the relevant
characteristics and features of source code is essential. Graph-based
representations have gained attention for their ability to model structural and
semantic information. However, existing tools lack flexibility in constructing
graphs across different programming languages, limiting their use.
Additionally, the output of these tools often lacks interoperability and
results in excessively large graphs, making graph-based neural networks
training slower and less scalable.
  We introduce CONCORD, a domain-specific language to build customizable graph
representations. It implements reduction heuristics to reduce graphs' size
complexity. We demonstrate its effectiveness in code smell detection as an
illustrative use case and show that: first, CONCORD can produce code
representations automatically per the specified configuration, and second, our
heuristics can achieve comparable performance with significantly reduced size.
CONCORD will help researchers a) create and experiment with customizable
graph-based code representations for different software engineering tasks
involving DL, b) reduce the engineering work to generate graph representations,
c) address the issue of scalability in GNN models, and d) enhance the
reproducibility of experiments in research through a standardized approach to
code representation and analysis.</div><div><a href='http://arxiv.org/abs/2401.17967v1'>2401.17967v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00699v1")'>PeaTMOSS: A Dataset and Initial Analysis of Pre-Trained Models in
  Open-Source Software</div>
<div id='2402.00699v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T15:55:50Z</div><div>Authors: Wenxin Jiang, Jerin Yasmin, Jason Jones, Nicholas Synovic, Jiashen Kuo, Nathaniel Bielanski, Yuan Tian, George K. Thiruvathukal, James C. Davis</div><div style='padding-top: 10px; width: 80ex'>The development and training of deep learning models have become increasingly
costly and complex. Consequently, software engineers are adopting pre-trained
models (PTMs) for their downstream applications. The dynamics of the PTM supply
chain remain largely unexplored, signaling a clear need for structured datasets
that document not only the metadata but also the subsequent applications of
these models. Without such data, the MSR community cannot comprehensively
understand the impact of PTM adoption and reuse. This paper presents the
PeaTMOSS dataset, which comprises metadata for 281,638 PTMs and detailed
snapshots for all PTMs with over 50 monthly downloads (14,296 PTMs), along with
28,575 open-source software repositories from GitHub that utilize these models.
Additionally, the dataset includes 44,337 mappings from 15,129 downstream
GitHub repositories to the 2,530 PTMs they use. To enhance the dataset's
comprehensiveness, we developed prompts for a large language model to
automatically extract model metadata, including the model's training datasets,
parameters, and evaluation metrics. Our analysis of this dataset provides the
first summary statistics for the PTM supply chain, showing the trend of PTM
development and common shortcomings of PTM package documentation. Our example
application reveals inconsistencies in software licenses across PTMs and their
dependent projects. PeaTMOSS lays the foundation for future research, offering
rich opportunities to investigate the PTM supply chain. We outline mining
opportunities on PTMs, their downstream usage, and cross-cutting questions.</div><div><a href='http://arxiv.org/abs/2402.00699v1'>2402.00699v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03069v1")'>Towards Enhancing the Reproducibility of Deep Learning Bugs: An
  Empirical Study</div>
<div id='2401.03069v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T21:30:13Z</div><div>Authors: Mehil B. Shah, Mohammad Masudur Rahman, Foutse Khomh</div><div style='padding-top: 10px; width: 80ex'>Context: Deep learning has achieved remarkable progress in various domains.
However, like traditional software systems, deep learning systems contain bugs,
which can have severe impacts, as evidenced by crashes involving autonomous
vehicles. Despite substantial advancements in deep learning techniques, little
research has focused on reproducing deep learning bugs, which hinders resolving
them. Existing literature suggests that only 3% of deep learning bugs are
reproducible, underscoring the need for further research.
  Objective: This paper examines the reproducibility of deep learning bugs. We
identify edit actions and useful information that could improve deep learning
bug reproducibility.
  Method: First, we construct a dataset of 668 deep learning bugs from Stack
Overflow and Defects4ML across 3 frameworks and 22 architectures. Second, we
select 102 bugs using stratified sampling and try to determine their
reproducibility. While reproducing these bugs, we identify edit actions and
useful information necessary for their reproduction. Third, we used the Apriori
algorithm to identify useful information and edit actions required to reproduce
specific bug types. Finally, we conduct a user study with 22 developers to
assess the effectiveness of our findings in real-life settings.
  Results: We successfully reproduced 85 bugs and identified ten edit actions
and five useful information categories that can help us reproduce deep learning
bugs. Our findings improved bug reproducibility by 22.92% and reduced
reproduction time by 24.35% based on our user study.
  Conclusions: Our research addresses the critical issue of deep learning bug
reproducibility. Practitioners and researchers can leverage our findings to
improve deep learning bug reproducibility.</div><div><a href='http://arxiv.org/abs/2401.03069v1'>2401.03069v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03322v1")'>Deep Configuration Performance Learning: A Systematic Survey and
  Taxonomy</div>
<div id='2403.03322v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T21:05:16Z</div><div>Authors: Jingzhi Gong, Tao Chen</div><div style='padding-top: 10px; width: 80ex'>Performance is arguably the most crucial attribute that reflects the behavior
of a configurable software system. However, given the increasing scale and
complexity of modern software, modeling and predicting how various
configurations can impact performance becomes one of the major challenges in
software maintenance. As such, performance is often modeled without having a
thorough knowledge of the software system, but relying mainly on data, which
fits precisely with the purpose of deep learning.
  In this paper, we conduct a comprehensive review exclusively on the topic of
deep learning for performance learning of configurable software, covering 948
searched papers spanning six indexing services, based on which 85 primary
papers were extracted and analyzed. Our results summarize the key topics and
statistics on how the configuration data is prepared; how the deep
configuration performance learning model is built; how the model is evaluated
and how they are exploited in different tasks related to software
configuration. We also identify the good practice and the potentially
problematic phenomena from the studies surveyed, together with insights on
future opportunities for the field. To promote open science, all the raw
results of this survey can be accessed at our repository:
https://github.com/ideas-labo/DCPL-SLR.</div><div><a href='http://arxiv.org/abs/2403.03322v1'>2403.03322v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.14975v1")'>Embedding-based search in JetBrains IDEs</div>
<div id='2401.14975v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T16:07:42Z</div><div>Authors: Evgeny Abramov, Nikolai Palchikov</div><div style='padding-top: 10px; width: 80ex'>Most modern Integrated Development Environments (IDEs) and code editors have
a feature to search across available functionality and items in an open
project. In JetBrains IDEs, this feature is called Search Everywhere: it allows
users to search for files, actions, classes, symbols, settings, and anything
from VCS history from a single entry point. However, it works with the
candidates obtained by algorithms that don't account for semantics, e.g.,
synonyms, complex word permutations, part of the speech modifications, and
typos. In this work, we describe the machine learning approach we implemented
to improve the discoverability of search items. We also share the obstacles
encountered during this process and how we overcame them.</div><div><a href='http://arxiv.org/abs/2401.14975v1'>2401.14975v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11585v1")'>Linguacodus: A Synergistic Framework for Transformative Code Generation
  in Machine Learning Pipelines</div>
<div id='2403.11585v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T08:58:47Z</div><div>Authors: Ekaterina Trofimova, Emil Sataev, Andrey E. Ustyuzhanin</div><div style='padding-top: 10px; width: 80ex'>In the ever-evolving landscape of machine learning, seamless translation of
natural language descriptions into executable code remains a formidable
challenge. This paper introduces Linguacodus, an innovative framework designed
to tackle this challenge by deploying a dynamic pipeline that iteratively
transforms natural language task descriptions into code through high-level
data-shaping instructions. The core of Linguacodus is a fine-tuned large
language model (LLM), empowered to evaluate diverse solutions for various
problems and select the most fitting one for a given task. This paper details
the fine-tuning process, and sheds light on how natural language descriptions
can be translated into functional code. Linguacodus represents a substantial
leap towards automated code generation, effectively bridging the gap between
task descriptions and executable code. It holds great promise for advancing
machine learning applications across diverse domains. Additionally, we propose
an algorithm capable of transforming a natural description of an ML task into
code with minimal human interaction. In extensive experiments on a vast machine
learning code dataset originating from Kaggle, we showcase the effectiveness of
Linguacodus. The investigations highlight its potential applications across
diverse domains, emphasizing its impact on applied machine learning in various
scientific fields.</div><div><a href='http://arxiv.org/abs/2403.11585v1'>2403.11585v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08500v1")'>Code Generation with AlphaCodium: From Prompt Engineering to Flow
  Engineering</div>
<div id='2401.08500v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T17:00:36Z</div><div>Authors: Tal Ridnik, Dedy Kredo, Itamar Friedman</div><div style='padding-top: 10px; width: 80ex'>Code generation problems differ from common natural language problems - they
require matching the exact syntax of the target language, identifying happy
paths and edge cases, paying attention to numerous small details in the problem
spec, and addressing other code-specific issues and requirements. Hence, many
of the optimizations and tricks that have been successful in natural language
generation may not be effective for code tasks. In this work, we propose a new
approach to code generation by LLMs, which we call AlphaCodium - a test-based,
multi-stage, code-oriented iterative flow, that improves the performances of
LLMs on code problems. We tested AlphaCodium on a challenging code generation
dataset called CodeContests, which includes competitive programming problems
from platforms such as Codeforces. The proposed flow consistently and
significantly improves results. On the validation set, for example, GPT-4
accuracy (pass@5) increased from 19% with a single well-designed direct prompt
to 44% with the AlphaCodium flow. Many of the principles and best practices
acquired in this work, we believe, are broadly applicable to general code
generation tasks. Full implementation is available at:
https://github.com/Codium-ai/AlphaCodium</div><div><a href='http://arxiv.org/abs/2401.08500v1'>2401.08500v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00097v1")'>Code-Aware Prompting: A study of Coverage Guided Test Generation in
  Regression Setting using LLM</div>
<div id='2402.00097v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T18:21:49Z</div><div>Authors: Gabriel Ryan, Siddhartha Jain, Mingyue Shang, Shiqi Wang, Xiaofei Ma, Murali Krishna Ramanathan, Baishakhi Ray</div><div style='padding-top: 10px; width: 80ex'>Testing plays a pivotal role in ensuring software quality, yet conventional
Search Based Software Testing (SBST) methods often struggle with complex
software units, achieving suboptimal test coverage. Recent work using large
language models (LLMs) for test generation have focused on improving generation
quality through optimizing the test generation context and correcting errors in
model outputs, but use fixed prompting strategies that prompt the model to
generate tests without additional guidance. As a result LLM-generated test
suites still suffer from low coverage. In this paper, we present SymPrompt, a
code-aware prompting strategy for LLMs in test generation. SymPrompt's approach
is based on recent work that demonstrates LLMs can solve more complex logical
problems when prompted to reason about the problem in a multi-step fashion. We
apply this methodology to test generation by deconstructing the testsuite
generation process into a multi-stage sequence, each of which is driven by a
specific prompt aligned with the execution paths of the method under test, and
exposing relevant type and dependency focal context to the model. Our approach
enables pretrained LLMs to generate more complete test cases without any
additional training. We implement SymPrompt using the TreeSitter parsing
framework and evaluate on a benchmark challenging methods from open source
Python projects. SymPrompt enhances correct test generations by a factor of 5
and bolsters relative coverage by 26% for CodeGen2. Notably, when applied to
GPT-4, symbolic path prompts improve coverage by over 2x compared to baseline
prompting strategies.</div><div><a href='http://arxiv.org/abs/2402.00097v1'>2402.00097v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17626v1")'>Generative AI to Generate Test Data Generators</div>
<div id='2401.17626v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T06:58:26Z</div><div>Authors: Benoit Baudry, Khashayar Etemadi, Sen Fang, Yogya Gamage, Yi Liu, Yuxin Liu, Martin Monperrus, Javier Ron, André Silva, Deepika Tiwari</div><div style='padding-top: 10px; width: 80ex'>Generating fake data is an essential dimension of modern software testing, as
demonstrated by the number and significance of data faking libraries. Yet,
developers of faking libraries cannot keep up with the wide range of data to be
generated for different natural languages and domains. In this paper, we assess
the ability of generative AI for generating test data in different domains. We
design three types of prompts for Large Language Models (LLMs), which perform
test data generation tasks at different levels of integrability: 1) raw test
data generation, 2) synthesizing programs in a specific language that generate
useful test data, and 3) producing programs that use state-of-the-art faker
libraries. We evaluate our approach by prompting LLMs to generate test data for
11 domains. The results show that LLMs can successfully generate realistic test
data generators in a wide range of domains at all three levels of
integrability.</div><div><a href='http://arxiv.org/abs/2401.17626v1'>2401.17626v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08147v1")'>Verified Multi-Step Synthesis using Large Language Models and Monte
  Carlo Tree Search</div>
<div id='2402.08147v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T00:55:14Z</div><div>Authors: David Brandfonbrener, Sibi Raja, Tarun Prasad, Chloe Loughridge, Jianang Yang, Simon Henniger, William E. Byrd, Robert Zinkov, Nada Amin</div><div style='padding-top: 10px; width: 80ex'>We present an approach using Monte Carlo Tree Search (MCTS) to guide Large
Language Models (LLMs) to generate verified programs in Dafny, Lean and Coq.
Our method, which we call VMCTS, leverages the verifier inside the search
algorithm by checking partial programs at each step. In combination with the
LLM prior, the verifier feedback raises the synthesis capabilities of open
source models. On a set of five verified programming problems, we find that in
four problems where the base model cannot solve the question even when
re-sampling solutions for one hour, VMCTS can solve the problems within 6
minutes. The base model with VMCTS is even competitive with ChatGPT4 augmented
with plugins and multiple re-tries on these problems. Our code and benchmarks
are available at
https://github.com/namin/llm-verified-with-monte-carlo-tree-search .</div><div><a href='http://arxiv.org/abs/2402.08147v1'>2402.08147v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00093v1")'>ChIRAAG: ChatGPT Informed Rapid and Automated Assertion Generation</div>
<div id='2402.00093v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T12:41:27Z</div><div>Authors: Bhabesh Mali, Karthik Maddala, Sweeya Reddy, Vatsal Gupta, Chandan Karfa, Ramesh Karri</div><div style='padding-top: 10px; width: 80ex'>System Verilog Assertion (SVA) formulation, a critical yet complex task, is a
pre-requisite in the Formal Property Verification (FPV) process. Traditionally,
SVA formulation involves expert-driven interpretation of specifications. This
is time consuming and prone to human error. However, recent advances in Large
Language Models (LLM), LLM-informed automatic assertion generation is gaining
interest. We designed a novel LLM-based pipeline to generate assertions in
English Language, Linear Temporal Logic, and SVA from natural language
specifications. We developed a custom LLM-based on OpenAI GPT4 for our
experiments. Furthermore, we developed testbenches to verify/validate the
LLM-generated assertions. Only 43% of LLM-generated raw assertions had errors,
including syntax and logical errors. By iteratively prompting the LLMs using
carefully crafted prompts derived from test case failures, the pipeline could
generate correct SVAs after a maximum of nine iterations of prompting. Our
results show that LLMs can streamline the assertion generation workflow,
reshaping verification workflows.</div><div><a href='http://arxiv.org/abs/2402.00093v1'>2402.00093v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.12222v1")'>CovRL: Fuzzing JavaScript Engines with Coverage-Guided Reinforcement
  Learning for LLM-based Mutation</div>
<div id='2402.12222v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T15:30:40Z</div><div>Authors: Jueon Eom, Seyeon Jeong, Taekyoung Kwon</div><div style='padding-top: 10px; width: 80ex'>Fuzzing is an effective bug-finding technique but it struggles with complex
systems like JavaScript engines that demand precise grammatical input.
Recently, researchers have adopted language models for context-aware mutation
in fuzzing to address this problem. However, existing techniques are limited in
utilizing coverage guidance for fuzzing, which is rather performed in a
black-box manner. This paper presents a novel technique called CovRL
(Coverage-guided Reinforcement Learning) that combines Large Language Models
(LLMs) with reinforcement learning from coverage feedback. Our fuzzer,
CovRL-Fuzz, integrates coverage feedback directly into the LLM by leveraging
the Term Frequency-Inverse Document Frequency (TF-IDF) method to construct a
weighted coverage map. This map is key in calculating the fuzzing reward, which
is then applied to the LLM-based mutator through reinforcement learning.
CovRL-Fuzz, through this approach, enables the generation of test cases that
are more likely to discover new coverage areas, thus improving vulnerability
detection while minimizing syntax and semantic errors, all without needing
extra post-processing. Our evaluation results indicate that CovRL-Fuzz
outperforms the state-of-the-art fuzzers in terms of code coverage and
bug-finding capabilities: CovRL-Fuzz identified 48 real-world security-related
bugs in the latest JavaScript engines, including 39 previously unknown
vulnerabilities and 11 CVEs.</div><div><a href='http://arxiv.org/abs/2402.12222v1'>2402.12222v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.11671v1")'>HDLdebugger: Streamlining HDL debugging with Large Language Models</div>
<div id='2403.11671v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T11:19:37Z</div><div>Authors: Xufeng Yao, Haoyang Li, Tsz Ho Chan, Wenyi Xiao, Mingxuan Yuan, Yu Huang, Lei Chen, Bei Yu</div><div style='padding-top: 10px; width: 80ex'>In the domain of chip design, Hardware Description Languages (HDLs) play a
pivotal role. However, due to the complex syntax of HDLs and the limited
availability of online resources, debugging HDL codes remains a difficult and
time-intensive task, even for seasoned engineers. Consequently, there is a
pressing need to develop automated HDL code debugging models, which can
alleviate the burden on hardware engineers. Despite the strong capabilities of
Large Language Models (LLMs) in generating, completing, and debugging software
code, their utilization in the specialized field of HDL debugging has been
limited and, to date, has not yielded satisfactory results. In this paper, we
propose an LLM-assisted HDL debugging framework, namely HDLdebugger, which
consists of HDL debugging data generation via a reverse engineering approach, a
search engine for retrieval-augmented generation, and a retrieval-augmented LLM
fine-tuning approach. Through the integration of these components, HDLdebugger
can automate and streamline HDL debugging for chip design. Our comprehensive
experiments, conducted on an HDL code dataset sourced from Huawei, reveal that
HDLdebugger outperforms 13 cutting-edge LLM baselines, displaying exceptional
effectiveness in HDL code debugging.</div><div><a href='http://arxiv.org/abs/2403.11671v1'>2403.11671v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08683v1")'>Zero-Shot RTL Code Generation with Attention Sink Augmented Large
  Language Models</div>
<div id='2401.08683v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T17:41:38Z</div><div>Authors: Selim Sandal, Ismail Akturk</div><div style='padding-top: 10px; width: 80ex'>The design and optimization of hardware have traditionally been
resource-intensive, demanding considerable expertise and dependence on
established design automation tools. This paper discusses the possibility of
exploiting large language models to streamline the code generation process in
hardware design. In contrast to earlier studies, this paper aims to use large
language models that accepts high-level design specifications through a single
prompt to generate corresponding Register-Transfer Level (RTL) code. The
ability to use large language models on RTL code generation not only expedites
design iteration cycles but also facilitates the exploration of design spaces
that have computational challenges for conventional techniques. Through our
evaluation, we demonstrate the shortcoming of existing attention mechanisms,
and present the abilities of language models to produce functional, optimized,
and industry-standard compliant RTL code when a novel attention mechanism is
used. These findings underscore the expanding role of large language models in
shaping the future landscape of architectural exploration and automation in
hardware design.</div><div><a href='http://arxiv.org/abs/2401.08683v1'>2401.08683v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03289v1")'>Make Every Move Count: LLM-based High-Quality RTL Code Generation Using
  MCTS</div>
<div id='2402.03289v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T18:47:04Z</div><div>Authors: Matthew DeLorenzo, Animesh Basak Chowdhury, Vasudev Gohil, Shailja Thakur, Ramesh Karri, Siddharth Garg, Jeyavijayan Rajendran</div><div style='padding-top: 10px; width: 80ex'>Existing large language models (LLMs) for register transfer level code
generation face challenges like compilation failures and suboptimal power,
performance, and area (PPA) efficiency. This is due to the lack of PPA
awareness in conventional transformer decoding algorithms. In response, we
present an automated transformer decoding algorithm that integrates Monte Carlo
tree-search for lookahead, guiding the transformer to produce compilable,
functionally correct, and PPA-optimized code. Empirical evaluation with a
fine-tuned language model on RTL codesets shows that our proposed technique
consistently generates functionally correct code compared to prompting-only
methods and effectively addresses the PPA-unawareness drawback of naive large
language models. For the largest design generated by the state-of-the-art LLM
(16-bit adder), our technique can achieve a 31.8% improvement in the area-delay
product.</div><div><a href='http://arxiv.org/abs/2402.03289v1'>2402.03289v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08696v1")'>Hierarchical Source-to-Post-Route QoR Prediction in High-Level Synthesis
  with GNNs</div>
<div id='2401.08696v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-14T07:24:08Z</div><div>Authors: Mingzhe Gao, Jieru Zhao, Zhe Lin, Minyi Guo</div><div style='padding-top: 10px; width: 80ex'>High-level synthesis (HLS) notably speeds up the hardware design process by
avoiding RTL programming. However, the turnaround time of HLS increases
significantly when post-route quality of results (QoR) are considered during
optimization. To tackle this issue, we propose a hierarchical post-route QoR
prediction approach for FPGA HLS, which features: (1) a modeling flow that
directly estimates latency and post-route resource usage from C/C++ programs;
(2) a graph construction method that effectively represents the control and
data flow graph of source code and effects of HLS pragmas; and (3) a
hierarchical GNN training and prediction method capable of capturing the impact
of loop hierarchies. Experimental results show that our method presents a
prediction error of less than 10% for different types of QoR metrics, which
gains tremendous improvement compared with the state-of-the-art GNN methods. By
adopting our proposed methodology, the runtime for design space exploration in
HLS is shortened to tens of minutes and the achieved ADRS is reduced to 6.91%
on average.</div><div><a href='http://arxiv.org/abs/2401.08696v1'>2401.08696v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18311v1")'>Escaping Local Optima in Global Placement</div>
<div id='2402.18311v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T13:11:06Z</div><div>Authors: Ke Xue, Xi Lin, Yunqi Shi, Shixiong Kai, Siyuan Xu, Chao Qian</div><div style='padding-top: 10px; width: 80ex'>Placement is crucial in the physical design, as it greatly affects power,
performance, and area metrics. Recent advancements in analytical methods, such
as DREAMPlace, have demonstrated impressive performance in global placement.
However, DREAMPlace has some limitations, e.g., may not guarantee legalizable
placements under the same settings, leading to fragile and unpredictable
results. This paper highlights the main issue as being stuck in local optima,
and proposes a hybrid optimization framework to efficiently escape the local
optima, by perturbing the placement result iteratively. The proposed framework
achieves significant improvements compared to state-of-the-art methods on two
popular benchmarks.</div><div><a href='http://arxiv.org/abs/2402.18311v1'>2402.18311v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00103v1")'>On Robustness and Generalization of ML-Based Congestion Predictors to
  Valid and Imperceptible Perturbations</div>
<div id='2403.00103v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T20:11:47Z</div><div>Authors: Chester Holtz, Yucheng Wang, Chung-Kuan Cheng, Bill Lin</div><div style='padding-top: 10px; width: 80ex'>There is substantial interest in the use of machine learning (ML)-based
techniques throughout the electronic computer-aided design (CAD) flow,
particularly methods based on deep learning. However, while deep learning
methods have achieved state-of-the-art performance in several applications,
recent work has demonstrated that neural networks are generally vulnerable to
small, carefully chosen perturbations of their input (e.g. a single pixel
change in an image). In this work, we investigate robustness in the context of
ML-based EDA tools -- particularly for congestion prediction. As far as we are
aware, we are the first to explore this concept in the context of ML-based EDA.
  We first describe a novel notion of imperceptibility designed specifically
for VLSI layout problems defined on netlists and cell placements. Our
definition of imperceptibility is characterized by a guarantee that a
perturbation to a layout will not alter its global routing. We then demonstrate
that state-of-the-art CNN and GNN-based congestion models exhibit brittleness
to imperceptible perturbations. Namely, we show that when a small number of
cells (e.g. 1%-5% of cells) have their positions shifted such that a measure of
global congestion is guaranteed to remain unaffected (e.g. 1% of the design
adversarially shifted by 0.001% of the layout space results in a predicted
decrease in congestion of up to 90%, while no change in congestion is implied
by the perturbation). In other words, the quality of a predictor can be made
arbitrarily poor (i.e. can be made to predict that a design is
"congestion-free") for an arbitrary input layout. Next, we describe a simple
technique to train predictors that improves robustness to these perturbations.
Our work indicates that CAD engineers should be cautious when integrating
neural network-based mechanisms in EDA flows to ensure robust and high-quality
results.</div><div><a href='http://arxiv.org/abs/2403.00103v1'>2403.00103v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.04811v1")'>Quantifying Contamination in Evaluating Code Generation Capabilities of
  Language Models</div>
<div id='2403.04811v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T21:45:35Z</div><div>Authors: Martin Riddell, Ansong Ni, Arman Cohan</div><div style='padding-top: 10px; width: 80ex'>While large language models have achieved remarkable performance on various
code generation benchmarks, there have been growing concerns regarding
potential contamination of these benchmarks as they may be leaked into
pretraining and finetuning data. While recent work has investigated
contamination in natural language generation and understanding tasks, there has
been less extensive research into how data contamination impacts the evaluation
of code generation, which is critical for understanding the robustness and
reliability of LLMs in programming contexts. In this work, we perform a
comprehensive study of data contamination of popular code generation
benchmarks, and precisely quantify their overlap with pretraining corpus
through both surface-level and semantic-level matching. In our experiments, we
show that there are substantial overlap between popular code generation
benchmarks and open training corpus, and models perform significantly better on
the subset of the benchmarks where similar solutions are seen during training.
We also conduct extensive analysis on the factors that affects model
memorization and generalization, such as model size, problem difficulty, and
question length. We release all resulting files from our matching pipeline for
future research.</div><div><a href='http://arxiv.org/abs/2403.04811v1'>2403.04811v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07865v2")'>Exploring Safety Generalization Challenges of Large Language Models via
  Code</div>
<div id='2403.07865v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T17:55:38Z</div><div>Authors: Qibing Ren, Chang Gao, Jing Shao, Junchi Yan, Xin Tan, Yu Qiao, Wai Lam, Lizhuang Ma</div><div style='padding-top: 10px; width: 80ex'>The rapid advancement of Large Language Models (LLMs) has brought about
remarkable capabilities in natural language processing but also raised concerns
about their potential misuse. While strategies like supervised fine-tuning and
reinforcement learning from human feedback have enhanced their safety, these
methods primarily focus on natural languages, which may not generalize to other
domains. This paper introduces CodeAttack, a framework that transforms natural
language inputs into code inputs, presenting a novel environment for testing
the safety generalization of LLMs. Our comprehensive studies on
state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a
common safety vulnerability of these models against code input: CodeAttack
consistently bypasses the safety guardrails of all models more than 80% of the
time. Furthermore, we find that a larger distribution gap between CodeAttack
and natural language leads to weaker safety generalization, such as encoding
natural language input with data structures or using less popular programming
languages. These findings highlight new safety risks in the code domain and the
need for more robust safety alignment algorithms to match the code capabilities
of LLMs.</div><div><a href='http://arxiv.org/abs/2403.07865v2'>2403.07865v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05566v3")'>Sleeper Agents: Training Deceptive LLMs that Persist Through Safety
  Training</div>
<div id='2401.05566v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T22:14:35Z</div><div>Authors: Evan Hubinger, Carson Denison, Jesse Mu, Mike Lambert, Meg Tong, Monte MacDiarmid, Tamera Lanham, Daniel M. Ziegler, Tim Maxwell, Newton Cheng, Adam Jermyn, Amanda Askell, Ansh Radhakrishnan, Cem Anil, David Duvenaud, Deep Ganguli, Fazl Barez, Jack Clark, Kamal Ndousse, Kshitij Sachan, Michael Sellitto, Mrinank Sharma, Nova DasSarma, Roger Grosse, Shauna Kravec, Yuntao Bai, Zachary Witten, Marina Favaro, Jan Brauner, Holden Karnofsky, Paul Christiano, Samuel R. Bowman, Logan Graham, Jared Kaplan, Sören Mindermann, Ryan Greenblatt, Buck Shlegeris, Nicholas Schiefer, Ethan Perez</div><div style='padding-top: 10px; width: 80ex'>Humans are capable of strategically deceptive behavior: behaving helpfully in
most situations, but then behaving very differently in order to pursue
alternative objectives when given the opportunity. If an AI system learned such
a deceptive strategy, could we detect it and remove it using current
state-of-the-art safety training techniques? To study this question, we
construct proof-of-concept examples of deceptive behavior in large language
models (LLMs). For example, we train models that write secure code when the
prompt states that the year is 2023, but insert exploitable code when the
stated year is 2024. We find that such backdoor behavior can be made
persistent, so that it is not removed by standard safety training techniques,
including supervised fine-tuning, reinforcement learning, and adversarial
training (eliciting unsafe behavior and then training to remove it). The
backdoor behavior is most persistent in the largest models and in models
trained to produce chain-of-thought reasoning about deceiving the training
process, with the persistence remaining even when the chain-of-thought is
distilled away. Furthermore, rather than removing backdoors, we find that
adversarial training can teach models to better recognize their backdoor
triggers, effectively hiding the unsafe behavior. Our results suggest that,
once a model exhibits deceptive behavior, standard techniques could fail to
remove such deception and create a false impression of safety.</div><div><a href='http://arxiv.org/abs/2401.05566v3'>2401.05566v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03792v1")'>Neural Exec: Learning (and Learning from) Execution Triggers for Prompt
  Injection Attacks</div>
<div id='2403.03792v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T15:40:30Z</div><div>Authors: Dario Pasquini, Martin Strohmeier, Carmela Troncoso</div><div style='padding-top: 10px; width: 80ex'>We introduce a new family of prompt injection attacks, termed Neural Exec.
Unlike known attacks that rely on handcrafted strings (e.g., "Ignore previous
instructions and..."), we show that it is possible to conceptualize the
creation of execution triggers as a differentiable search problem and use
learning-based methods to autonomously generate them.
  Our results demonstrate that a motivated adversary can forge triggers that
are not only drastically more effective than current handcrafted ones but also
exhibit inherent flexibility in shape, properties, and functionality. In this
direction, we show that an attacker can design and generate Neural Execs
capable of persisting through multi-stage preprocessing pipelines, such as in
the case of Retrieval-Augmented Generation (RAG)-based applications. More
critically, our findings show that attackers can produce triggers that deviate
markedly in form and shape from any known attack, sidestepping existing
blacklist-based detection and sanitation approaches.</div><div><a href='http://arxiv.org/abs/2403.03792v1'>2403.03792v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05030v1")'>Defending Against Unforeseen Failure Modes with Latent Adversarial
  Training</div>
<div id='2403.05030v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T04:22:48Z</div><div>Authors: Stephen Casper, Lennart Schulze, Oam Patel, Dylan Hadfield-Menell</div><div style='padding-top: 10px; width: 80ex'>AI systems sometimes exhibit harmful unintended behaviors post-deployment.
This is often despite extensive diagnostics and debugging by developers.
Minimizing risks from models is challenging because the attack surface is so
large. It is not tractable to exhaustively search for inputs that may cause a
model to fail. Red-teaming and adversarial training (AT) are commonly used to
make AI systems more robust. However, they have not been sufficient to avoid
many real-world failure modes that differ from the ones adversarially trained
on. In this work, we utilize latent adversarial training (LAT) to defend
against vulnerabilities without generating inputs that elicit them. LAT
leverages the compressed, abstract, and structured latent representations of
concepts that the network actually uses for prediction. We use LAT to remove
trojans and defend against held-out classes of adversarial attacks. We show in
image classification, text classification, and text generation tasks that LAT
usually improves both robustness and performance on clean data relative to AT.
This suggests that LAT can be a promising tool for defending against failure
modes that are not explicitly identified by developers.</div><div><a href='http://arxiv.org/abs/2403.05030v1'>2403.05030v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.01706v1")'>MULTIVERSE: Exposing Large Language Model Alignment Problems in Diverse
  Worlds</div>
<div id='2402.01706v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T02:57:40Z</div><div>Authors: Xiaolong Jin, Zhuo Zhang, Xiangyu Zhang</div><div style='padding-top: 10px; width: 80ex'>Large Language Model (LLM) alignment aims to ensure that LLM outputs match
with human values. Researchers have demonstrated the severity of alignment
problems with a large spectrum of jailbreak techniques that can induce LLMs to
produce malicious content during conversations. Finding the corresponding
jailbreaking prompts usually requires substantial human intelligence or
computation resources. In this paper, we report that LLMs have different levels
of alignment in various contexts. As such, by systematically constructing many
contexts, called worlds, leveraging a Domain Specific Language describing
possible worlds (e.g., time, location, characters, actions and languages) and
the corresponding compiler, we can cost-effectively expose latent alignment
issues. Given the low cost of our method, we are able to conduct a large scale
study regarding LLM alignment issues in different worlds. Our results show that
our method outperforms the-state-of-the-art jailbreaking techniques on both
effectiveness and efficiency. In addition, our results indicate that existing
LLMs are extremely vulnerable to nesting worlds and programming language
worlds. They imply that existing alignment training focuses on the real-world
and is lacking in various (virtual) worlds where LLMs can be exploited.</div><div><a href='http://arxiv.org/abs/2402.01706v1'>2402.01706v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08699v1")'>Unsupervised Evaluation of Code LLMs with Round-Trip Correctness</div>
<div id='2402.08699v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T11:08:08Z</div><div>Authors: Miltiadis Allamanis, Sheena Panthaplackel, Pengcheng Yin</div><div style='padding-top: 10px; width: 80ex'>To evaluate code large language models (LLMs), research has relied on a few
small manually curated benchmarks, such as HumanEval and MBPP, which represent
a narrow part of the real-world software domains. In this work, we introduce
round-trip correctness (RTC) as an alternative evaluation method. RTC allows
Code LLM evaluation on a broader spectrum of real-world software domains
without the need for costly human curation. RTC rests on the idea that we can
ask a model to make a prediction (e.g., describe some code using natural
language), feed that prediction back (e.g., synthesize code from the predicted
description), and check if this round-trip leads to code that is semantically
equivalent to the original input. We show how to employ RTC to evaluate code
synthesis and editing. We find that RTC strongly correlates with model
performance on existing narrow-domain code synthesis benchmarks while allowing
us to expand to a much broader set of domains and tasks which was not
previously possible without costly human annotations.</div><div><a href='http://arxiv.org/abs/2402.08699v1'>2402.08699v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15963v2")'>NoFunEval: Funny How Code LMs Falter on Requirements Beyond Functional
  Correctness</div>
<div id='2401.15963v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T08:47:31Z</div><div>Authors: Manav Singhal, Tushar Aggarwal, Abhijeet Awasthi, Nagarajan Natarajan, Aditya Kanade</div><div style='padding-top: 10px; width: 80ex'>Existing evaluation benchmarks of language models of code (code LMs) focus
almost exclusively on whether the LMs can generate functionally-correct code.
In real-world software engineering, developers think beyond functional
correctness. They have requirements on "how" a functionality should be
implemented to meet overall system design objectives like efficiency, security,
and maintainability. They would also trust the code LMs more if the LMs
demonstrate robust understanding of requirements and code semantics.
  We propose a new benchmark NoFunEval to evaluate code LMs on non-functional
requirements and simple classification instances for both functional and
non-functional requirements. We propose a prompting method, Coding Concepts
(CoCo), as a way for a developer to communicate the domain knowledge to the
LMs. We conduct an extensive evaluation of twenty-two code LMs. Our finding is
that they generally falter when tested on our benchmark, hinting at fundamental
blindspots in their training setups. Surprisingly, even the classification
accuracy on functional-correctness instances derived from the popular HumanEval
benchmark is low, calling in question the depth of their comprehension and the
source of their success in generating functionally-correct code in the first
place. We will release our benchmark and evaluation scripts publicly at
https://aka.ms/NoFunEval.</div><div><a href='http://arxiv.org/abs/2401.15963v2'>2401.15963v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16197v1")'>Language Models for Code Completion: A Practical Evaluation</div>
<div id='2402.16197v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-25T20:43:55Z</div><div>Authors: Maliheh Izadi, Jonathan Katzy, Tim van Dam, Marc Otten, Razvan Mihai Popescu, Arie van Deursen</div><div style='padding-top: 10px; width: 80ex'>Transformer-based language models for automatic code completion have shown
great promise so far, yet the evaluation of these models rarely uses real data.
This study provides both quantitative and qualitative assessments of three
public code language models when completing real-world code. We first developed
an open-source IDE extension, Code4Me, for the online evaluation of the models.
We collected real auto-completion usage data for over a year from more than
1200 users, resulting in over 600K valid completions. These models were then
evaluated using six standard metrics across twelve programming languages. Next,
we conducted a qualitative study of 1690 real-world completion requests to
identify the reasons behind the poor model performance. A comparative analysis
of the models' performance in online and offline settings was also performed,
using benchmark synthetic datasets and two masking strategies. Our findings
suggest that while developers utilize code completion across various languages,
the best results are achieved for mainstream languages such as Python and Java.
InCoder outperformed the other models across all programming languages,
highlighting the significance of training data and objectives. Our study also
revealed that offline evaluations do not accurately reflect real-world
scenarios. Upon qualitative analysis of the model's predictions, we found that
66.3% of failures were due to the models' limitations, 24.4% occurred due to
inappropriate model usage in a development context, and 9.3% were valid
requests that developers overwrote. Given these findings, we propose several
strategies to overcome the current limitations. These include refining training
objectives, improving resilience to typographical errors, adopting hybrid
approaches, and enhancing implementations and usability.</div><div><a href='http://arxiv.org/abs/2402.16197v1'>2402.16197v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09299v1")'>Trained Without My Consent: Detecting Code Inclusion In Language Models
  Trained on Code</div>
<div id='2402.09299v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T16:41:35Z</div><div>Authors: Vahid Majdinasab, Amin Nikanjam, Foutse Khomh</div><div style='padding-top: 10px; width: 80ex'>Code auditing ensures that the developed code adheres to standards,
regulations, and copyright protection by verifying that it does not contain
code from protected sources. The recent advent of Large Language Models (LLMs)
as coding assistants in the software development process poses new challenges
for code auditing. The dataset for training these models is mainly collected
from publicly available sources. This raises the issue of intellectual property
infringement as developers' codes are already included in the dataset.
Therefore, auditing code developed using LLMs is challenging, as it is
difficult to reliably assert if an LLM used during development has been trained
on specific copyrighted codes, given that we do not have access to the training
datasets of these models. Given the non-disclosure of the training datasets,
traditional approaches such as code clone detection are insufficient for
asserting copyright infringement. To address this challenge, we propose a new
approach, TraWiC; a model-agnostic and interpretable method based on membership
inference for detecting code inclusion in an LLM's training dataset. We extract
syntactic and semantic identifiers unique to each program to train a classifier
for detecting code inclusion. In our experiments, we observe that TraWiC is
capable of detecting 83.87% of codes that were used to train an LLM. In
comparison, the prevalent clone detection tool NiCad is only capable of
detecting 47.64%. In addition to its remarkable performance, TraWiC has low
resource overhead in contrast to pair-wise clone detection that is conducted
during the auditing process of tools like CodeWhisperer reference tracker,
across thousands of code snippets.</div><div><a href='http://arxiv.org/abs/2402.09299v1'>2402.09299v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.15230v1")'>An Exploratory Investigation into Code License Infringements in Large
  Language Model Training Datasets</div>
<div id='2403.15230v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T14:23:21Z</div><div>Authors: Jonathan Katzy, Răzvan-Mihai Popescu, Arie van Deursen, Maliheh Izadi</div><div style='padding-top: 10px; width: 80ex'>Does the training of large language models potentially infringe upon code
licenses? Furthermore, are there any datasets available that can be safely used
for training these models without violating such licenses? In our study, we
assess the current trends in the field and the importance of incorporating code
into the training of large language models. Additionally, we examine publicly
available datasets to see whether these models can be trained on them without
the risk of legal issues in the future. To accomplish this, we compiled a list
of 53 large language models trained on file-level code. We then extracted their
datasets and analyzed how much they overlap with a dataset we created,
consisting exclusively of strong copyleft code.
  Our analysis revealed that every dataset we examined contained license
inconsistencies, despite being selected based on their associated repository
licenses. We analyzed a total of 514 million code files, discovering 38 million
exact duplicates present in our strong copyleft dataset. Additionally, we
examined 171 million file-leading comments, identifying 16 million with strong
copyleft licenses and another 11 million comments that discouraged copying
without explicitly mentioning a license. Based on the findings of our study,
which highlights the pervasive issue of license inconsistencies in large
language models trained on code, our recommendation for both researchers and
the community is to prioritize the development and adoption of best practices
for dataset creation and management.</div><div><a href='http://arxiv.org/abs/2403.15230v1'>2403.15230v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02047v3")'>Calibration and Correctness of Language Models for Code</div>
<div id='2402.02047v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T05:52:28Z</div><div>Authors: Claudio Spiess, David Gros, Kunal Suresh Pai, Michael Pradel, Md Rafiqul Islam Rabin, Amin Alipour, Susmit Jha, Prem Devanbu, Toufique Ahmed</div><div style='padding-top: 10px; width: 80ex'>Machine learning models are widely used but can also often be wrong. Users
would benefit from a reliable indication of whether a given output from a given
model should be trusted, so a rational decision can be made whether to use the
output or not. For example, outputs can be associated with a confidence
measure; if this confidence measure is strongly associated with likelihood of
correctness, then the model is said to be well-calibrated. In this case, for
example, high-confidence outputs could be safely accepted, and low-confidence
outputs rejected.
  Calibration has so far been studied in mostly non-generative (e.g.,
classification) settings, especially in Software Engineering. However,
generated code can quite often be wrong: Developers need to know when they
should e.g., directly use, use after careful review, or discard model-generated
code; thus Calibration is vital in generative settings. However, the notion of
correctness of generated code is non-trivial, and thus so is Calibration. In
this paper we make several contributions. We develop a framework for evaluating
the Calibration of code-generating models. We consider several tasks,
correctness criteria, datasets, and approaches, and find that by and large
generative code models are not well-calibrated out of the box. We then show how
Calibration can be improved, using standard methods such as Platt scaling. Our
contributions will lead to better-calibrated decision-making in the current use
of code generated by language models, and offers a framework for future
research to further improve calibration methods for generative models in
Software Engineering.</div><div><a href='http://arxiv.org/abs/2402.02047v3'>2402.02047v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16896v2")'>On Trojan Signatures in Large Language Models of Code</div>
<div id='2402.16896v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T22:48:29Z</div><div>Authors: Aftab Hussain, Md Rafiqul Islam Rabin, Mohammad Amin Alipour</div><div style='padding-top: 10px; width: 80ex'>Trojan signatures, as described by Fields et al. (2021), are noticeable
differences in the distribution of the trojaned class parameters (weights) and
the non-trojaned class parameters of the trojaned model, that can be used to
detect the trojaned model. Fields et al. (2021) found trojan signatures in
computer vision classification tasks with image models, such as, Resnet,
WideResnet, Densenet, and VGG. In this paper, we investigate such signatures in
the classifier layer parameters of large language models of source code.
  Our results suggest that trojan signatures could not generalize to LLMs of
code. We found that trojaned code models are stubborn, even when the models
were poisoned under more explicit settings (finetuned with pre-trained weights
frozen). We analyzed nine trojaned models for two binary classification tasks:
clone and defect detection. To the best of our knowledge, this is the first
work to examine weight-based trojan signature revelation techniques for
large-language models of code and furthermore to demonstrate that detecting
trojans only from the weights in such models is a hard problem.</div><div><a href='http://arxiv.org/abs/2402.16896v2'>2402.16896v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07562v1")'>A Flexible Cell Classification for ML Projects in Jupyter Notebooks</div>
<div id='2403.07562v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T11:50:47Z</div><div>Authors: Miguel Perez, Selin Aydin, Horst Lichter</div><div style='padding-top: 10px; width: 80ex'>Jupyter Notebook is an interactive development environment commonly used for
rapid experimentation of machine learning (ML) solutions. Describing the ML
activities performed along code cells improves the readability and
understanding of Notebooks. Manual annotation of code cells is time-consuming
and error-prone. Therefore, tools have been developed that classify the cells
of a notebook concerning the ML activity performed in them. However, the
current tools are not flexible, as they work based on look-up tables that have
been created, which map function calls of commonly used ML libraries to ML
activities. These tables must be manually adjusted to account for new or
changed libraries.
  This paper presents a more flexible approach to cell classification based on
a hybrid classification approach that combines a rule-based and a decision tree
classifier. We discuss the design rationales and describe the developed
classifiers in detail. We implemented the new flexible cell classification
approach in a tool called JupyLabel. Its evaluation and the obtained metric
scores regarding precision, recall, and F1-score are discussed. Additionally,
we compared JupyLabel with HeaderGen, an existing cell classification tool. We
were able to show that the presented flexible cell classification approach
outperforms this tool significantly.</div><div><a href='http://arxiv.org/abs/2403.07562v1'>2403.07562v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.13583v1")'>CONLINE: Complex Code Generation and Refinement with Online Searching
  and Correctness Testing</div>
<div id='2403.13583v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T13:33:55Z</div><div>Authors: Xinyi He, Jiaru Zou, Yun Lin, Mengyu Zhou, Shi Han, Zejian Yuan, Dongmei Zhang</div><div style='padding-top: 10px; width: 80ex'>Large Language Models (LLMs) have revolutionized code generation ability by
converting natural language descriptions into executable code. However,
generating complex code within real-world scenarios remains challenging due to
intricate structures, subtle bugs, understanding of advanced data types, and
lack of supplementary contents. To address these challenges, we introduce the
CONLINE framework, which enhances code generation by incorporating planned
online searches for information retrieval and automated correctness testing for
iterative refinement. CONLINE also serializes the complex inputs and outputs to
improve comprehension and generate test case to ensure the framework's
adaptability for real-world applications. CONLINE is validated through rigorous
experiments on the DS-1000 and ClassEval datasets. It shows that CONLINE
substantially improves the quality of complex code generation, highlighting its
potential to enhance the practicality and reliability of LLMs in generating
intricate code.</div><div><a href='http://arxiv.org/abs/2403.13583v1'>2403.13583v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15526v1")'>Chain-of-Specificity: An Iteratively Refining Method for Eliciting
  Knowledge from Large Language Models</div>
<div id='2402.15526v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T08:03:05Z</div><div>Authors: Kaiwen Wei, Jingyuan Zhang, Hongzhi Zhang, Fuzheng Zhang, Di Zhang, Li Jin, Yue Yu</div><div style='padding-top: 10px; width: 80ex'>Large Language Models (LLMs) exhibit remarkable generative capabilities,
enabling the generation of valuable information. Despite these advancements,
previous research found that LLMs sometimes struggle with adhering to specific
constraints (e.g., in specific place or at specific time), at times even
overlooking them, which leads to responses that are either too generic or not
fully satisfactory. Existing approaches attempted to address this issue by
decomposing or rewriting input instructions, yet they fall short in adequately
emphasizing specific constraints and in unlocking the underlying knowledge
(e.g., programming within the context of software development). In response,
this paper proposes a simple yet effective method named Chain-of-Specificity
(CoS). Specifically, CoS iteratively emphasizes the specific constraints in the
input instructions, unlocks knowledge within LLMs, and refines responses.
Experiments conducted on publicly available and self-build complex datasets
demonstrate that CoS outperforms existing methods in enhancing generated
content especially for the specificity. Besides, as the number of specific
constraints increase, other baselines falter, while CoS still performs well.
Moreover, we show that distilling responses generated by CoS effectively
enhances the ability of smaller models to follow the constrained instructions.
Resources of this paper will be released for further research.</div><div><a href='http://arxiv.org/abs/2402.15526v1'>2402.15526v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09032v1")'>CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language
  Models to Coding Preferences</div>
<div id='2403.09032v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T01:51:35Z</div><div>Authors: Martin Weyssow, Aton Kamanda, Houari Sahraoui</div><div style='padding-top: 10px; width: 80ex'>Evaluating the alignment of large language models (LLMs) with user-defined
coding preferences is a challenging endeavour that requires assessing intricate
textual LLMs' outputs. By relying on automated metrics and static analysis
tools, existing benchmarks fail to assess nuances in user instructions and LLM
outputs, highlighting the need for large-scale datasets and benchmarks for LLM
preference alignment. In this paper, we introduce CodeUltraFeedback, a
preference dataset of 10,000 complex instructions to tune and align LLMs to
coding preferences through AI feedback. We generate responses to the
instructions using a pool of 14 diverse LLMs, which we then annotate according
to their alignment with five coding preferences using the LLM-as-a-Judge
approach with GPT-3.5, producing both numerical and textual feedback. We also
present CODAL-Bench, a benchmark for assessing LLM alignment with these coding
preferences. Our results show that CodeLlama-7B-Instruct, aligned through
reinforcement learning from AI feedback (RLAIF) with direct preference
optimization (DPO) using CodeUltraFeedback's AI feedback data, outperforms 34B
LLMs on CODAL-Bench, validating the utility of CodeUltraFeedback for preference
tuning. Furthermore, we show our DPO-aligned CodeLlama model improves
functional correctness on HumanEval+ compared to the unaligned base model.
Therefore, our contributions bridge the gap in preference tuning of LLMs for
code and set the stage for further advancements in model alignment and RLAIF
for code intelligence. Our code and data are available at
https://github.com/martin-wey/CodeUltraFeedback.</div><div><a href='http://arxiv.org/abs/2403.09032v1'>2403.09032v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03003v3")'>AST-T5: Structure-Aware Pretraining for Code Generation and
  Understanding</div>
<div id='2401.03003v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T06:51:08Z</div><div>Authors: Linyuan Gong, Mostafa Elhoushi, Alvin Cheung</div><div style='padding-top: 10px; width: 80ex'>Large language models (LLMs) have made significant advancements in
code-related tasks, yet many LLMs treat code as simple sequences, neglecting
its structured nature. We introduce AST-T5, a novel pretraining paradigm that
leverages the Abstract Syntax Tree (AST) for enhanced code generation,
transpilation, and understanding. Using dynamic programming, our AST-Aware
Segmentation retains code structure, while our AST-Aware Span Corruption
objective equips the model to reconstruct various code structures. Unlike other
models, AST-T5 avoids intricate program analyses or architectural changes, so
it integrates seamlessly with any encoder-decoder Transformer. Evaluations show
that AST-T5 consistently outperforms similar-sized LMs across various
code-related tasks. Structure-awareness makes AST-T5 particularly powerful in
code-to-code tasks, surpassing CodeT5 by 2 points in exact match score for the
Bugs2Fix task and by 3 points in exact match score for Java-C# Transpilation in
CodeXGLUE. Our code and model are publicly available at
https://github.com/gonglinyuan/ast_t5.</div><div><a href='http://arxiv.org/abs/2401.03003v3'>2401.03003v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.04514v1")'>Rewriting the Code: A Simple Method for Large Language Model Augmented
  Code Search</div>
<div id='2401.04514v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-09T12:12:50Z</div><div>Authors: Haochen Li, Xin Zhou, Zhiqi Shen</div><div style='padding-top: 10px; width: 80ex'>In code search, the Generation-Augmented Retrieval (GAR) framework, which
generates exemplar code snippets to augment queries, has emerged as a promising
strategy to address the principal challenge of modality misalignment between
code snippets and natural language queries, particularly with the demonstrated
code generation capabilities of Large Language Models (LLMs). Nevertheless, our
preliminary investigations indicate that the improvements conferred by such an
LLM-augmented framework are somewhat constrained. This limitation could
potentially be ascribed to the fact that the generated codes, albeit
functionally accurate, frequently display a pronounced stylistic deviation from
the ground truth code in the codebase. In this paper, we extend the
foundational GAR framework and propose a simple yet effective method that
additionally Rewrites the Code (ReCo) within the codebase for style
normalization. Experimental results demonstrate that ReCo significantly boosts
retrieval accuracy across sparse (up to 35.7%), zero-shot dense (up to 27.6%),
and fine-tuned dense (up to 23.6%) retrieval settings in diverse search
scenarios. To further elucidate the advantages of ReCo and stimulate research
in code style normalization, we introduce Code Style Similarity, the first
metric tailored to quantify stylistic similarities in code. Notably, our
empirical findings reveal the inadequacy of existing metrics in capturing
stylistic nuances.</div><div><a href='http://arxiv.org/abs/2401.04514v1'>2401.04514v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07994v1")'>A Novel Approach for Automatic Program Repair using Round-Trip
  Translation with Large Language Models</div>
<div id='2401.07994v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T22:36:31Z</div><div>Authors: Fernando Vallecillos Ruiz, Anastasiia Grishina, Max Hort, Leon Moonen</div><div style='padding-top: 10px; width: 80ex'>Research shows that grammatical mistakes in a sentence can be corrected by
translating it to another language and back using neural machine translation
with language models. We investigate whether this correction capability of
Large Language Models (LLMs) extends to Automatic Program Repair (APR). Current
generative models for APR are pre-trained on source code and fine-tuned for
repair. This paper proposes bypassing the fine-tuning step and using Round-Trip
Translation (RTT): translation of code from one programming language to another
programming or natural language, and back. We hypothesize that RTT with LLMs
restores the most commonly seen patterns in code during pre-training, i.e.,
performs a regression toward the mean, which removes bugs as they are a form of
noise w.r.t. the more frequent, natural, bug-free code in the training data. To
test this hypothesis, we employ eight recent LLMs pre-trained on code,
including the latest GPT versions, and four common program repair benchmarks in
Java. We find that RTT with English as an intermediate language repaired 101 of
164 bugs with GPT-4 on the HumanEval-Java dataset. Moreover, 46 of these are
unique bugs that are not repaired by other LLMs fine-tuned for APR. Our
findings highlight the viability of round-trip translation with LLMs as a
technique for automated program repair and its potential for research in
software engineering.
  Keywords: automated program repair, large language model, machine translation</div><div><a href='http://arxiv.org/abs/2401.07994v1'>2401.07994v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13291v2")'>DeepCode AI Fix: Fixing Security Vulnerabilities with Large Language
  Models</div>
<div id='2402.13291v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T18:35:40Z</div><div>Authors: Berkay Berabi, Alexey Gronskiy, Veselin Raychev, Gishor Sivanrupan, Victor Chibotaru, Martin Vechev</div><div style='padding-top: 10px; width: 80ex'>The automated program repair field has attracted substantial interest over
the years, but despite significant research efforts, creating a system that
works well for complex semantic bugs such as security vulnerabilities has
proven difficult. A promising direction to solve this challenge is by
leveraging large language models (LLMs), which are increasingly used to solve
various programming tasks. In this paper, we investigate the effectiveness of
LLMs for solving code-repair task. We show that the task is difficult as it
requires the model to learn long-range code relationships, a task that
inherently relies on extensive amounts of training data. At the same time,
creating a large, clean dataset for complex program bugs and their
corresponding fixes is non-trivial. We propose a technique to address these
challenges with a new approach for querying and fine-tuning LLMs. The idea is
to use program analysis to limit the LLM's attention mechanism on the portions
of code needed to perform the fix, drastically reducing the amount of required
training data. Concretely, for training and inference, rather than feeding the
entire program to the LLM, we reduce its code to a much shorter snippet that
contains the reported defect together with the necessary context - and use that
instead. Our evaluation shows that this code reduction approach substantially
improves available models such as GPT-4 using few-shot learning, as well as
fine-tuning models. To train and evaluate our system, we created a
comprehensive code fixing dataset by extensively labeling 156 bug patterns
(including 40 security rules), requiring complex interprocedural dataflow to
discover. Our best system with Mixtral-8x7B can remove more than 80% of the
reported defects while exactly matching the human fix in between 10 and 50% of
cases, outperforming baselines based on GPT-3.5 and GPT-4, or based on
window-based models like TFix.</div><div><a href='http://arxiv.org/abs/2402.13291v2'>2402.13291v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17010v4")'>Finetuning Large Language Models for Vulnerability Detection</div>
<div id='2401.17010v4' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T13:46:49Z</div><div>Authors: Alexey Shestov, Rodion Levichev, Ravil Mussabayev, Evgeny Maslov, Anton Cheshkov, Pavel Zadorozhny</div><div style='padding-top: 10px; width: 80ex'>This paper presents the results of finetuning large language models (LLMs)
for the task of detecting vulnerabilities in source code. We leverage
WizardCoder, a recent improvement of the state-of-the-art LLM StarCoder, and
adapt it for vulnerability detection through further finetuning. To accelerate
training, we modify WizardCoder's training procedure, also we investigate
optimal training regimes. For the imbalanced dataset with many more negative
examples than positive, we also explore different techniques to improve
classification performance. The finetuned WizardCoder model achieves
improvement in ROC AUC and F1 measures on balanced and imbalanced vulnerability
datasets over CodeBERT-like model, demonstrating the effectiveness of adapting
pretrained LLMs for vulnerability detection in source code. The key
contributions are finetuning the state-of-the-art code LLM, WizardCoder,
increasing its training speed without the performance harm, optimizing the
training procedure and regimes, handling class imbalance, and improving
performance on difficult vulnerability detection datasets. This demonstrates
the potential for transfer learning by finetuning large pretrained language
models for specialized source code analysis tasks.</div><div><a href='http://arxiv.org/abs/2401.17010v4'>2401.17010v4</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10646v1")'>A Survey of Source Code Representations for Machine Learning-Based
  Cybersecurity Tasks</div>
<div id='2403.10646v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T19:27:48Z</div><div>Authors: Beatrice Casey, Joanna C. S. Santos, George Perry</div><div style='padding-top: 10px; width: 80ex'>Machine learning techniques for cybersecurity-related software engineering
tasks are becoming increasingly popular. The representation of source code is a
key portion of the technique that can impact the way the model is able to learn
the features of the source code. With an increasing number of these techniques
being developed, it is valuable to see the current state of the field to better
understand what exists and what's not there yet. This paper presents a study of
these existing ML-based approaches and demonstrates what type of
representations were used for different cybersecurity tasks and programming
languages. Additionally, we study what types of models are used with different
representations. We have found that graph-based representations are the most
popular category of representation, and Tokenizers and Abstract Syntax Trees
(ASTs) are the two most popular representations overall. We also found that the
most popular cybersecurity task is vulnerability detection, and the language
that is covered by the most techniques is C. Finally, we found that
sequence-based models are the most popular category of models, and Support
Vector Machines (SVMs) are the most popular model overall.</div><div><a href='http://arxiv.org/abs/2403.10646v1'>2403.10646v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07501v1")'>Detecting Security-Relevant Methods using Multi-label Machine Learning</div>
<div id='2403.07501v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T10:38:54Z</div><div>Authors: Oshando Johnson, Goran Piskachev, Ranjith Krishnamurthy, Eric Bodden</div><div style='padding-top: 10px; width: 80ex'>To detect security vulnerabilities, static analysis tools need to be
configured with security-relevant methods. Current approaches can automatically
identify such methods using binary relevance machine learning approaches.
However, they ignore dependencies among security-relevant methods,
over-generalize and perform poorly in practice. Additionally, users have to
nevertheless manually configure static analysis tools using the detected
methods. Based on feedback from users and our observations, the excessive
manual steps can often be tedious, error-prone and counter-intuitive.
  In this paper, we present Dev-Assist, an IntelliJ IDEA plugin that detects
security-relevant methods using a multi-label machine learning approach that
considers dependencies among labels. The plugin can automatically generate
configurations for static analysis tools, run the static analysis, and show the
results in IntelliJ IDEA. Our experiments reveal that Dev-Assist's machine
learning approach has a higher F1-Measure than related approaches. Moreover,
the plugin reduces and simplifies the manual effort required when configuring
and using static analysis tools.</div><div><a href='http://arxiv.org/abs/2403.07501v1'>2403.07501v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.11105v1")'>Are Latent Vulnerabilities Hidden Gems for Software Vulnerability
  Prediction? An Empirical Study</div>
<div id='2401.11105v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-20T03:36:01Z</div><div>Authors: Triet H. M. Le, Xiaoning Du, M. Ali Babar</div><div style='padding-top: 10px; width: 80ex'>Collecting relevant and high-quality data is integral to the development of
effective Software Vulnerability (SV) prediction models. Most of the current SV
datasets rely on SV-fixing commits to extract vulnerable functions and lines.
However, none of these datasets have considered latent SVs existing between the
introduction and fix of the collected SVs. There is also little known about the
usefulness of these latent SVs for SV prediction. To bridge these gaps, we
conduct a large-scale study on the latent vulnerable functions in two commonly
used SV datasets and their utilization for function-level and line-level SV
predictions. Leveraging the state-of-the-art SZZ algorithm, we identify more
than 100k latent vulnerable functions in the studied datasets. We find that
these latent functions can increase the number of SVs by 4x on average and
correct up to 5k mislabeled functions, yet they have a noise level of around
6%. Despite the noise, we show that the state-of-the-art SV prediction model
can significantly benefit from such latent SVs. The improvements are up to
24.5% in the performance (F1-Score) of function-level SV predictions and up to
67% in the effectiveness of localizing vulnerable lines. Overall, our study
presents the first promising step toward the use of latent SVs to improve the
quality of SV datasets and enhance the performance of SV prediction tasks.</div><div><a href='http://arxiv.org/abs/2401.11105v1'>2401.11105v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02686v2")'>Beyond Fidelity: Explaining Vulnerability Localization of Learning-based
  Detectors</div>
<div id='2401.02686v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T07:37:35Z</div><div>Authors: Baijun Cheng, Shengming Zhao, Kailong Wang, Meizhen Wang, Guangdong Bai, Ruitao Feng, Yao Guo, Lei Ma, Haoyu Wang</div><div style='padding-top: 10px; width: 80ex'>Vulnerability detectors based on deep learning (DL) models have proven their
effectiveness in recent years. However, the shroud of opacity surrounding the
decision-making process of these detectors makes it difficult for security
analysts to comprehend. To address this, various explanation approaches have
been proposed to explain the predictions by highlighting important features,
which have been demonstrated effective in other domains such as computer vision
and natural language processing. Unfortunately, an in-depth evaluation of
vulnerability-critical features, such as fine-grained vulnerability-related
code lines, learned and understood by these explanation approaches remains
lacking. In this study, we first evaluate the performance of ten explanation
approaches for vulnerability detectors based on graph and sequence
representations, measured by two quantitative metrics including fidelity and
vulnerability line coverage rate. Our results show that fidelity alone is not
sufficient for evaluating these approaches, as fidelity incurs significant
fluctuations across different datasets and detectors. We subsequently check the
precision of the vulnerability-related code lines reported by the explanation
approaches, and find poor accuracy in this task among all of them. This can be
attributed to the inefficiency of explainers in selecting important features
and the presence of irrelevant artifacts learned by DL-based detectors.</div><div><a href='http://arxiv.org/abs/2401.02686v2'>2401.02686v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.09715v1")'>Textual analysis of End User License Agreement for red-flagging
  potentially malicious software</div>
<div id='2403.09715v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T20:45:27Z</div><div>Authors: Behraj Khan, Tahir Syed, Zeshan Khan, Muhammad Rafi</div><div style='padding-top: 10px; width: 80ex'>New software and updates are downloaded by end users every day. Each
dowloaded software has associated with it an End Users License Agreements
(EULA), but this is rarely read. An EULA includes information to avoid legal
repercussions. However,this proposes a host of potential problems such as
spyware or producing an unwanted affect in the target system. End users do not
read these EULA's because of length of the document and users find it extremely
difficult to understand. Text summarization is one of the relevant solution to
these kind of problems. This require a solution which can summarize the EULA
and classify the EULA as "Benign" or "Malicious". We propose a solution in
which we have summarize the EULA and classify the EULA as "Benign" or
"Malicious". We extract EULA text of different sofware's then we classify the
text using eight different supervised classifiers. we use ensemble learning to
classify the EULA as benign or malicious using five different text
summarization methods. An accuracy of $95.8$\% shows the effectiveness of the
presented approach.</div><div><a href='http://arxiv.org/abs/2403.09715v1'>2403.09715v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.08100v1")'>Investigating the Impact of Data Contamination of Large Language Models
  in Text-to-SQL Translation</div>
<div id='2402.08100v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T22:35:40Z</div><div>Authors: Federico Ranaldi, Elena Sofia Ruzzetti, Dario Onorati, Leonardo Ranaldi, Cristina Giannone, Andrea Favalli, Raniero Romagnoli, Fabio Massimo Zanzotto</div><div style='padding-top: 10px; width: 80ex'>Understanding textual description to generate code seems to be an achieved
capability of instruction-following Large Language Models (LLMs) in zero-shot
scenario. However, there is a severe possibility that this translation ability
may be influenced by having seen target textual descriptions and the related
code. This effect is known as Data Contamination.
  In this study, we investigate the impact of Data Contamination on the
performance of GPT-3.5 in the Text-to-SQL code-generating tasks. Hence, we
introduce a novel method to detect Data Contamination in GPTs and examine
GPT-3.5's Text-to-SQL performances using the known Spider Dataset and our new
unfamiliar dataset Termite. Furthermore, we analyze GPT-3.5's efficacy on
databases with modified information via an adversarial table disconnection
(ATD) approach, complicating Text-to-SQL tasks by removing structural pieces of
information from the database. Our results indicate a significant performance
drop in GPT-3.5 on the unfamiliar Termite dataset, even with ATD modifications,
highlighting the effect of Data Contamination on LLMs in Text-to-SQL
translation tasks.</div><div><a href='http://arxiv.org/abs/2402.08100v1'>2402.08100v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.05939v1")'>Uncertainty Awareness of Large Language Models Under Code Distribution
  Shifts: A Benchmark Study</div>
<div id='2402.05939v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T00:00:32Z</div><div>Authors: Yufei Li, Simin Chen, Yanghong Guo, Wei Yang, Yue Dong, Cong Liu</div><div style='padding-top: 10px; width: 80ex'>Large Language Models (LLMs) have been widely employed in programming
language analysis to enhance human productivity. Yet, their reliability can be
compromised by various code distribution shifts, leading to inconsistent
outputs. While probabilistic methods are known to mitigate such impact through
uncertainty calibration and estimation, their efficacy in the language domain
remains underexplored compared to their application in image-based tasks. In
this work, we first introduce a large-scale benchmark dataset, incorporating
three realistic patterns of code distribution shifts at varying intensities.
Then we thoroughly investigate state-of-the-art probabilistic methods applied
to CodeLlama using these shifted code snippets. We observe that these methods
generally improve the uncertainty awareness of CodeLlama, with increased
calibration quality and higher uncertainty estimation~(UE) precision. However,
our study further reveals varied performance dynamics across different criteria
(e.g., calibration error vs misclassification detection) and trade-off between
efficacy and efficiency, highlighting necessary methodological selection
tailored to specific contexts.</div><div><a href='http://arxiv.org/abs/2402.05939v1'>2402.05939v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13802v3")'>Investigating the Efficacy of Large Language Models for Code Clone
  Detection</div>
<div id='2401.13802v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T20:43:36Z</div><div>Authors: Mohamad Khajezade, Jie JW Wu, Fatemeh Hendijani Fard, Gema Rodríguez-Pérez, Mohamed Sami Shehata</div><div style='padding-top: 10px; width: 80ex'>Large Language Models (LLMs) have demonstrated remarkable success in various
natural language processing and software engineering tasks, such as code
generation. The LLMs are mainly utilized in the prompt-based zero/few-shot
paradigm to guide the model in accomplishing the task. GPT-based models are one
of the popular ones studied for tasks such as code comment generation or test
generation. These tasks are `generative' tasks. However, there is limited
research on the usage of LLMs for `non-generative' tasks such as classification
using the prompt-based paradigm. In this preliminary exploratory study, we
investigated the applicability of LLMs for Code Clone Detection (CCD), a
non-generative task. By building a mono-lingual and cross-lingual CCD dataset
derived from CodeNet, we first investigated two different prompts using ChatGPT
to detect Type-4 code clones in Java-Java and Java-Ruby pairs in a zero-shot
setting. We then conducted an analysis to understand the strengths and
weaknesses of ChatGPT in CCD. ChatGPT surpasses the baselines in cross-language
CCD attaining an F1-score of 0.877 and achieves comparable performance to fully
fine-tuned models for mono-lingual CCD, with an F1-score of 0.878. Also, the
prompt and the difficulty level of the problems has an impact on the
performance of ChatGPT. Finally we provide insights and future directions based
on our initial analysis</div><div><a href='http://arxiv.org/abs/2401.13802v3'>2401.13802v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15100v1")'>Studying LLM Performance on Closed- and Open-source Data</div>
<div id='2402.15100v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T05:17:28Z</div><div>Authors: Toufique Ahmed, Christian Bird, Premkumar Devanbu, Saikat Chakraborty</div><div style='padding-top: 10px; width: 80ex'>Large Language models (LLMs) are finding wide use in software engineering
practice. These models are extremely data-hungry, and are largely trained on
open-source (OSS) code distributed with permissive licenses. In terms of actual
use however, a great deal of software development still occurs in the
for-profit/proprietary sphere, where the code under development is not, and
never has been, in the public domain; thus, many developers, do their work, and
use LLMs, in settings where the models may not be as familiar with the code
under development. In such settings, do LLMs work as well as they do for OSS
code? If not, what are the differences? When performance differs, what are the
possible causes, and are there work-arounds? In this paper, we examine this
issue using proprietary, closed-source software data from Microsoft, where most
proprietary code is in C# and C++. We find that performance for C# changes
little from OSS --&gt; proprietary code, but does significantly reduce for C++; we
find that this difference is attributable to differences in identifiers. We
also find that some performance degradation, in some cases, can be ameliorated
efficiently by in-context learning.</div><div><a href='http://arxiv.org/abs/2402.15100v1'>2402.15100v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.04637v1")'>Applying Large Language Models API to Issue Classification Problem</div>
<div id='2401.04637v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-09T16:05:47Z</div><div>Authors: Gabriel Aracena, Kyle Luster, Fabio Santos, Igor Steinmacher, Marco A. Gerosa</div><div style='padding-top: 10px; width: 80ex'>Effective prioritization of issue reports is crucial in software engineering
to optimize resource allocation and address critical problems promptly.
However, the manual classification of issue reports for prioritization is
laborious and lacks scalability. Alternatively, many open source software (OSS)
projects employ automated processes for this task, albeit relying on
substantial datasets for adequate training. This research seeks to devise an
automated approach that ensures reliability in issue prioritization, even when
trained on smaller datasets. Our proposed methodology harnesses the power of
Generative Pre-trained Transformers (GPT), recognizing their potential to
efficiently handle this task. By leveraging the capabilities of such models, we
aim to develop a robust system for prioritizing issue reports accurately,
mitigating the necessity for extensive training data while maintaining
reliability. In our research, we have developed a reliable GPT-based approach
to accurately label and prioritize issue reports with a reduced training
dataset. By reducing reliance on massive data requirements and focusing on
few-shot fine-tuning, our methodology offers a more accessible and efficient
solution for issue prioritization in software engineering. Our model predicted
issue types in individual projects up to 93.2% in precision, 95% in recall, and
89.3% in F1-score.</div><div><a href='http://arxiv.org/abs/2401.04637v1'>2401.04637v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04814v1")'>Evaluation of LLMs on Syntax-Aware Code Fill-in-the-Middle Tasks</div>
<div id='2403.04814v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T05:05:56Z</div><div>Authors: Linyuan Gong, Sida Wang, Mostafa Elhoushi, Alvin Cheung</div><div style='padding-top: 10px; width: 80ex'>We introduce Syntax-Aware Fill-In-the-Middle (SAFIM), a new benchmark for
evaluating Large Language Models (LLMs) on the code Fill-in-the-Middle (FIM)
task. This benchmark focuses on syntax-aware completions of program structures
such as code blocks and conditional expressions, and includes 17,720 examples
from multiple programming languages, sourced from recent code submissions after
April 2022 to minimize data contamination. SAFIM provides a robust framework
with various prompt designs and novel syntax-aware post-processing techniques,
facilitating accurate and fair comparisons across LLMs. Our comprehensive
evaluation of 15 LLMs shows that FIM pretraining not only enhances FIM
proficiency but also improves Left-to-Right (L2R) inference using LLMs. Our
findings challenge conventional beliefs and suggest that pretraining methods
and data quality have more impact than model size. SAFIM thus serves as a
foundational platform for future research in effective pretraining strategies
for code LLMs. The evaluation toolkit and dataset are available at
https://github.com/gonglinyuan/safim, and the leaderboard is available at
https://safimbenchmark.com.</div><div><a href='http://arxiv.org/abs/2403.04814v1'>2403.04814v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11702v2")'>Can ChatGPT Support Developers? An Empirical Evaluation of Large
  Language Models for Code Generation</div>
<div id='2402.11702v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T20:48:09Z</div><div>Authors: Kailun Jin, Chung-Yu Wang, Hung Viet Pham, Hadi Hemmati</div><div style='padding-top: 10px; width: 80ex'>Large language models (LLMs) have demonstrated notable proficiency in code
generation, with numerous prior studies showing their promising capabilities in
various development scenarios. However, these studies mainly provide
evaluations in research settings, which leaves a significant gap in
understanding how effectively LLMs can support developers in real-world. To
address this, we conducted an empirical analysis of conversations in DevGPT, a
dataset collected from developers' conversations with ChatGPT (captured with
the Share Link feature on platforms such as GitHub). Our empirical findings
indicate that the current practice of using LLM-generated code is typically
limited to either demonstrating high-level concepts or providing examples in
documentation, rather than to be used as production-ready code. These findings
indicate that there is much future work needed to improve LLMs in code
generation before they can be integral parts of modern software development.</div><div><a href='http://arxiv.org/abs/2402.11702v2'>2402.11702v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14852v1")'>HumanEval on Latest GPT Models -- 2024</div>
<div id='2402.14852v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T04:17:21Z</div><div>Authors: Daniel Li, Lincoln Murr</div><div style='padding-top: 10px; width: 80ex'>In 2023, we are using the latest models of GPT-4 to advance program
synthesis. The large language models have significantly improved the
state-of-the-art for this purpose. To make these advancements more accessible,
we have created a repository that connects these models to Huamn Eval. This
dataset was initally developed to be used with a language model called CODEGEN
on natural and programming language data. The utility of these trained models
is showcased by demonstrating their competitive performance in zero-shot Python
code generation on HumanEval tasks compared to previous state-of-the-art
solutions. Additionally, this gives way to developing more multi-step paradigm
synthesis. This benchmark features 160 diverse problem sets factorized into
multistep prompts that our analysis shows significantly improves program
synthesis over single-turn inputs. All code is open source at
https://github.com/daniel442li/gpt-human-eval .</div><div><a href='http://arxiv.org/abs/2402.14852v1'>2402.14852v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09126v1")'>MPIrigen: MPI Code Generation through Domain-Specific Language Models</div>
<div id='2402.09126v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T12:24:21Z</div><div>Authors: Nadav Schneider, Niranjan Hasabnis, Vy A. Vo, Tal Kadosh, Neva Krien, Mihai Capotă, Abdul Wasay, Guy Tamir, Ted Willke, Nesreen Ahmed, Yuval Pinter, Timothy Mattson, Gal Oren</div><div style='padding-top: 10px; width: 80ex'>The imperative need to scale computation across numerous nodes highlights the
significance of efficient parallel computing, particularly in the realm of
Message Passing Interface (MPI) integration. The challenging parallel
programming task of generating MPI-based parallel programs has remained
unexplored. This study first investigates the performance of state-of-the-art
language models in generating MPI-based parallel programs. Findings reveal that
widely used models such as GPT-3.5 and PolyCoder (specialized multi-lingual
code models) exhibit notable performance degradation, when generating MPI-based
programs compared to general-purpose programs. In contrast, domain-specific
models such as MonoCoder, which are pretrained on MPI-related programming
languages of C and C++, outperform larger models. Subsequently, we introduce a
dedicated downstream task of MPI-based program generation by fine-tuning
MonoCoder on HPCorpusMPI. We call the resulting model as MPIrigen. We propose
an innovative preprocessing for completion only after observing the whole code,
thus enabling better completion with a wider context. Comparative analysis
against GPT-3.5 zero-shot performance, using a novel HPC-oriented evaluation
method, demonstrates that MPIrigen excels in generating accurate MPI functions
up to 0.8 accuracy in location and function predictions, and with more than 0.9
accuracy for argument predictions. The success of this tailored solution
underscores the importance of domain-specific fine-tuning in optimizing
language models for parallel computing code generation, paving the way for a
new generation of automatic parallelization tools. The sources of this work are
available at our GitHub MPIrigen repository:
https://github.com/Scientific-Computing-Lab-NRCN/MPI-rigen</div><div><a href='http://arxiv.org/abs/2402.09126v1'>2402.09126v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.01131v2")'>LLaMoCo: Instruction Tuning of Large Language Models for Optimization
  Code Generation</div>
<div id='2403.01131v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T08:21:59Z</div><div>Authors: Zeyuan Ma, Hongshu Guo, Jiacheng Chen, Guojun Peng, Zhiguang Cao, Yining Ma, Yue-Jiao Gong</div><div style='padding-top: 10px; width: 80ex'>Recent research explores optimization using large language models (LLMs) by
either iteratively seeking next-step solutions from LLMs or directly prompting
LLMs for an optimizer. However, these approaches exhibit inherent limitations,
including low operational efficiency, high sensitivity to prompt design, and a
lack of domain-specific knowledge. We introduce LLaMoCo, the first
instruction-tuning framework designed to adapt LLMs for solving optimization
problems in a code-to-code manner. Specifically, we establish a comprehensive
instruction set containing well-described problem prompts and effective
optimization codes. We then develop a novel two-phase learning strategy that
incorporates a contrastive learning-based warm-up procedure before the
instruction-tuning phase to enhance the convergence behavior during model
fine-tuning. The experiment results demonstrate that a CodeGen (350M) model
fine-tuned by our LLaMoCo achieves superior optimization performance compared
to GPT-4 Turbo and the other competitors across both synthetic and realistic
problem sets. The fine-tuned model and the usage instructions are available at
https://anonymous.4open.science/r/LLaMoCo-722A.</div><div><a href='http://arxiv.org/abs/2403.01131v2'>2403.01131v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16823v2")'>Language Agents as Optimizable Graphs</div>
<div id='2402.16823v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T18:48:27Z</div><div>Authors: Mingchen Zhuge, Wenyi Wang, Louis Kirsch, Francesco Faccio, Dmitrii Khizbullin, Jürgen Schmidhuber</div><div style='padding-top: 10px; width: 80ex'>Various human-designed prompt engineering techniques have been proposed to
improve problem solvers based on Large Language Models (LLMs), yielding many
disparate code bases. We unify these approaches by describing LLM-based agents
as computational graphs. The nodes implement functions to process multimodal
data or query LLMs, and the edges describe the information flow between
operations. Graphs can be recursively combined into larger composite graphs
representing hierarchies of inter-agent collaboration (where edges connect
operations of different agents). Our novel automatic graph optimizers (1)
refine node-level LLM prompts (node optimization) and (2) improve agent
orchestration by changing graph connectivity (edge optimization). Experiments
demonstrate that our framework can be used to efficiently develop, integrate,
and automatically improve various LLM agents. The code can be found at
https://github.com/metauto-ai/gptswarm.</div><div><a href='http://arxiv.org/abs/2402.16823v2'>2402.16823v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.03636v1")'>SheetAgent: A Generalist Agent for Spreadsheet Reasoning and
  Manipulation via Large Language Models</div>
<div id='2403.03636v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T11:48:08Z</div><div>Authors: Yibin Chen, Yifu Yuan, Zeyu Zhang, Yan Zheng, Jinyi Liu, Fei Ni, Jianye Hao</div><div style='padding-top: 10px; width: 80ex'>Spreadsheet manipulation is widely existing in most daily works and
significantly improves working efficiency. Large language model (LLM) has been
recently attempted for automatic spreadsheet manipulation but has not yet been
investigated in complicated and realistic tasks where reasoning challenges
exist (e.g., long horizon manipulation with multi-step reasoning and ambiguous
requirements). To bridge the gap with the real-world requirements, we introduce
$\textbf{SheetRM}$, a benchmark featuring long-horizon and multi-category tasks
with reasoning-dependent manipulation caused by real-life challenges. To
mitigate the above challenges, we further propose $\textbf{SheetAgent}$, a
novel autonomous agent that utilizes the power of LLMs. SheetAgent consists of
three collaborative modules: $\textit{Planner}$, $\textit{Informer}$, and
$\textit{Retriever}$, achieving both advanced reasoning and accurate
manipulation over spreadsheets without human interaction through iterative task
reasoning and reflection. Extensive experiments demonstrate that SheetAgent
delivers 20-30% pass rate improvements on multiple benchmarks over baselines,
achieving enhanced precision in spreadsheet manipulation and demonstrating
superior table reasoning abilities. More details and visualizations are
available at https://sheetagent.github.io.</div><div><a href='http://arxiv.org/abs/2403.03636v1'>2403.03636v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.12993v1")'>An Autonomous Large Language Model Agent for Chemical Literature Data
  Mining</div>
<div id='2402.12993v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T13:21:46Z</div><div>Authors: Kexin Chen, Hanqun Cao, Junyou Li, Yuyang Du, Menghao Guo, Xin Zeng, Lanqing Li, Jiezhong Qiu, Pheng Ann Heng, Guangyong Chen</div><div style='padding-top: 10px; width: 80ex'>Chemical synthesis, which is crucial for advancing material synthesis and
drug discovery, impacts various sectors including environmental science and
healthcare. The rise of technology in chemistry has generated extensive
chemical data, challenging researchers to discern patterns and refine synthesis
processes. Artificial intelligence (AI) helps by analyzing data to optimize
synthesis and increase yields. However, AI faces challenges in processing
literature data due to the unstructured format and diverse writing style of
chemical literature. To overcome these difficulties, we introduce an end-to-end
AI agent framework capable of high-fidelity extraction from extensive chemical
literature. This AI agent employs large language models (LLMs) for prompt
generation and iterative optimization. It functions as a chemistry assistant,
automating data collection and analysis, thereby saving manpower and enhancing
performance. Our framework's efficacy is evaluated using accuracy, recall, and
F1 score of reaction condition data, and we compared our method with human
experts in terms of content correctness and time efficiency. The proposed
approach marks a significant advancement in automating chemical literature
extraction and demonstrates the potential for AI to revolutionize data
management and utilization in chemistry.</div><div><a href='http://arxiv.org/abs/2402.12993v1'>2402.12993v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02388v1")'>Solution-oriented Agent-based Models Generation with Verifier-assisted
  Iterative In-context Learning</div>
<div id='2402.02388v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T07:59:06Z</div><div>Authors: Tong Niu, Weihao Zhang, Rong Zhao</div><div style='padding-top: 10px; width: 80ex'>Agent-based models (ABMs) stand as an essential paradigm for proposing and
validating hypothetical solutions or policies aimed at addressing challenges
posed by complex systems and achieving various objectives. This process demands
labor-intensive endeavors and multidisciplinary expertise. Large language
models (LLMs) encapsulating cross-domain knowledge and programming proficiency
could potentially alleviate the difficulty of this process. However, LLMs excel
in handling sequential information, making it challenging for analyzing the
intricate interactions and nonlinear dynamics inherent in ABMs. Additionally,
due to the lack of self-evaluation capability of LLMs, relying solely on LLMs
is insufficient to effectively accomplish this process. In this paper, we
present SAGE, a general solution-oriented ABM generation framework designed for
automatic modeling and generating solutions for targeted problems. Unlike
approaches reliant on expert handcrafting or resource-intensive neural network
training, SAGE establishes a verifier-assisted iterative in-context learning
process employing large language models (LLMs) to leverages their inherent
cross-domain knowledge for tackling intricate demands from diverse domain
scenarios. In SAGE, we introduce an semi-structured conceptual representation
expliciting the intricate structures of ABMs and an objective representation to
guide LLMs in modeling scenarios and proposing hypothetical solutions through
in-context learning. To ensure the model executability and solution
feasibility, SAGE devises a two-level verifier with chain-of-thought prompting
tailored to the complex interactions and non-linear dynamics of ABMs, driving
the iterative generation optimization. Moreover, we construct an evaluation
dataset of solution-oriented ABMs from open sources.It contains practical
models across various domains.</div><div><a href='http://arxiv.org/abs/2402.02388v1'>2402.02388v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09404v1")'>AQA-Bench: An Interactive Benchmark for Evaluating LLMs' Sequential
  Reasoning Ability</div>
<div id='2402.09404v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T18:59:33Z</div><div>Authors: Siwei Yang, Bingchen Zhao, Cihang Xie</div><div style='padding-top: 10px; width: 80ex'>This paper introduces AQA-Bench, a novel benchmark to assess the sequential
reasoning capabilities of large language models (LLMs) in algorithmic contexts,
such as depth-first search (DFS). The key feature of our evaluation benchmark
lies in its interactive evaluation protocol -- for example, in DFS, the
availability of each node's connected edge is contingent upon the model's
traversal to that node, thereby necessitating the LLM's ability to effectively
remember visited nodes and strategize subsequent moves. We comprehensively
build AQA-Bench with three different algorithms, namely binary search,
depth-first search, and breadth-first search, and to evaluate the sequential
reasoning ability of 12 different LLMs. Our investigations reveal several
interesting findings: (1) Closed-source models like GPT-4 and Gemini generally
show strong sequential reasoning ability, significantly outperforming
open-source LLMs. (2) Naively providing interactive examples may inadvertently
hurt few-shot performance. (3) A very limited number of predecessor steps
following the optimal policy can substantially boost small models' performance.
(4) The scaling correlation between performance and model size is not always
significant, sometimes even showcasing an inverse trend. We hope our study can
catalyze future work on advancing the understanding and enhancement of LLMs'
capabilities in sequential reasoning. The code is available at
https://github.com/UCSC-VLAA/AQA-Bench.</div><div><a href='http://arxiv.org/abs/2402.09404v1'>2402.09404v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13178v1")'>AgentBoard: An Analytical Evaluation Board of Multi-turn LLM Agents</div>
<div id='2401.13178v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T01:51:00Z</div><div>Authors: Chang Ma, Junlei Zhang, Zhihao Zhu, Cheng Yang, Yujiu Yang, Yaohui Jin, Zhenzhong Lan, Lingpeng Kong, Junxian He</div><div style='padding-top: 10px; width: 80ex'>Evaluating large language models (LLMs) as general-purpose agents is
essential for understanding their capabilities and facilitating their
integration into practical applications. However, the evaluation process
presents substantial challenges. A primary obstacle is the benchmarking of
agent performance across diverse scenarios within a unified framework,
especially in maintaining partially-observable environments and ensuring
multi-round interactions. Moreover, current evaluation frameworks mostly focus
on the final success rate, revealing few insights during the process and
failing to provide a deep understanding of the model abilities. To address
these challenges, we introduce AgentBoard, a pioneering comprehensive benchmark
and accompanied open-source evaluation framework tailored to analytical
evaluation of LLM agents. AgentBoard offers a fine-grained progress rate metric
that captures incremental advancements as well as a comprehensive evaluation
toolkit that features easy assessment of agents for multi-faceted analysis
through interactive visualization. This not only sheds light on the
capabilities and limitations of LLM agents but also propels the
interpretability of their performance to the forefront. Ultimately, AgentBoard
serves as a significant step towards demystifying agent behaviors and
accelerating the development of stronger LLM agents.</div><div><a href='http://arxiv.org/abs/2401.13178v1'>2401.13178v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.05120v1")'>More Agents Is All You Need</div>
<div id='2402.05120v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T05:55:24Z</div><div>Authors: Junyou Li, Qin Zhang, Yangbin Yu, Qiang Fu, Deheng Ye</div><div style='padding-top: 10px; width: 80ex'>We find that, simply via a sampling-and-voting method, the performance of
large language models (LLMs) scales with the number of agents instantiated.
Also, this method is orthogonal to existing complicated methods to further
enhance LLMs, while the degree of enhancement is correlated to the task
difficulty. We conduct comprehensive experiments on a wide range of LLM
benchmarks to verify the presence of our finding, and to study the properties
that can facilitate its occurrence. Our code is publicly available at:
\url{https://anonymous.4open.science/r/more_agent_is_all_you_need}.</div><div><a href='http://arxiv.org/abs/2402.05120v1'>2402.05120v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14992v1")'>tinyBenchmarks: evaluating LLMs with fewer examples</div>
<div id='2402.14992v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T22:05:23Z</div><div>Authors: Felipe Maia Polo, Lucas Weber, Leshem Choshen, Yuekai Sun, Gongjun Xu, Mikhail Yurochkin</div><div style='padding-top: 10px; width: 80ex'>The versatility of large language models (LLMs) led to the creation of
diverse benchmarks that thoroughly test a variety of language models'
abilities. These benchmarks consist of tens of thousands of examples making
evaluation of LLMs very expensive. In this paper, we investigate strategies to
reduce the number of evaluations needed to assess the performance of an LLM on
several key benchmarks. For example, we show that to accurately estimate the
performance of an LLM on MMLU, a popular multiple-choice QA benchmark
consisting of 14K examples, it is sufficient to evaluate this LLM on 100
curated examples. We release evaluation tools and tiny versions of popular
benchmarks: Open LLM Leaderboard, MMLU, HELM, and AlpacaEval 2.0. Our empirical
analysis demonstrates that these tools and tiny benchmarks are sufficient to
reliably and efficiently reproduce the original evaluation results.</div><div><a href='http://arxiv.org/abs/2402.14992v1'>2402.14992v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01781v1")'>When Benchmarks are Targets: Revealing the Sensitivity of Large Language
  Model Leaderboards</div>
<div id='2402.01781v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T19:12:25Z</div><div>Authors: Norah Alzahrani, Hisham Abdullah Alyahya, Yazeed Alnumay, Sultan Alrashed, Shaykhah Alsubaie, Yusef Almushaykeh, Faisal Mirza, Nouf Alotaibi, Nora Altwairesh, Areeb Alowisheq, M Saiful Bari, Haidar Khan</div><div style='padding-top: 10px; width: 80ex'>Large Language Model (LLM) leaderboards based on benchmark rankings are
regularly used to guide practitioners in model selection. Often, the published
leaderboard rankings are taken at face value - we show this is a (potentially
costly) mistake. Under existing leaderboards, the relative performance of LLMs
is highly sensitive to (often minute) details. We show that for popular
multiple choice question benchmarks (e.g. MMLU) minor perturbations to the
benchmark, such as changing the order of choices or the method of answer
selection, result in changes in rankings up to 8 positions. We explain this
phenomenon by conducting systematic experiments over three broad categories of
benchmark perturbations and identifying the sources of this behavior. Our
analysis results in several best-practice recommendations, including the
advantage of a hybrid scoring method for answer selection. Our study highlights
the dangers of relying on simple benchmark evaluations and charts the path for
more robust evaluation schemes on the existing benchmarks.</div><div><a href='http://arxiv.org/abs/2402.01781v1'>2402.01781v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01830v1")'>Peer-review-in-LLMs: Automatic Evaluation Method for LLMs in
  Open-environment</div>
<div id='2402.01830v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T18:49:26Z</div><div>Authors: Kun-Peng Ning, Shuo Yang, Yu-Yang Liu, Jia-Yu Yao, Zhen-Hui Liu, Yu Wang, Ming Pang, Li Yuan</div><div style='padding-top: 10px; width: 80ex'>Existing large language models (LLMs) evaluation methods typically focus on
testing the performance on some closed-environment and domain-specific
benchmarks with human annotations. In this paper, we explore a novel
unsupervised evaluation direction, utilizing peer-review mechanisms to measure
LLMs automatically. In this setting, both open-source and closed-source LLMs
lie in the same environment, capable of answering unlabeled questions and
evaluating each other, where each LLM's response score is jointly determined by
other anonymous ones. To obtain the ability hierarchy among these models, we
assign each LLM a learnable capability parameter to adjust the final ranking.
We formalize it as a constrained optimization problem, intending to maximize
the consistency of each LLM's capabilities and scores. The key assumption
behind is that high-level LLM can evaluate others' answers more accurately than
low-level ones, while higher-level LLM can also achieve higher response scores.
Moreover, we propose three metrics called PEN, CIN, and LIS to evaluate the gap
in aligning human rankings. We perform experiments on multiple datasets with
these metrics, validating the effectiveness of the proposed approach.</div><div><a href='http://arxiv.org/abs/2402.01830v1'>2402.01830v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08078v1")'>Large Language Models as Agents in Two-Player Games</div>
<div id='2402.08078v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T21:44:32Z</div><div>Authors: Yang Liu, Peng Sun, Hang Li</div><div style='padding-top: 10px; width: 80ex'>By formally defining the training processes of large language models (LLMs),
which usually encompasses pre-training, supervised fine-tuning, and
reinforcement learning with human feedback, within a single and unified machine
learning paradigm, we can glean pivotal insights for advancing LLM
technologies. This position paper delineates the parallels between the training
methods of LLMs and the strategies employed for the development of agents in
two-player games, as studied in game theory, reinforcement learning, and
multi-agent systems. We propose a re-conceptualization of LLM learning
processes in terms of agent learning in language-based games. This framework
unveils innovative perspectives on the successes and challenges in LLM
development, offering a fresh understanding of addressing alignment issues
among other strategic considerations. Furthermore, our two-player game approach
sheds light on novel data preparation and machine learning techniques for
training LLMs.</div><div><a href='http://arxiv.org/abs/2402.08078v1'>2402.08078v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15506v3")'>AgentOhana: Design Unified Data and Training Pipeline for Effective
  Agent Learning</div>
<div id='2402.15506v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T18:56:26Z</div><div>Authors: Jianguo Zhang, Tian Lan, Rithesh Murthy, Zhiwei Liu, Weiran Yao, Juntao Tan, Thai Hoang, Liangwei Yang, Yihao Feng, Zuxin Liu, Tulika Awalgaonkar, Juan Carlos Niebles, Silvio Savarese, Shelby Heinecke, Huan Wang, Caiming Xiong</div><div style='padding-top: 10px; width: 80ex'>Autonomous agents powered by large language models (LLMs) have garnered
significant research attention. However, fully harnessing the potential of LLMs
for agent-based tasks presents inherent challenges due to the heterogeneous
nature of diverse data sources featuring multi-turn trajectories. In this
paper, we introduce \textbf{AgentOhana} as a comprehensive solution to address
these challenges. \textit{AgentOhana} aggregates agent trajectories from
distinct environments, spanning a wide array of scenarios. It meticulously
standardizes and unifies these trajectories into a consistent format,
streamlining the creation of a generic data loader optimized for agent
training. Leveraging the data unification, our training pipeline maintains
equilibrium across different data sources and preserves independent randomness
across devices during dataset partitioning and model training. Additionally, we
present \textbf{xLAM-v0.1}, a large action model tailored for AI agents, which
demonstrates exceptional performance across various benchmarks. Begin the
exploration at \url{https://github.com/SalesforceAIResearch/xLAM}.</div><div><a href='http://arxiv.org/abs/2402.15506v3'>2402.15506v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.02419v1")'>Are More LLM Calls All You Need? Towards Scaling Laws of Compound
  Inference Systems</div>
<div id='2403.02419v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-04T19:12:48Z</div><div>Authors: Lingjiao Chen, Jared Quincy Davis, Boris Hanin, Peter Bailis, Ion Stoica, Matei Zaharia, James Zou</div><div style='padding-top: 10px; width: 80ex'>Many recent state-of-the-art results in language tasks were achieved using
compound systems that perform multiple Large Language Model (LLM) calls and
aggregate their responses. However, there is little understanding of how the
number of LLM calls -- e.g., when asking the LLM to answer each question
multiple times and taking a consensus -- affects such a compound system's
performance. In this paper, we initiate the study of scaling laws of compound
inference systems. We analyze, theoretically and empirically, how the number of
LLM calls affects the performance of one-layer Voting Inference Systems -- one
of the simplest compound systems, which aggregates LLM responses via majority
voting. We find empirically that across multiple language tasks, surprisingly,
Voting Inference Systems' performance first increases but then decreases as a
function of the number of LLM calls. Our theoretical results suggest that this
non-monotonicity is due to the diversity of query difficulties within a task:
more LLM calls lead to higher performance on "easy" queries, but lower
performance on "hard" queries, and non-monotone behavior emerges when a task
contains both types of queries. This insight then allows us to compute, from a
small number of samples, the number of LLM calls that maximizes system
performance, and define a scaling law of Voting Inference Systems. Experiments
show that our scaling law can predict the performance of Voting Inference
Systems and find the optimal number of LLM calls to make.</div><div><a href='http://arxiv.org/abs/2403.02419v1'>2403.02419v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04693v1")'>Analysis of Systems' Performance in Natural Language Processing
  Competitions</div>
<div id='2403.04693v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T17:42:40Z</div><div>Authors: Sergio Nava-Muñoz, Mario Graff, Hugo Jair Escalante</div><div style='padding-top: 10px; width: 80ex'>Collaborative competitions have gained popularity in the scientific and
technological fields. These competitions involve defining tasks, selecting
evaluation scores, and devising result verification methods. In the standard
scenario, participants receive a training set and are expected to provide a
solution for a held-out dataset kept by organizers. An essential challenge for
organizers arises when comparing algorithms' performance, assessing multiple
participants, and ranking them. Statistical tools are often used for this
purpose; however, traditional statistical methods often fail to capture
decisive differences between systems' performance. This manuscript describes an
evaluation methodology for statistically analyzing competition results and
competition. The methodology is designed to be universally applicable; however,
it is illustrated using eight natural language competitions as case studies
involving classification and regression problems. The proposed methodology
offers several advantages, including off-the-shell comparisons with correction
mechanisms and the inclusion of confidence intervals. Furthermore, we introduce
metrics that allow organizers to assess the difficulty of competitions. Our
analysis shows the potential usefulness of our methodology for effectively
evaluating competition results.</div><div><a href='http://arxiv.org/abs/2403.04693v1'>2403.04693v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12094v1")'>Are LLMs Good Cryptic Crossword Solvers?</div>
<div id='2403.12094v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T06:57:08Z</div><div>Authors: Abdelrahman "Boda" Sadallah, Daria Kotova, Ekaterina Kochmar</div><div style='padding-top: 10px; width: 80ex'>Cryptic crosswords are puzzles that rely not only on general knowledge but
also on the solver's ability to manipulate language on different levels and
deal with various types of wordplay. Previous research suggests that solving
such puzzles is a challenge even for modern NLP models. However, the abilities
of large language models (LLMs) have not yet been tested on this task. In this
paper, we establish the benchmark results for three popular LLMs -- LLaMA2,
Mistral, and ChatGPT -- showing that their performance on this task is still
far from that of humans.</div><div><a href='http://arxiv.org/abs/2403.12094v1'>2403.12094v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14865v1")'>DyVal 2: Dynamic Evaluation of Large Language Models by Meta Probing
  Agents</div>
<div id='2402.14865v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T06:46:34Z</div><div>Authors: Kaijie Zhu, Jindong Wang, Qinlin Zhao, Ruochen Xu, Xing Xie</div><div style='padding-top: 10px; width: 80ex'>Evaluation of large language models (LLMs) has raised great concerns in the
community due to the issue of data contamination. Existing work designed
evaluation protocols using well-defined algorithms for specific tasks, which
cannot be easily extended to diverse scenarios. Moreover, current evaluation
benchmarks can only provide the overall benchmark results and cannot support a
fine-grained and multifaceted analysis of LLMs' abilities. In this paper, we
propose meta probing agents (MPA), a general dynamic evaluation protocol
inspired by psychometrics to evaluate LLMs. MPA is the key component of DyVal
2, which naturally extends the previous DyVal~\citep{zhu2023dyval}. MPA designs
the probing and judging agents to automatically transform an original
evaluation problem into a new one following psychometric theory on three basic
cognitive abilities: language understanding, problem solving, and domain
knowledge. These basic abilities are also dynamically configurable, allowing
multifaceted analysis. We conducted extensive evaluations using MPA and found
that most LLMs achieve poorer performance, indicating room for improvement. Our
multifaceted analysis demonstrated the strong correlation between the basic
abilities and an implicit Matthew effect on model size, i.e., larger models
possess stronger correlations of the abilities. MPA can also be used as a data
augmentation approach to enhance LLMs.</div><div><a href='http://arxiv.org/abs/2402.14865v1'>2402.14865v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01108v1")'>Reasoning Capacity in Multi-Agent Systems: Limitations, Challenges and
  Human-Centered Solutions</div>
<div id='2402.01108v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T02:53:11Z</div><div>Authors: Pouya Pezeshkpour, Eser Kandogan, Nikita Bhutani, Sajjadur Rahman, Tom Mitchell, Estevam Hruschka</div><div style='padding-top: 10px; width: 80ex'>Remarkable performance of large language models (LLMs) in a variety of tasks
brings forth many opportunities as well as challenges of utilizing them in
production settings. Towards practical adoption of LLMs, multi-agent systems
hold great promise to augment, integrate, and orchestrate LLMs in the larger
context of enterprise platforms that use existing proprietary data and models
to tackle complex real-world tasks. Despite the tremendous success of these
systems, current approaches rely on narrow, single-focus objectives for
optimization and evaluation, often overlooking potential constraints in
real-world scenarios, including restricted budgets, resources and time.
Furthermore, interpreting, analyzing, and debugging these systems requires
different components to be evaluated in relation to one another. This demand is
currently not feasible with existing methodologies. In this postion paper, we
introduce the concept of reasoning capacity as a unifying criterion to enable
integration of constraints during optimization and establish connections among
different components within the system, which also enable a more holistic and
comprehensive approach to evaluation. We present a formal definition of
reasoning capacity and illustrate its utility in identifying limitations within
each component of the system. We then argue how these limitations can be
addressed with a self-reflective process wherein human-feedback is used to
alleviate shortcomings in reasoning and enhance overall consistency of the
system.</div><div><a href='http://arxiv.org/abs/2402.01108v1'>2402.01108v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07718v1")'>WorkArena: How Capable Are Web Agents at Solving Common Knowledge Work
  Tasks?</div>
<div id='2403.07718v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T14:58:45Z</div><div>Authors: Alexandre Drouin, Maxime Gasse, Massimo Caccia, Issam H. Laradji, Manuel Del Verme, Tom Marty, Léo Boisvert, Megh Thakkar, Quentin Cappart, David Vazquez, Nicolas Chapados, Alexandre Lacoste</div><div style='padding-top: 10px; width: 80ex'>We study the use of large language model-based agents for interacting with
software via web browsers. Unlike prior work, we focus on measuring the agents'
ability to perform tasks that span the typical daily work of knowledge workers
utilizing enterprise software systems. To this end, we propose WorkArena, a
remote-hosted benchmark of 29 tasks based on the widely-used ServiceNow
platform. We also introduce BrowserGym, an environment for the design and
evaluation of such agents, offering a rich set of actions as well as multimodal
observations. Our empirical evaluation reveals that while current agents show
promise on WorkArena, there remains a considerable gap towards achieving full
task automation. Notably, our analysis uncovers a significant performance
disparity between open and closed-source LLMs, highlighting a critical area for
future exploration and development in the field.</div><div><a href='http://arxiv.org/abs/2403.07718v1'>2403.07718v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12348v1")'>GTBench: Uncovering the Strategic Reasoning Limitations of LLMs via
  Game-Theoretic Evaluations</div>
<div id='2402.12348v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T18:23:36Z</div><div>Authors: Jinhao Duan, Renming Zhang, James Diffenderfer, Bhavya Kailkhura, Lichao Sun, Elias Stengel-Eskin, Mohit Bansal, Tianlong Chen, Kaidi Xu</div><div style='padding-top: 10px; width: 80ex'>As Large Language Models (LLMs) are integrated into critical real-world
applications, their strategic and logical reasoning abilities are increasingly
crucial. This paper evaluates LLMs' reasoning abilities in competitive
environments through game-theoretic tasks, e.g., board and card games that
require pure logic and strategic reasoning to compete with opponents. We first
propose GTBench, a language-driven environment composing 10 widely-recognized
tasks, across a comprehensive game taxonomy: complete versus incomplete
information, dynamic versus static, and probabilistic versus deterministic
scenarios. Then, we investigate two key problems: (1) Characterizing
game-theoretic reasoning of LLMs; (2) LLM-vs-LLM competitions as reasoning
evaluation. We observe that (1) LLMs have distinct behaviors regarding various
gaming scenarios; for example, LLMs fail in complete and deterministic games
yet they are competitive in probabilistic gaming scenarios; (2) Open-source
LLMs, e.g., CodeLlama-34b-Instruct, are less competitive than commercial LLMs,
e.g., GPT-4, in complex games. In addition, code-pretraining greatly benefits
strategic reasoning, while advanced reasoning methods such as Chain-of-Thought
(CoT) and Tree-of-Thought (ToT) do not always help. Detailed error profiles are
also provided for a better understanding of LLMs' behavior.</div><div><a href='http://arxiv.org/abs/2402.12348v1'>2402.12348v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.12631v1")'>A Reply to Makelov et al. (2023)'s "Interpretability Illusion" Arguments</div>
<div id='2401.12631v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T10:27:42Z</div><div>Authors: Zhengxuan Wu, Atticus Geiger, Jing Huang, Aryaman Arora, Thomas Icard, Christopher Potts, Noah D. Goodman</div><div style='padding-top: 10px; width: 80ex'>We respond to the recent paper by Makelov et al. (2023), which reviews
subspace interchange intervention methods like distributed alignment search
(DAS; Geiger et al. 2023) and claims that these methods potentially cause
"interpretability illusions". We first review Makelov et al. (2023)'s technical
notion of what an "interpretability illusion" is, and then we show that even
intuitive and desirable explanations can qualify as illusions in this sense. As
a result, their method of discovering "illusions" can reject explanations they
consider "non-illusory". We then argue that the illusions Makelov et al. (2023)
see in practice are artifacts of their training and evaluation paradigms. We
close by emphasizing that, though we disagree with their core characterization,
Makelov et al. (2023)'s examples and discussion have undoubtedly pushed the
field of interpretability forward.</div><div><a href='http://arxiv.org/abs/2401.12631v1'>2401.12631v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.10890v1")'>When is Tree Search Useful for LLM Planning? It Depends on the
  Discriminator</div>
<div id='2402.10890v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T18:45:58Z</div><div>Authors: Ziru Chen, Michael White, Raymond Mooney, Ali Payani, Yu Su, Huan Sun</div><div style='padding-top: 10px; width: 80ex'>In this paper, we examine how large language models (LLMs) solve multi-step
problems under a language agent framework with three components: a generator, a
discriminator, and a planning method. We investigate the practical utility of
two advanced planning methods, iterative correction and tree search. We present
a comprehensive analysis of how discrimination accuracy affects the overall
performance of agents when using these two methods or a simpler method,
re-ranking. Experiments on two tasks, text-to-SQL parsing and mathematical
reasoning, show that: (1) advanced planning methods demand discriminators with
at least 90% accuracy to achieve significant improvements over re-ranking; (2)
current LLMs' discrimination abilities have not met the needs of advanced
planning methods to achieve such improvements; (3) with LLM-based
discriminators, advanced planning methods may not adequately balance accuracy
and efficiency. For example, compared to the other two methods, tree search is
at least 10--20 times slower but leads to negligible performance gains, which
hinders its real-world applications. Code and data will be released at
https://github.com/OSU-NLP-Group/llm-planning-eval.</div><div><a href='http://arxiv.org/abs/2402.10890v1'>2402.10890v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06326v1")'>From Instructions to Constraints: Language Model Alignment with
  Automatic Constraint Verification</div>
<div id='2403.06326v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-10T22:14:54Z</div><div>Authors: Fei Wang, Chao Shang, Sarthak Jain, Shuai Wang, Qiang Ning, Bonan Min, Vittorio Castelli, Yassine Benajiba, Dan Roth</div><div style='padding-top: 10px; width: 80ex'>User alignment is crucial for adapting general-purpose language models (LMs)
to downstream tasks, but human annotations are often not available for all
types of instructions, especially those with customized constraints. We observe
that user instructions typically contain constraints. While assessing response
quality in terms of the whole instruction is often costly, efficiently
evaluating the satisfaction rate of constraints is feasible. We investigate
common constraints in NLP tasks, categorize them into three classes based on
the types of their arguments, and propose a unified framework, ACT (Aligning to
ConsTraints), to automatically produce supervision signals for user alignment
with constraints. Specifically, ACT uses constraint verifiers, which are
typically easy to implement in practice, to compute constraint satisfaction
rate (CSR) of each response. It samples multiple responses for each prompt and
collect preference labels based on their CSR automatically. Subsequently, ACT
adapts the LM to the target task through a ranking-based learning process.
Experiments on fine-grained entity typing, abstractive summarization, and
temporal question answering show that ACT is able to enhance LMs' capability to
adhere to different classes of constraints, thereby improving task performance.
Further experiments show that the constraint-following capabilities are
transferable.</div><div><a href='http://arxiv.org/abs/2403.06326v1'>2403.06326v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13212v1")'>Soft Self-Consistency Improves Language Model Agents</div>
<div id='2402.13212v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T18:22:38Z</div><div>Authors: Han Wang, Archiki Prasad, Elias Stengel-Eskin, Mohit Bansal</div><div style='padding-top: 10px; width: 80ex'>Generations from large language models (LLMs) can be improved by sampling and
scoring multiple solutions to select a final answer. Current "sample and
select" methods such as self-consistency (SC) rely on majority voting to score
answers. However, when tasks have many distinct and valid answers, selection by
voting requires a large number of samples. This makes SC prohibitively
expensive for interactive tasks that involve generating multiple actions
(answers) sequentially. After establishing that majority voting fails to
provide consistent gains on such tasks, we demonstrate how to increase success
rates by softening the scoring criterion. We introduce Soft Self-Consistency
(Soft-SC), which replaces SC's discontinuous scoring with a continuous score
computed from model likelihoods, allowing for selection even when actions are
sparsely distributed. Soft-SC improves both performance and efficiency on
long-horizon interactive tasks, requiring half as many samples as SC for
comparable or better performance. For a fixed number of samples, Soft-SC leads
to a 1.3% increase over SC in absolute success rate on writing bash programs, a
6.6% increase on online shopping (WebShop), and a 4.7% increase for an
interactive household game (ALFWorld). Finally, we show that Soft-SC can be
applied to both open-source and black-box models.</div><div><a href='http://arxiv.org/abs/2402.13212v1'>2402.13212v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07757v1")'>Towards an Understanding of Stepwise Inference in Transformers: A
  Synthetic Graph Navigation Model</div>
<div id='2402.07757v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T16:25:47Z</div><div>Authors: Mikail Khona, Maya Okawa, Jan Hula, Rahul Ramesh, Kento Nishi, Robert Dick, Ekdeep Singh Lubana, Hidenori Tanaka</div><div style='padding-top: 10px; width: 80ex'>Stepwise inference protocols, such as scratchpads and chain-of-thought, help
language models solve complex problems by decomposing them into a sequence of
simpler subproblems. Despite the significant gain in performance achieved via
these protocols, the underlying mechanisms of stepwise inference have remained
elusive. To address this, we propose to study autoregressive Transformer models
on a synthetic task that embodies the multi-step nature of problems where
stepwise inference is generally most useful. Specifically, we define a graph
navigation problem wherein a model is tasked with traversing a path from a
start to a goal node on the graph. Despite is simplicity, we find we can
empirically reproduce and analyze several phenomena observed at scale: (i) the
stepwise inference reasoning gap, the cause of which we find in the structure
of the training data; (ii) a diversity-accuracy tradeoff in model generations
as sampling temperature varies; (iii) a simplicity bias in the model's output;
and (iv) compositional generalization and a primacy bias with in-context
exemplars. Overall, our work introduces a grounded, synthetic framework for
studying stepwise inference and offers mechanistic hypotheses that can lay the
foundation for a deeper understanding of this phenomenon.</div><div><a href='http://arxiv.org/abs/2402.07757v1'>2402.07757v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13787v1")'>RewardBench: Evaluating Reward Models for Language Modeling</div>
<div id='2403.13787v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T17:49:54Z</div><div>Authors: Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, Noah A. Smith, Hannaneh Hajishirzi</div><div style='padding-top: 10px; width: 80ex'>Reward models (RMs) are at the crux of successful RLHF to align pretrained
models to human preferences, yet there has been relatively little study that
focuses on evaluation of those reward models. Evaluating reward models presents
an opportunity to understand the opaque technologies used for alignment of
language models and which values are embedded in them. To date, very few
descriptors of capabilities, training methods, or open-source reward models
exist. In this paper, we present RewardBench, a benchmark dataset and code-base
for evaluation, to enhance scientific understanding of reward models. The
RewardBench dataset is a collection of prompt-win-lose trios spanning chat,
reasoning, and safety, to benchmark how reward models perform on challenging,
structured and out-of-distribution queries. We created specific comparison
datasets for RMs that have subtle, but verifiable reasons (e.g. bugs, incorrect
facts) why one answer should be preferred to another. On the RewardBench
leaderboard, we evaluate reward models trained with a variety of methods, such
as the direct MLE training of classifiers and the implicit reward modeling of
Direct Preference Optimization (DPO), and on a spectrum of datasets. We present
many findings on propensity for refusals, reasoning limitations, and
instruction following shortcomings of various reward models towards a better
understanding of the RLHF process.</div><div><a href='http://arxiv.org/abs/2403.13787v1'>2403.13787v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.04536v2")'>Evaluating Language Model Agency through Negotiations</div>
<div id='2401.04536v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-09T13:19:37Z</div><div>Authors: Tim R. Davidson, Veniamin Veselovsky, Martin Josifoski, Maxime Peyrard, Antoine Bosselut, Michal Kosinski, Robert West</div><div style='padding-top: 10px; width: 80ex'>We introduce an approach to evaluate language model (LM) agency using
negotiation games. This approach better reflects real-world use cases and
addresses some of the shortcomings of alternative LM benchmarks. Negotiation
games enable us to study multi-turn, and cross-model interactions, modulate
complexity, and side-step accidental evaluation data leakage. We use our
approach to test six widely used and publicly accessible LMs, evaluating
performance and alignment in both self-play and cross-play settings. Noteworthy
findings include: (i) only closed-source models tested here were able to
complete these tasks; (ii) cooperative bargaining games proved to be most
challenging to the models; and (iii) even the most powerful models sometimes
"lose" to weaker opponents</div><div><a href='http://arxiv.org/abs/2401.04536v2'>2401.04536v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12031v1")'>ROUTERBENCH: A Benchmark for Multi-LLM Routing System</div>
<div id='2403.12031v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T17:59:04Z</div><div>Authors: Qitian Jason Hu, Jacob Bieker, Xiuyu Li, Nan Jiang, Benjamin Keigwin, Gaurav Ranganath, Kurt Keutzer, Shriyash Kaustubh Upadhyay</div><div style='padding-top: 10px; width: 80ex'>As the range of applications for Large Language Models (LLMs) continues to
grow, the demand for effective serving solutions becomes increasingly critical.
Despite the versatility of LLMs, no single model can optimally address all
tasks and applications, particularly when balancing performance with cost. This
limitation has led to the development of LLM routing systems, which combine the
strengths of various models to overcome the constraints of individual LLMs.
Yet, the absence of a standardized benchmark for evaluating the performance of
LLM routers hinders progress in this area. To bridge this gap, we present
ROUTERBENCH, a novel evaluation framework designed to systematically assess the
efficacy of LLM routing systems, along with a comprehensive dataset comprising
over 405k inference outcomes from representative LLMs to support the
development of routing strategies. We further propose a theoretical framework
for LLM routing, and deliver a comparative analysis of various routing
approaches through ROUTERBENCH, highlighting their potentials and limitations
within our evaluation framework. This work not only formalizes and advances the
development of LLM routing systems but also sets a standard for their
assessment, paving the way for more accessible and economically viable LLM
deployments. The code and data are available at
https://github.com/withmartian/routerbench.</div><div><a href='http://arxiv.org/abs/2403.12031v1'>2403.12031v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.00625v2")'>Beyond Efficiency: A Systematic Survey of Resource-Efficient Large
  Language Models</div>
<div id='2401.00625v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-01T01:12:42Z</div><div>Authors: Guangji Bai, Zheng Chai, Chen Ling, Shiyu Wang, Jiaying Lu, Nan Zhang, Tingwei Shi, Ziyang Yu, Mengdan Zhu, Yifei Zhang, Carl Yang, Yue Cheng, Liang Zhao</div><div style='padding-top: 10px; width: 80ex'>The burgeoning field of Large Language Models (LLMs), exemplified by
sophisticated models like OpenAI's ChatGPT, represents a significant
advancement in artificial intelligence. These models, however, bring forth
substantial challenges in the high consumption of computational, memory,
energy, and financial resources, especially in environments with limited
resource capabilities. This survey aims to systematically address these
challenges by reviewing a broad spectrum of techniques designed to enhance the
resource efficiency of LLMs. We categorize methods based on their optimization
focus: computational, memory, energy, financial, and network resources and
their applicability across various stages of an LLM's lifecycle, including
architecture design, pretraining, finetuning, and system design. Additionally,
the survey introduces a nuanced categorization of resource efficiency
techniques by their specific resource types, which uncovers the intricate
relationships and mappings between various resources and corresponding
optimization techniques. A standardized set of evaluation metrics and datasets
is also presented to facilitate consistent and fair comparisons across
different models and techniques. By offering a comprehensive overview of the
current sota and identifying open research avenues, this survey serves as a
foundational reference for researchers and practitioners, aiding them in
developing more sustainable and efficient LLMs in a rapidly evolving landscape.</div><div><a href='http://arxiv.org/abs/2401.00625v2'>2401.00625v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08908v1")'>Herding LLaMaS: Using LLMs as an OS Module</div>
<div id='2401.08908v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T01:32:45Z</div><div>Authors: Aditya K Kamath, Sujay Yadalam</div><div style='padding-top: 10px; width: 80ex'>Computer systems are becoming increasingly heterogeneous with the emergence
of new memory technologies and compute devices. GPUs alongside CPUs have become
commonplace and CXL is poised to be a mainstay of cloud systems. The operating
system is responsible for managing these hardware resources, requiring
modification every time a new device is released. Years of research and
development are sunk into tuning the OS for high performance with each new
heterogeneous device. With the recent explosion in memory technologies and
domain-specific accelerators, it would be beneficial to have an OS that could
provide high performance for new devices without significant effort.
  We propose LLaMaS which can adapt to new devices easily. LLaMaS uses Large
Language Models (LLMs) to extract the useful features of new devices from their
textual description and uses these features to make operating system decisions
at runtime. Adding support to LLaMaS for a new device is as simple as
describing the system and new device properties in plaintext.
  LLaMaS reduces the burden on system administrators to enable easy integration
of new devices into production systems.
  Preliminary evaluation using ChatGPT shows that LLMs are capable of
extracting device features from text and make correct OS decisions based on
those features.</div><div><a href='http://arxiv.org/abs/2401.08908v1'>2401.08908v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.13089v1")'>Towards an empirical understanding of MoE design choices</div>
<div id='2402.13089v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T15:31:44Z</div><div>Authors: Dongyang Fan, Bettina Messmer, Martin Jaggi</div><div style='padding-top: 10px; width: 80ex'>In this study, we systematically evaluate the impact of common design choices
in Mixture of Experts (MoEs) on validation performance, uncovering distinct
influences at token and sequence levels. We also present empirical evidence
showing comparable performance between a learned router and a frozen, randomly
initialized router, suggesting that learned routing may not be essential. Our
study further reveals that Sequence-level routing can result in topic-specific
weak expert specialization, in contrast to syntax specialization observed with
Token-level routing.</div><div><a href='http://arxiv.org/abs/2402.13089v1'>2402.13089v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.00854v2")'>SymbolicAI: A framework for logic-based approaches combining generative
  models and solvers</div>
<div id='2402.00854v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T18:50:50Z</div><div>Authors: Marius-Constantin Dinu, Claudiu Leoveanu-Condrei, Markus Holzleitner, Werner Zellinger, Sepp Hochreiter</div><div style='padding-top: 10px; width: 80ex'>We introduce SymbolicAI, a versatile and modular framework employing a
logic-based approach to concept learning and flow management in generative
processes. SymbolicAI enables the seamless integration of generative models
with a diverse range of solvers by treating large language models (LLMs) as
semantic parsers that execute tasks based on both natural and formal language
instructions, thus bridging the gap between symbolic reasoning and generative
AI. We leverage probabilistic programming principles to tackle complex tasks,
and utilize differentiable and classical programming paradigms with their
respective strengths. The framework introduces a set of polymorphic,
compositional, and self-referential operations for data stream manipulation,
aligning LLM outputs with user objectives. As a result, we can transition
between the capabilities of various foundation models endowed with zero- and
few-shot learning capabilities and specialized, fine-tuned models or solvers
proficient in addressing specific problems. In turn, the framework facilitates
the creation and evaluation of explainable computational graphs. We conclude by
introducing a quality measure and its empirical score for evaluating these
computational graphs, and propose a benchmark that compares various
state-of-the-art LLMs across a set of complex workflows. We refer to the
empirical score as the "Vector Embedding for Relational Trajectory Evaluation
through Cross-similarity", or VERTEX score for short. The framework codebase
and benchmark are linked below.</div><div><a href='http://arxiv.org/abs/2402.00854v2'>2402.00854v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03391v2")'>CoRMF: Criticality-Ordered Recurrent Mean Field Ising Solver</div>
<div id='2403.03391v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T16:55:06Z</div><div>Authors: Zhenyu Pan, Ammar Gilani, En-Jui Kuo, Zhuo Liu</div><div style='padding-top: 10px; width: 80ex'>We propose an RNN-based efficient Ising model solver, the Criticality-ordered
Recurrent Mean Field (CoRMF), for forward Ising problems. In its core, a
criticality-ordered spin sequence of an $N$-spin Ising model is introduced by
sorting mission-critical edges with greedy algorithm, such that an
autoregressive mean-field factorization can be utilized and optimized with
Recurrent Neural Networks (RNNs). Our method has two notable characteristics:
(i) by leveraging the approximated tree structure of the underlying Ising
graph, the newly-obtained criticality order enables the unification between
variational mean-field and RNN, allowing the generally intractable Ising model
to be efficiently probed with probabilistic inference; (ii) it is
well-modulized, model-independent while at the same time expressive enough, and
hence fully applicable to any forward Ising inference problems with minimal
effort. Computationally, by using a variance-reduced Monte Carlo gradient
estimator, CoRFM solves the Ising problems in a self-train fashion without
data/evidence, and the inference tasks can be executed by directly sampling
from RNN. Theoretically, we establish a provably tighter error bound than naive
mean-field by using the matrix cut decomposition machineries. Numerically, we
demonstrate the utility of this framework on a series of Ising datasets.</div><div><a href='http://arxiv.org/abs/2403.03391v2'>2403.03391v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14029v1")'>Partial Search in a Frozen Network is Enough to Find a Strong Lottery
  Ticket</div>
<div id='2402.14029v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T03:14:45Z</div><div>Authors: Hikari Otsuka, Daiki Chijiwa, Ángel López García-Arias, Yasuyuki Okoshi, Kazushi Kawamura, Thiem Van Chu, Daichi Fujiki, Susumu Takeuchi, Masato Motomura</div><div style='padding-top: 10px; width: 80ex'>Randomly initialized dense networks contain subnetworks that achieve high
accuracy without weight learning -- strong lottery tickets (SLTs). Recently,
Gadhikar et al. (2023) demonstrated theoretically and experimentally that SLTs
can also be found within a randomly pruned source network, thus reducing the
SLT search space. However, this limits the search to SLTs that are even sparser
than the source, leading to worse accuracy due to unintentionally high
sparsity. This paper proposes a method that reduces the SLT search space by an
arbitrary ratio that is independent of the desired SLT sparsity. A random
subset of the initial weights is excluded from the search space by freezing it
-- i.e., by either permanently pruning them or locking them as a fixed part of
the SLT. Indeed, the SLT existence in such a reduced search space is
theoretically guaranteed by our subset-sum approximation with randomly frozen
variables. In addition to reducing search space, the random freezing pattern
can also be exploited to reduce model size in inference. Furthermore,
experimental results show that the proposed method finds SLTs with better
accuracy and model size trade-off than the SLTs obtained from dense or randomly
pruned source networks. In particular, the SLT found in a frozen graph neural
network achieves higher accuracy than its weight trained counterpart while
reducing model size by $40.3\times$.</div><div><a href='http://arxiv.org/abs/2402.14029v1'>2402.14029v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04861v2")'>A Survey of Lottery Ticket Hypothesis</div>
<div id='2403.04861v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T19:27:01Z</div><div>Authors: Bohan Liu, Zijie Zhang, Peixiong He, Zhensen Wang, Yang Xiao, Ruimeng Ye, Yang Zhou, Wei-Shinn Ku, Bo Hui</div><div style='padding-top: 10px; width: 80ex'>The Lottery Ticket Hypothesis (LTH) states that a dense neural network model
contains a highly sparse subnetwork (i.e., winning tickets) that can achieve
even better performance than the original model when trained in isolation.
While LTH has been proved both empirically and theoretically in many works,
there still are some open issues, such as efficiency and scalability, to be
addressed. Also, the lack of open-source frameworks and consensual experimental
setting poses a challenge to future research on LTH. We, for the first time,
examine previous research and studies on LTH from different perspectives. We
also discuss issues in existing works and list potential directions for further
exploration. This survey aims to provide an in-depth look at the state of LTH
and develop a duly maintained platform to conduct experiments and compare with
the most updated baselines.</div><div><a href='http://arxiv.org/abs/2403.04861v2'>2403.04861v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.14236v1")'>A Unified Framework for Model Editing</div>
<div id='2403.14236v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T08:54:24Z</div><div>Authors: Akshat Gupta, Dev Sajnani, Gopala Anumanchipalli</div><div style='padding-top: 10px; width: 80ex'>Model editing is a growing area focused on updating the knowledge embedded
within models. Among the various methodologies, ROME and MEMIT stand out as
leading "locate-and-edit" model editing techniques. While MEMIT enables batched
editing of memories, ROME is limited to changing one fact at a time. This paper
introduces a unifying framework that brings ROME and MEMIT under a single
conceptual umbrella, optimizing for the same goal, which we call the
"preservation-memorization" objective. This objective aims to preserve the
representations of certain selected vectors while memorizing the
representations of new factual information. Specifically, ROME optimizes this
objective using an equality constraint, whereas MEMIT employs a more flexible
least-square constraint. In addition to making batched edits, MEMIT also edits
the model at multiple layers. We disentangle the distribution of edits to
multiple layers from the optimization objective of MEMIT and show that these
edit-distribution algorithms should be considered separate entities worthy of
their own line of research.
  Finally, we present EMMET - an Equality-constrained Mass Model Editing
algorithm for Transformers, a new batched memory-editing algorithm. With EMMET,
we present a closed form solution for the equality-constrained version of the
preservation-memorization objective. We show that EMMET is able to perform
batched-edits on par with MEMIT up to a batch-size of 256 and discuss the
challenges in stabilizing EMMET. By articulating the "locate-and-edit" model
editing algorithms under a simple conceptual framework of
"preservation-memorization", we aim to bridge the gap between intuition and
mathematics and hope to simplify the journey for future researchers in model
editing.</div><div><a href='http://arxiv.org/abs/2403.14236v1'>2403.14236v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.15000v1")'>Divide-or-Conquer? Which Part Should You Distill Your LLM?</div>
<div id='2402.15000v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T22:28:46Z</div><div>Authors: Zhuofeng Wu, He Bai, Aonan Zhang, Jiatao Gu, VG Vinod Vydiswaran, Navdeep Jaitly, Yizhe Zhang</div><div style='padding-top: 10px; width: 80ex'>Recent methods have demonstrated that Large Language Models (LLMs) can solve
reasoning tasks better when they are encouraged to solve subtasks of the main
task first. In this paper we devise a similar strategy that breaks down
reasoning tasks into a problem decomposition phase and a problem solving phase
and show that the strategy is able to outperform a single stage solution.
Further, we hypothesize that the decomposition should be easier to distill into
a smaller model compared to the problem solving because the latter requires
large amounts of domain knowledge while the former only requires learning
general problem solving strategies. We propose methods to distill these two
capabilities and evaluate their impact on reasoning outcomes and inference
cost. We find that we can distill the problem decomposition phase and at the
same time achieve good generalization across tasks, datasets, and models.
However, it is harder to distill the problem solving capability without losing
performance and the resulting distilled model struggles with generalization.
These results indicate that by using smaller, distilled problem decomposition
models in combination with problem solving LLMs we can achieve reasoning with
cost-efficient inference and local adaptation.</div><div><a href='http://arxiv.org/abs/2402.15000v1'>2402.15000v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03268v2")'>Understanding the Reasoning Ability of Language Models From the
  Perspective of Reasoning Paths Aggregation</div>
<div id='2402.03268v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T18:25:51Z</div><div>Authors: Xinyi Wang, Alfonso Amayuelas, Kexun Zhang, Liangming Pan, Wenhu Chen, William Yang Wang</div><div style='padding-top: 10px; width: 80ex'>Pre-trained language models (LMs) are able to perform complex reasoning
without explicit fine-tuning. To understand how pre-training with a next-token
prediction objective contributes to the emergence of such reasoning capability,
we propose that we can view an LM as deriving new conclusions by aggregating
indirect reasoning paths seen at pre-training time. We found this perspective
effective in two important cases of reasoning: logic reasoning with knowledge
graphs (KGs) and math reasoning with math word problems (MWPs). More
specifically, we formalize the reasoning paths as random walk paths on the
knowledge/reasoning graphs. Analyses of learned LM distributions suggest that a
weighted sum of relevant random walk path probabilities is a reasonable way to
explain how LMs reason. Experiments and analysis on multiple KG and MWP
datasets reveal the effect of training on random walk paths and suggest that
augmenting unlabeled random walk reasoning paths can improve real-world
multi-step reasoning performance. code:
https://github.com/WANGXinyiLinda/LM_random_walk</div><div><a href='http://arxiv.org/abs/2402.03268v2'>2402.03268v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11140v1")'>Boosting of Thoughts: Trial-and-Error Problem Solving with Large
  Language Models</div>
<div id='2402.11140v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-17T00:13:36Z</div><div>Authors: Sijia Chen, Baochun Li, Di Niu</div><div style='padding-top: 10px; width: 80ex'>The reasoning performance of Large Language Models (LLMs) on a wide range of
problems critically relies on chain-of-thought prompting, which involves
providing a few chain of thought demonstrations as exemplars in prompts. Recent
work, e.g., Tree of Thoughts, has pointed out the importance of exploration and
self-evaluation in reasoning step selection for complex problem solving. In
this paper, we present Boosting of Thoughts (BoT), an automated prompting
framework for problem solving with LLMs by iteratively exploring and
self-evaluating many trees of thoughts in order to acquire an ensemble of
trial-and-error reasoning experiences, which will serve as a new form of
prompting to solve the complex problem. Starting from a simple prompt without
requiring examples, BoT iteratively explores and evaluates a large collection
of reasoning steps, and more importantly, uses error analysis obtained from the
LLM on them to explicitly revise prompting, which in turn enhances reasoning
step generation, until a final answer is attained. Our experiments with GPT-4
and Llama2 across extensive complex mathematical problems demonstrate that BoT
consistently achieves higher or comparable problem-solving rates than other
advanced prompting approaches.</div><div><a href='http://arxiv.org/abs/2402.11140v1'>2402.11140v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14295v1")'>Topologies of Reasoning: Demystifying Chains, Trees, and Graphs of
  Thoughts</div>
<div id='2401.14295v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T16:34:00Z</div><div>Authors: Maciej Besta, Florim Memedi, Zhenyu Zhang, Robert Gerstenberger, Nils Blach, Piotr Nyczyk, Marcin Copik, Grzegorz Kwaśniewski, Jürgen Müller, Lukas Gianinazzi, Ales Kubicek, Hubert Niewiadomski, Onur Mutlu, Torsten Hoefler</div><div style='padding-top: 10px; width: 80ex'>The field of natural language processing (NLP) has witnessed significant
progress in recent years, with a notable focus on improving large language
models' (LLM) performance through innovative prompting techniques. Among these,
prompt engineering coupled with structures has emerged as a promising paradigm,
with designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts,
in which the overall LLM reasoning is guided by a structure such as a graph. As
illustrated with numerous examples, this paradigm significantly enhances the
LLM's capability to solve numerous tasks, ranging from logical or mathematical
reasoning to planning or creative writing. To facilitate the understanding of
this growing field and pave the way for future developments, we devise a
general blueprint for effective and efficient LLM reasoning schemes. For this,
we conduct an in-depth analysis of the prompt execution pipeline, clarifying
and clearly defining different concepts. We then build the first taxonomy of
structure-enhanced LLM reasoning schemes. We focus on identifying fundamental
classes of harnessed structures, and we analyze the representations of these
structures, algorithms executed with these structures, and many others. We
refer to these structures as reasoning topologies, because their representation
becomes to a degree spatial, as they are contained within the LLM context. Our
study compares existing prompting schemes using the proposed taxonomy,
discussing how certain design choices lead to different patterns in performance
and cost. We also outline theoretical underpinnings, relationships between
prompting and others parts of the LLM ecosystem such as knowledge bases, and
the associated research challenges. Our work will help to advance future prompt
engineering techniques.</div><div><a href='http://arxiv.org/abs/2401.14295v1'>2401.14295v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12576v1")'>LLMCheckup: Conversational Examination of Large Language Models via
  Interpretability Tools</div>
<div id='2401.12576v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T09:11:07Z</div><div>Authors: Qianli Wang, Tatiana Anikina, Nils Feldhus, Josef van Genabith, Leonhard Hennig, Sebastian Möller</div><div style='padding-top: 10px; width: 80ex'>Interpretability tools that offer explanations in the form of a dialogue have
demonstrated their efficacy in enhancing users' understanding, as one-off
explanations may occasionally fall short in providing sufficient information to
the user. Current solutions for dialogue-based explanations, however, require
many dependencies and are not easily transferable to tasks they were not
designed for. With LLMCheckup, we present an easily accessible tool that allows
users to chat with any state-of-the-art large language model (LLM) about its
behavior. We enable LLMs to generate all explanations by themselves and take
care of intent recognition without fine-tuning, by connecting them with a broad
spectrum of Explainable AI (XAI) tools, e.g. feature attributions,
embedding-based similarity, and prompting strategies for counterfactual and
rationale generation. LLM (self-)explanations are presented as an interactive
dialogue that supports follow-up questions and generates suggestions.
LLMCheckup provides tutorials for operations available in the system, catering
to individuals with varying levels of expertise in XAI and supports multiple
input modalities. We introduce a new parsing strategy called multi-prompt
parsing substantially enhancing the parsing accuracy of LLMs. Finally, we
showcase the tasks of fact checking and commonsense question answering.</div><div><a href='http://arxiv.org/abs/2401.12576v1'>2401.12576v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03271v1")'>Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information
  Seeking in Large Language Models</div>
<div id='2402.03271v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T18:28:44Z</div><div>Authors: Zhiyuan Hu, Chumin Liu, Xidong Feng, Yilun Zhao, See-Kiong Ng, Anh Tuan Luu, Junxian He, Pang Wei Koh, Bryan Hooi</div><div style='padding-top: 10px; width: 80ex'>In the face of uncertainty, the ability to seek information is of fundamental
importance. In many practical applications, such as medical diagnosis and
troubleshooting, the information needed to solve the task is not initially
given, and has to be actively sought by asking follow-up questions (for
example, a doctor asking a patient for more details about their symptoms). In
this work, we introduce Uncertainty of Thoughts (UoT), an algorithm to augment
large language models with the ability to actively seek information by asking
effective questions. UoT combines 1) an uncertainty-aware simulation approach
which enables the model to simulate possible future scenarios and how likely
they are to occur, 2) uncertainty-based rewards motivated by information gain
which incentivizes the model to seek information, and 3) a reward propagation
scheme to select the optimal question to ask in a way that maximizes the
expected reward. In experiments on medical diagnosis, troubleshooting and the
'20 Questions' game, UoT achieves an average performance improvement of 57.8%
in the rate of successful task completion across multiple LLMs compared with
direct prompting, and also improves efficiency (i.e., the number of questions
needed to complete the task).</div><div><a href='http://arxiv.org/abs/2402.03271v1'>2402.03271v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07812v1")'>Retrieval-Augmented Thought Process as Sequential Decision Making</div>
<div id='2402.07812v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T17:17:50Z</div><div>Authors: Thomas Pouplin, Hao Sun, Samuel Holt, Mihaela van der Schaar</div><div style='padding-top: 10px; width: 80ex'>Large Language Models (LLMs) have demonstrated their strong ability to assist
people and show "sparks of intelligence". However, several open challenges
hinder their wider application: such as concerns over privacy, tendencies to
produce hallucinations, and difficulties in handling long contexts. In this
work, we address those challenges by introducing the Retrieval-Augmented
Thought Process (RATP). Given access to external knowledge, RATP formulates the
thought generation of LLMs as a multiple-step decision process. To optimize
such a thought process, RATP leverages Monte-Carlo Tree Search, and learns a
Q-value estimator that permits cost-efficient inference. In addressing the task
of question-answering with private data, where ethical and security concerns
limit LLM training methods, RATP achieves a 50% improvement over existing
in-context retrieval-augmented language models.</div><div><a href='http://arxiv.org/abs/2402.07812v1'>2402.07812v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07955v1")'>A Study on Large Language Models' Limitations in Multiple-Choice
  Question Answering</div>
<div id='2401.07955v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T20:42:16Z</div><div>Authors: Aisha Khatun, Daniel G. Brown</div><div style='padding-top: 10px; width: 80ex'>The widespread adoption of Large Language Models (LLMs) has become
commonplace, particularly with the emergence of open-source models. More
importantly, smaller models are well-suited for integration into consumer
devices and are frequently employed either as standalone solutions or as
subroutines in various AI tasks. Despite their ubiquitous use, there is no
systematic analysis of their specific capabilities and limitations. In this
study, we tackle one of the most widely used tasks - answering Multiple Choice
Question (MCQ). We analyze 26 small open-source models and find that 65% of the
models do not understand the task, only 4 models properly select an answer from
the given choices, and only 5 of these models are choice order independent.
These results are rather alarming given the extensive use of MCQ tests with
these models. We recommend exercising caution and testing task understanding
before using MCQ to evaluate LLMs in any field whatsoever.</div><div><a href='http://arxiv.org/abs/2401.07955v1'>2401.07955v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00251v1")'>Efficient Non-Parametric Uncertainty Quantification for Black-Box Large
  Language Models and Decision Planning</div>
<div id='2402.00251v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T00:23:31Z</div><div>Authors: Yao-Hung Hubert Tsai, Walter Talbott, Jian Zhang</div><div style='padding-top: 10px; width: 80ex'>Step-by-step decision planning with large language models (LLMs) is gaining
attention in AI agent development. This paper focuses on decision planning with
uncertainty estimation to address the hallucination problem in language models.
Existing approaches are either white-box or computationally demanding, limiting
use of black-box proprietary LLMs within budgets. The paper's first
contribution is a non-parametric uncertainty quantification method for LLMs,
efficiently estimating point-wise dependencies between input-decision on the
fly with a single inference, without access to token logits. This estimator
informs the statistical interpretation of decision trustworthiness. The second
contribution outlines a systematic design for a decision-making agent,
generating actions like ``turn on the bathroom light'' based on user prompts
such as ``take a bath''. Users will be asked to provide preferences when more
than one action has high estimated point-wise dependencies. In conclusion, our
uncertainty estimation and decision-making agent design offer a cost-efficient
approach for AI agent development.</div><div><a href='http://arxiv.org/abs/2402.00251v1'>2402.00251v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14807v2")'>A Decision-Language Model (DLM) for Dynamic Restless Multi-Armed Bandit
  Tasks in Public Health</div>
<div id='2402.14807v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T18:58:27Z</div><div>Authors: Nikhil Behari, Edwin Zhang, Yunfan Zhao, Aparna Taneja, Dheeraj Nagaraj, Milind Tambe</div><div style='padding-top: 10px; width: 80ex'>Efforts to reduce maternal mortality rate, a key UN Sustainable Development
target (SDG Target 3.1), rely largely on preventative care programs to spread
critical health information to high-risk populations. These programs face two
important challenges: efficiently allocating limited health resources to large
beneficiary populations, and adapting to evolving policy priorities. While
prior works in restless multi-armed bandit (RMAB) demonstrated success in
public health allocation tasks, they lack flexibility to adapt to evolving
policy priorities. Concurrently, Large Language Models (LLMs) have emerged as
adept, automated planners in various domains, including robotic control and
navigation. In this paper, we propose DLM: a Decision Language Model for RMABs.
To enable dynamic fine-tuning of RMAB policies for challenging public health
settings using human-language commands, we propose using LLMs as automated
planners to (1) interpret human policy preference prompts, (2) propose code
reward functions for a multi-agent RL environment for RMABs, and (3) iterate on
the generated reward using feedback from RMAB simulations to effectively adapt
policy outcomes. In collaboration with ARMMAN, an India-based public health
organization promoting preventative care for pregnant mothers, we conduct a
simulation study, showing DLM can dynamically shape policy outcomes using only
human language commands as input.</div><div><a href='http://arxiv.org/abs/2402.14807v2'>2402.14807v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.04758v1")'>KnowledgeVIS: Interpreting Language Models by Comparing
  Fill-in-the-Blank Prompts</div>
<div id='2403.04758v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T18:56:31Z</div><div>Authors: Adam Coscia, Alex Endert</div><div style='padding-top: 10px; width: 80ex'>Recent growth in the popularity of large language models has led to their
increased usage for summarizing, predicting, and generating text, making it
vital to help researchers and engineers understand how and why they work. We
present KnowledgeVis, a human-in-the-loop visual analytics system for
interpreting language models using fill-in-the-blank sentences as prompts. By
comparing predictions between sentences, KnowledgeVis reveals learned
associations that intuitively connect what language models learn during
training to natural language tasks downstream, helping users create and test
multiple prompt variations, analyze predicted words using a novel semantic
clustering technique, and discover insights using interactive visualizations.
Collectively, these visualizations help users identify the likelihood and
uniqueness of individual predictions, compare sets of predictions between
prompts, and summarize patterns and relationships between predictions across
all prompts. We demonstrate the capabilities of KnowledgeVis with feedback from
six NLP experts as well as three different use cases: (1) probing biomedical
knowledge in two domain-adapted models; and (2) evaluating harmful identity
stereotypes and (3) discovering facts and relationships between three
general-purpose models.</div><div><a href='http://arxiv.org/abs/2403.04758v1'>2403.04758v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01719v3")'>Measuring Moral Inconsistencies in Large Language Models</div>
<div id='2402.01719v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T18:05:47Z</div><div>Authors: Vamshi Krishna Bonagiri, Sreeram Vennam, Manas Gaur, Ponnurangam Kumaraguru</div><div style='padding-top: 10px; width: 80ex'>A Large Language Model (LLM) is considered consistent if semantically
equivalent prompts produce semantically equivalent responses. Despite recent
advancements showcasing the impressive capabilities of LLMs in conversational
systems, we show that even state-of-the-art LLMs are highly inconsistent in
their generations, questioning their reliability. Prior research has tried to
measure this with task-specific accuracy. However, this approach is unsuitable
for moral scenarios, such as the trolley problem, with no "correct" answer. To
address this issue, we propose a novel information-theoretic measure called
Semantic Graph Entropy (SGE) to measure the consistency of an LLM in moral
scenarios. We leverage "Rules of Thumb" (RoTs) to explain a model's
decision-making strategies and further enhance our metric. Compared to existing
consistency metrics, SGE correlates better with human judgments across five
LLMs. In the future, we aim to investigate the root causes of LLM
inconsistencies and propose improvements.</div><div><a href='http://arxiv.org/abs/2402.01719v3'>2402.01719v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.06634v1")'>SocraSynth: Multi-LLM Reasoning with Conditional Statistics</div>
<div id='2402.06634v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T07:16:21Z</div><div>Authors: Edward Y. Chang</div><div style='padding-top: 10px; width: 80ex'>Large language models (LLMs), while promising, face criticisms for biases,
hallucinations, and a lack of reasoning capability. This paper introduces
SocraSynth, a multi-LLM agent reasoning platform developed to mitigate these
issues. SocraSynth utilizes conditional statistics and systematic context
enhancement through continuous arguments, alongside adjustable debate
contentiousness levels. The platform typically involves a human moderator and
two LLM agents representing opposing viewpoints on a given subject. SocraSynth
operates in two main phases: knowledge generation and reasoning evaluation. In
the knowledge generation phase, the moderator defines the debate topic and
contentiousness level, prompting the agents to formulate supporting arguments
for their respective stances. The reasoning evaluation phase then employs
Socratic reasoning and formal logic principles to appraise the quality of the
arguments presented. The dialogue concludes with the moderator adjusting the
contentiousness from confrontational to collaborative, gathering final,
conciliatory remarks to aid in human reasoning and decision-making. Through
case studies in three distinct application domains, this paper showcases
SocraSynth's effectiveness in fostering rigorous research, dynamic reasoning,
comprehensive assessment, and enhanced collaboration. This underscores the
value of multi-agent interactions in leveraging LLMs for advanced knowledge
extraction and decision-making support.</div><div><a href='http://arxiv.org/abs/2402.06634v1'>2402.06634v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02006v2")'>PresAIse, A Prescriptive AI Solution for Enterprises</div>
<div id='2402.02006v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T03:23:08Z</div><div>Authors: Wei Sun, Scott McFaddin, Linh Ha Tran, Shivaram Subramanian, Kristjan Greenewald, Yeshi Tenzin, Zack Xue, Youssef Drissi, Markus Ettl</div><div style='padding-top: 10px; width: 80ex'>Prescriptive AI represents a transformative shift in decision-making,
offering causal insights and actionable recommendations. Despite its huge
potential, enterprise adoption often faces several challenges. The first
challenge is caused by the limitations of observational data for accurate
causal inference which is typically a prerequisite for good decision-making.
The second pertains to the interpretability of recommendations, which is
crucial for enterprise decision-making settings. The third challenge is the
silos between data scientists and business users, hindering effective
collaboration. This paper outlines an initiative from IBM Research, aiming to
address some of these challenges by offering a suite of prescriptive AI
solutions. Leveraging insights from various research papers, the solution suite
includes scalable causal inference methods, interpretable decision-making
approaches, and the integration of large language models (LLMs) to bridge
communication gaps via a conversation agent. A proof-of-concept, PresAIse,
demonstrates the solutions' potential by enabling non-ML experts to interact
with prescriptive AI models via a natural language interface, democratizing
advanced analytics for strategic decision-making.</div><div><a href='http://arxiv.org/abs/2402.02006v2'>2402.02006v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.14727v1")'>Protected group bias and stereotypes in Large Language Models</div>
<div id='2403.14727v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T00:21:38Z</div><div>Authors: Hadas Kotek, David Q. Sun, Zidi Xiu, Margit Bowler, Christopher Klein</div><div style='padding-top: 10px; width: 80ex'>As modern Large Language Models (LLMs) shatter many state-of-the-art
benchmarks in a variety of domains, this paper investigates their behavior in
the domains of ethics and fairness, focusing on protected group bias. We
conduct a two-part study: first, we solicit sentence continuations describing
the occupations of individuals from different protected groups, including
gender, sexuality, religion, and race. Second, we have the model generate
stories about individuals who hold different types of occupations. We collect
&gt;10k sentence completions made by a publicly available LLM, which we subject to
human annotation. We find bias across minoritized groups, but in particular in
the domains of gender and sexuality, as well as Western bias, in model
generations. The model not only reflects societal biases, but appears to
amplify them. The model is additionally overly cautious in replies to queries
relating to minoritized groups, providing responses that strongly emphasize
diversity and equity to an extent that other group characteristics are
overshadowed. This suggests that artificially constraining potentially harmful
outputs may itself lead to harm, and should be applied in a careful and
controlled manner.</div><div><a href='http://arxiv.org/abs/2403.14727v1'>2403.14727v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01766v1")'>LLM Voting: Human Choices and AI Collective Decision Making</div>
<div id='2402.01766v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T14:52:02Z</div><div>Authors: Joshua C. Yang, Marcin Korecki, Damian Dailisan, Carina I. Hausladen, Dirk Helbing</div><div style='padding-top: 10px; width: 80ex'>This paper investigates the voting behaviors of Large Language Models (LLMs),
particularly OpenAI's GPT4 and LLaMA2, and their alignment with human voting
patterns. Our approach included a human voting experiment to establish a
baseline for human preferences and a parallel experiment with LLM agents. The
study focused on both collective outcomes and individual preferences, revealing
differences in decision-making and inherent biases between humans and LLMs. We
observed a trade-off between preference diversity and alignment in LLMs, with a
tendency towards more uniform choices as compared to the diverse preferences of
human voters. This finding indicates that LLMs could lead to more homogenized
collective outcomes when used in voting assistance, underscoring the need for
cautious integration of LLMs into democratic processes.</div><div><a href='http://arxiv.org/abs/2402.01766v1'>2402.01766v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01981v1")'>Self-Debiasing Large Language Models: Zero-Shot Recognition and
  Reduction of Stereotypes</div>
<div id='2402.01981v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T01:40:11Z</div><div>Authors: Isabel O. Gallegos, Ryan A. Rossi, Joe Barrow, Md Mehrab Tanjim, Tong Yu, Hanieh Deilamsalehy, Ruiyi Zhang, Sungchul Kim, Franck Dernoncourt</div><div style='padding-top: 10px; width: 80ex'>Large language models (LLMs) have shown remarkable advances in language
generation and understanding but are also prone to exhibiting harmful social
biases. While recognition of these behaviors has generated an abundance of bias
mitigation techniques, most require modifications to the training data, model
parameters, or decoding strategy, which may be infeasible without access to a
trainable model. In this work, we leverage the zero-shot capabilities of LLMs
to reduce stereotyping in a technique we introduce as zero-shot self-debiasing.
With two approaches, self-debiasing via explanation and self-debiasing via
reprompting, we show that self-debiasing can significantly reduce the degree of
stereotyping across nine different social groups while relying only on the LLM
itself and a simple prompt, with explanations correctly identifying invalid
assumptions and reprompting delivering the greatest reductions in bias. We hope
this work opens inquiry into other zero-shot techniques for bias mitigation.</div><div><a href='http://arxiv.org/abs/2402.01981v1'>2402.01981v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14875v2")'>What's in a Name? Auditing Large Language Models for Race and Gender
  Bias</div>
<div id='2402.14875v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T18:25:25Z</div><div>Authors: Amit Haim, Alejandro Salinas, Julian Nyarko</div><div style='padding-top: 10px; width: 80ex'>We employ an audit design to investigate biases in state-of-the-art large
language models, including GPT-4. In our study, we prompt the models for advice
involving a named individual across a variety of scenarios, such as during car
purchase negotiations or election outcome predictions. We find that the advice
systematically disadvantages names that are commonly associated with racial
minorities and women. Names associated with Black women receive the least
advantageous outcomes. The biases are consistent across 42 prompt templates and
several models, indicating a systemic issue rather than isolated incidents.
While providing numerical, decision-relevant anchors in the prompt can
successfully counteract the biases, qualitative details have inconsistent
effects and may even increase disparities. Our findings underscore the
importance of conducting audits at the point of LLM deployment and
implementation to mitigate their potential for harm against marginalized
communities.</div><div><a href='http://arxiv.org/abs/2402.14875v2'>2402.14875v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17013v1")'>Towards Explainability and Fairness in Swiss Judgement Prediction:
  Benchmarking on a Multilingual Dataset</div>
<div id='2402.17013v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T20:42:40Z</div><div>Authors: Santosh T. Y. S. S, Nina Baumgartner, Matthias Stürmer, Matthias Grabmair, Joel Niklaus</div><div style='padding-top: 10px; width: 80ex'>The assessment of explainability in Legal Judgement Prediction (LJP) systems
is of paramount importance in building trustworthy and transparent systems,
particularly considering the reliance of these systems on factors that may lack
legal relevance or involve sensitive attributes. This study delves into the
realm of explainability and fairness in LJP models, utilizing Swiss Judgement
Prediction (SJP), the only available multilingual LJP dataset. We curate a
comprehensive collection of rationales that `support' and `oppose' judgement
from legal experts for 108 cases in German, French, and Italian. By employing
an occlusion-based explainability approach, we evaluate the explainability
performance of state-of-the-art monolingual and multilingual BERT-based LJP
models, as well as models developed with techniques such as data augmentation
and cross-lingual transfer, which demonstrated prediction performance
improvement. Notably, our findings reveal that improved prediction performance
does not necessarily correspond to enhanced explainability performance,
underscoring the significance of evaluating models from an explainability
perspective. Additionally, we introduce a novel evaluation framework, Lower
Court Insertion (LCI), which allows us to quantify the influence of lower court
information on model predictions, exposing current models' biases.</div><div><a href='http://arxiv.org/abs/2402.17013v1'>2402.17013v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14654v1")'>A Korean Legal Judgment Prediction Dataset for Insurance Disputes</div>
<div id='2401.14654v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T05:26:27Z</div><div>Authors: Alice Saebom Kwak, Cheonkam Jeong, Ji Weon Lim, Byeongcheol Min</div><div style='padding-top: 10px; width: 80ex'>This paper introduces a Korean legal judgment prediction (LJP) dataset for
insurance disputes. Successful LJP models on insurance disputes can benefit
insurance companies and their customers. It can save both sides' time and money
by allowing them to predict how the result would come out if they proceed to
the dispute mediation process. As is often the case with low-resource
languages, there is a limitation on the amount of data available for this
specific task. To mitigate this issue, we investigate how one can achieve a
good performance despite the limitation in data. In our experiment, we
demonstrate that Sentence Transformer Fine-tuning (SetFit, Tunstall et al.,
2022) is a good alternative to standard fine-tuning when training data are
limited. The models fine-tuned with the SetFit approach on our data show
similar performance to the Korean LJP benchmark models (Hwang et al., 2022)
despite the much smaller data size.</div><div><a href='http://arxiv.org/abs/2401.14654v1'>2401.14654v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13107v1")'>Towards Unsupervised Question Answering System with Multi-level
  Summarization for Legal Text</div>
<div id='2403.13107v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T19:15:13Z</div><div>Authors: M Manvith Prabhu, Haricharana Srinivasa, Anand Kumar M</div><div style='padding-top: 10px; width: 80ex'>This paper summarizes Team SCaLAR's work on SemEval-2024 Task 5: Legal
Argument Reasoning in Civil Procedure. To address this Binary Classification
task, which was daunting due to the complexity of the Legal Texts involved, we
propose a simple yet novel similarity and distance-based unsupervised approach
to generate labels. Further, we explore the Multi-level fusion of Legal-Bert
embeddings using ensemble features, including CNN, GRU, and LSTM. To address
the lengthy nature of Legal explanation in the dataset, we introduce T5-based
segment-wise summarization, which successfully retained crucial information,
enhancing the model's performance. Our unsupervised system witnessed a 20-point
increase in macro F1-score on the development set and a 10-point increase on
the test set, which is promising given its uncomplicated architecture.</div><div><a href='http://arxiv.org/abs/2403.13107v1'>2403.13107v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.09516v1")'>Leveraging Prototypical Representations for Mitigating Social Bias
  without Demographic Information</div>
<div id='2403.09516v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T15:58:36Z</div><div>Authors: Shadi Iskander, Kira Radinsky, Yonatan Belinkov</div><div style='padding-top: 10px; width: 80ex'>Mitigating social biases typically requires identifying the social groups
associated with each data sample. In this paper, we present DAFair, a novel
approach to address social bias in language models. Unlike traditional methods
that rely on explicit demographic labels, our approach does not require any
such information. Instead, we leverage predefined prototypical demographic
texts and incorporate a regularization term during the fine-tuning process to
mitigate bias in the model's representations. Our empirical results across two
tasks and two models demonstrate the effectiveness of our method compared to
previous approaches that do not rely on labeled data. Moreover, with limited
demographic-annotated data, our approach outperforms common debiasing
approaches.</div><div><a href='http://arxiv.org/abs/2403.09516v1'>2403.09516v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.05581v1")'>Can Interpretability Layouts Influence Human Perception of Offensive
  Sentences?</div>
<div id='2403.05581v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T13:25:54Z</div><div>Authors: Thiago Freitas dos Santos, Nardine Osman, Marco Schorlemmer</div><div style='padding-top: 10px; width: 80ex'>This paper conducts a user study to assess whether three machine learning
(ML) interpretability layouts can influence participants' views when evaluating
sentences containing hate speech, focusing on the "Misogyny" and "Racism"
classes. Given the existence of divergent conclusions in the literature, we
provide empirical evidence on using ML interpretability in online communities
through statistical and qualitative analyses of questionnaire responses. The
Generalized Additive Model estimates participants' ratings, incorporating
within-subject and between-subject designs. While our statistical analysis
indicates that none of the interpretability layouts significantly influences
participants' views, our qualitative analysis demonstrates the advantages of ML
interpretability: 1) triggering participants to provide corrective feedback in
case of discrepancies between their views and the model, and 2) providing
insights to evaluate a model's behavior beyond traditional performance metrics.</div><div><a href='http://arxiv.org/abs/2403.05581v1'>2403.05581v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.08829v1")'>Mitigating Biases in Collective Decision-Making: Enhancing Performance
  in the Face of Fake News</div>
<div id='2403.08829v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T12:08:08Z</div><div>Authors: Axel Abels, Elias Fernandez Domingos, Ann Nowé, Tom Lenaerts</div><div style='padding-top: 10px; width: 80ex'>Individual and social biases undermine the effectiveness of human advisers by
inducing judgment errors which can disadvantage protected groups. In this
paper, we study the influence these biases can have in the pervasive problem of
fake news by evaluating human participants' capacity to identify false
headlines. By focusing on headlines involving sensitive characteristics, we
gather a comprehensive dataset to explore how human responses are shaped by
their biases. Our analysis reveals recurring individual biases and their
permeation into collective decisions. We show that demographic factors,
headline categories, and the manner in which information is presented
significantly influence errors in human judgment. We then use our collected
data as a benchmark problem on which we evaluate the efficacy of adaptive
aggregation algorithms. In addition to their improved accuracy, our results
highlight the interactions between the emergence of collective intelligence and
the mitigation of participant biases.</div><div><a href='http://arxiv.org/abs/2403.08829v1'>2403.08829v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.14255v1")'>ERD: A Framework for Improving LLM Reasoning for Cognitive Distortion
  Classification</div>
<div id='2403.14255v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T09:28:38Z</div><div>Authors: Sehee Lim, Yejin Kim, Chi-Hyun Choi, Jy-yong Sohn, Byung-Hoon Kim</div><div style='padding-top: 10px; width: 80ex'>Improving the accessibility of psychotherapy with the aid of Large Language
Models (LLMs) is garnering a significant attention in recent years. Recognizing
cognitive distortions from the interviewee's utterances can be an essential
part of psychotherapy, especially for cognitive behavioral therapy. In this
paper, we propose ERD, which improves LLM-based cognitive distortion
classification performance with the aid of additional modules of (1) extracting
the parts related to cognitive distortion, and (2) debating the reasoning steps
by multiple agents. Our experimental results on a public dataset show that ERD
improves the multi-class F1 score as well as binary specificity score.
Regarding the latter score, it turns out that our method is effective in
debiasing the baseline method which has high false positive rate, especially
when the summary of multi-agent debate is provided to LLMs.</div><div><a href='http://arxiv.org/abs/2403.14255v1'>2403.14255v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.18059v1")'>RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval</div>
<div id='2401.18059v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T18:30:21Z</div><div>Authors: Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D. Manning</div><div style='padding-top: 10px; width: 80ex'>Retrieval-augmented language models can better adapt to changes in world
state and incorporate long-tail knowledge. However, most existing methods
retrieve only short contiguous chunks from a retrieval corpus, limiting
holistic understanding of the overall document context. We introduce the novel
approach of recursively embedding, clustering, and summarizing chunks of text,
constructing a tree with differing levels of summarization from the bottom up.
At inference time, our RAPTOR model retrieves from this tree, integrating
information across lengthy documents at different levels of abstraction.
Controlled experiments show that retrieval with recursive summaries offers
significant improvements over traditional retrieval-augmented LMs on several
tasks. On question-answering tasks that involve complex, multi-step reasoning,
we show state-of-the-art results; for example, by coupling RAPTOR retrieval
with the use of GPT-4, we can improve the best performance on the QuALITY
benchmark by 20% in absolute accuracy.</div><div><a href='http://arxiv.org/abs/2401.18059v1'>2401.18059v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.12900v1")'>Toward Sustainable GenAI using Generation Directives for Carbon-Friendly
  Large Language Model Inference</div>
<div id='2403.12900v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T16:53:53Z</div><div>Authors: Baolin Li, Yankai Jiang, Vijay Gadepally, Devesh Tiwari</div><div style='padding-top: 10px; width: 80ex'>The rapid advancement of Generative Artificial Intelligence (GenAI) across
diverse sectors raises significant environmental concerns, notably the carbon
emissions from their cloud and high performance computing (HPC) infrastructure.
This paper presents Sprout, an innovative framework designed to address these
concerns by reducing the carbon footprint of generative Large Language Model
(LLM) inference services. Sprout leverages the innovative concept of
"generation directives" to guide the autoregressive generation process, thereby
enhancing carbon efficiency. Our proposed method meticulously balances the need
for ecological sustainability with the demand for high-quality generation
outcomes. Employing a directive optimizer for the strategic assignment of
generation directives to user prompts and an original offline quality
evaluator, Sprout demonstrates a significant reduction in carbon emissions by
over 40% in real-world evaluations using the Llama2 LLM and global electricity
grid data. This research marks a critical step toward aligning AI technology
with sustainable practices, highlighting the potential for mitigating
environmental impacts in the rapidly expanding domain of generative artificial
intelligence.</div><div><a href='http://arxiv.org/abs/2403.12900v1'>2403.12900v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09939v1")'>Generative AI in the Construction Industry: A State-of-the-art Analysis</div>
<div id='2402.09939v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T13:39:55Z</div><div>Authors: Ridwan Taiwo, Idris Temitope Bello, Sulemana Fatoama Abdulai, Abdul-Mugis Yussif, Babatunde Abiodun Salami, Abdullahi Saka, Tarek Zayed</div><div style='padding-top: 10px; width: 80ex'>The construction industry is a vital sector of the global economy, but it
faces many productivity challenges in various processes, such as design,
planning, procurement, inspection, and maintenance. Generative artificial
intelligence (AI), which can create novel and realistic data or content, such
as text, image, video, or code, based on some input or prior knowledge, offers
innovative and disruptive solutions to address these challenges. However, there
is a gap in the literature on the current state, opportunities, and challenges
of generative AI in the construction industry. This study aims to fill this gap
by providing a state-of-the-art analysis of generative AI in construction, with
three objectives: (1) to review and categorize the existing and emerging
generative AI opportunities and challenges in the construction industry; (2) to
propose a framework for construction firms to build customized generative AI
solutions using their own data, comprising steps such as data collection,
dataset curation, training custom large language model (LLM), model evaluation,
and deployment; and (3) to demonstrate the framework via a case study of
developing a generative model for querying contract documents. The results show
that retrieval augmented generation (RAG) improves the baseline LLM by 5.2,
9.4, and 4.8% in terms of quality, relevance, and reproducibility. This study
provides academics and construction professionals with a comprehensive analysis
and practical framework to guide the adoption of generative AI techniques to
enhance productivity, quality, safety, and sustainability across the
construction industry.</div><div><a href='http://arxiv.org/abs/2402.09939v1'>2402.09939v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.12219v1")'>Reformatted Alignment</div>
<div id='2402.12219v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T15:21:58Z</div><div>Authors: Run-Ze Fan, Xuefeng Li, Haoyang Zou, Junlong Li, Shwai He, Ethan Chern, Jiewen Hu, Pengfei Liu</div><div style='padding-top: 10px; width: 80ex'>The quality of finetuning data is crucial for aligning large language models
(LLMs) with human values. Current methods to improve data quality are either
labor-intensive or prone to factual errors caused by LLM hallucinations. This
paper explores elevating the quality of existing instruction data to better
align with human values, introducing a simple and effective approach named
ReAlign, which reformats the responses of instruction data into a format that
better aligns with pre-established criteria and the collated evidence. This
approach minimizes human annotation, hallucination, and the difficulty in
scaling, remaining orthogonal to existing alignment techniques. Experimentally,
ReAlign significantly boosts the general alignment ability, math reasoning,
factuality, and readability of the LLMs.
  Encouragingly, without introducing any additional data or advanced training
techniques, and merely by reformatting the response, LLaMA-2-13B's mathematical
reasoning ability on GSM8K can be improved from 46.77% to 56.63% in accuracy.
Additionally, a mere 5% of ReAlign data yields a 67% boost in general alignment
ability measured by the Alpaca dataset. This work highlights the need for
further research into the science and mechanistic interpretability of LLMs. We
have made the associated code and data publicly accessible to support future
studies at https://github.com/GAIR-NLP/ReAlign.</div><div><a href='http://arxiv.org/abs/2402.12219v1'>2402.12219v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.10884v1")'>Multi-modal preference alignment remedies regression of visual
  instruction tuning on language model</div>
<div id='2402.10884v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T18:42:08Z</div><div>Authors: Shengzhi Li, Rongyu Lin, Shichao Pei</div><div style='padding-top: 10px; width: 80ex'>In production, multi-modal large language models (MLLMs) are expected to
support multi-turn queries of interchanging image and text modalities. However,
the current MLLMs trained with visual-question-answering (VQA) datasets could
suffer from degradation, as VQA datasets lack the diversity and complexity of
the original text instruction datasets which the underlying language model had
been trained with. To address this challenging degradation, we first collect a
lightweight (6k entries) VQA preference dataset where answers were annotated by
Gemini for 5 quality metrics in a granular fashion, and investigate standard
Supervised Fine-tuning, rejection sampling, Direct Preference Optimization
(DPO), and SteerLM. Our findings indicate that the with DPO we are able to
surpass instruction-following capabilities of the language model, achieving a
6.73 score on MT-Bench, compared to Vicuna's 6.57 and LLaVA's 5.99 despite
small data scale. This enhancement in textual instruction proficiency
correlates with boosted visual instruction performance (+4.9\% on MM-Vet, +6\%
on LLaVA-Bench), with minimal alignment tax on visual knowledge benchmarks
compared to previous RLHF approach. In conclusion, we propose a
distillation-based multi-modal alignment model with fine-grained annotations on
a small dataset that reconciles the textual and visual performance of MLLMs,
restoring and boosting language capability after visual instruction tuning.</div><div><a href='http://arxiv.org/abs/2402.10884v1'>2402.10884v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14811v1")'>Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity
  Tracking</div>
<div id='2402.14811v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T18:59:24Z</div><div>Authors: Nikhil Prakash, Tamar Rott Shaham, Tal Haklay, Yonatan Belinkov, David Bau</div><div style='padding-top: 10px; width: 80ex'>Fine-tuning on generalized tasks such as instruction following, code
generation, and mathematics has been shown to enhance language models'
performance on a range of tasks. Nevertheless, explanations of how such
fine-tuning influences the internal computations in these models remain
elusive. We study how fine-tuning affects the internal mechanisms implemented
in language models. As a case study, we explore the property of entity
tracking, a crucial facet of language comprehension, where models fine-tuned on
mathematics have substantial performance gains. We identify the mechanism that
enables entity tracking and show that (i) in both the original model and its
fine-tuned versions primarily the same circuit implements entity tracking. In
fact, the entity tracking circuit of the original model on the fine-tuned
versions performs better than the full original model. (ii) The circuits of all
the models implement roughly the same functionality: Entity tracking is
performed by tracking the position of the correct entity in both the original
model and its fine-tuned versions. (iii) Performance boost in the fine-tuned
models is primarily attributed to its improved ability to handle the augmented
positional information. To uncover these findings, we employ: Patch Patching,
DCM, which automatically detects model components responsible for specific
semantics, and CMAP, a new approach for patching activations across models to
reveal improved mechanisms. Our findings suggest that fine-tuning enhances,
rather than fundamentally alters, the mechanistic operation of the model.</div><div><a href='http://arxiv.org/abs/2402.14811v1'>2402.14811v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00800v1")'>Brain-Inspired Two-Stage Approach: Enhancing Mathematical Reasoning by
  Imitating Human Thought Processes</div>
<div id='2403.00800v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T17:40:31Z</div><div>Authors: Yezeng Chen, Zui Chen, Yi Zhou</div><div style='padding-top: 10px; width: 80ex'>Although large language models demonstrate emergent abilities in solving math
word problems, there is a challenging task in complex multi-step mathematical
reasoning tasks. To improve model performance on mathematical reasoning tasks,
previous work has conducted supervised fine-tuning on open-source models by
improving the quality and quantity of data. In this paper, we propose a novel
approach, named Brain, to imitate human thought processes to enhance
mathematical reasoning abilities, using the Frontal Lobe Model to generate
plans, and then employing the Parietal Lobe Model to generate code and execute
to obtain answers. First, we achieve SOTA performance in comparison with Code
LLaMA 7B based models through this method. Secondly, we find that plans can be
explicitly extracted from natural language, code, or formal language. Our code
and data are publicly available at https://github.com/cyzhh/Brain.</div><div><a href='http://arxiv.org/abs/2403.00800v1'>2403.00800v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13714v1")'>An Evaluation of Large Language Models in Bioinformatics Research</div>
<div id='2402.13714v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T11:27:31Z</div><div>Authors: Hengchuang Yin, Zhonghui Gu, Fanhao Wang, Yiparemu Abuduhaibaier, Yanqiao Zhu, Xinming Tu, Xian-Sheng Hua, Xiao Luo, Yizhou Sun</div><div style='padding-top: 10px; width: 80ex'>Large language models (LLMs) such as ChatGPT have gained considerable
interest across diverse research communities. Their notable ability for text
completion and generation has inaugurated a novel paradigm for
language-interfaced problem solving. However, the potential and efficacy of
these models in bioinformatics remain incompletely explored. In this work, we
study the performance LLMs on a wide spectrum of crucial bioinformatics tasks.
These tasks include the identification of potential coding regions, extraction
of named entities for genes and proteins, detection of antimicrobial and
anti-cancer peptides, molecular optimization, and resolution of educational
bioinformatics problems. Our findings indicate that, given appropriate prompts,
LLMs like GPT variants can successfully handle most of these tasks. In
addition, we provide a thorough analysis of their limitations in the context of
complicated bioinformatics tasks. In conclusion, we believe that this work can
provide new perspectives and motivate future research in the field of LLMs
applications, AI for Science and bioinformatics.</div><div><a href='http://arxiv.org/abs/2402.13714v1'>2402.13714v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00123v1")'>Comparing Template-based and Template-free Language Model Probing</div>
<div id='2402.00123v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T19:07:37Z</div><div>Authors: Sagi Shaier, Kevin Bennett, Lawrence E Hunter, Katharina von der Wense</div><div style='padding-top: 10px; width: 80ex'>The differences between cloze-task language model (LM) probing with 1)
expert-made templates and 2) naturally-occurring text have often been
overlooked. Here, we evaluate 16 different LMs on 10 probing English datasets
-- 4 template-based and 6 template-free -- in general and biomedical domains to
answer the following research questions: (RQ1) Do model rankings differ between
the two approaches? (RQ2) Do models' absolute scores differ between the two
approaches? (RQ3) Do the answers to RQ1 and RQ2 differ between general and
domain-specific models? Our findings are: 1) Template-free and template-based
approaches often rank models differently, except for the top domain-specific
models. 2) Scores decrease by up to 42% Acc@1 when comparing parallel
template-free and template-based prompts. 3) Perplexity is negatively
correlated with accuracy in the template-free approach, but,
counter-intuitively, they are positively correlated for template-based probing.
4) Models tend to predict the same answers frequently across prompts for
template-based probing, which is less common when employing template-free
techniques.</div><div><a href='http://arxiv.org/abs/2402.00123v1'>2402.00123v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.08957v2")'>MUSTARD: Mastering Uniform Synthesis of Theorem and Proof Data</div>
<div id='2402.08957v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T05:57:58Z</div><div>Authors: Yinya Huang, Xiaohan Lin, Zhengying Liu, Qingxing Cao, Huajian Xin, Haiming Wang, Zhenguo Li, Linqi Song, Xiaodan Liang</div><div style='padding-top: 10px; width: 80ex'>Recent large language models (LLMs) have witnessed significant advancement in
various tasks, including mathematical reasoning and theorem proving. As these
two tasks require strict and formal multi-step inference, they are appealing
domains for exploring the reasoning ability of LLMs but still face important
challenges. Previous studies such as Chain-of-Thought (CoT) have revealed the
effectiveness of intermediate steps guidance. However, such step-wise
annotation requires heavy labor, leading to insufficient training steps for
current benchmarks. To fill this gap, this work introduces MUSTARD, a data
generation framework that masters uniform synthesis of theorem and proof data
of high quality and diversity. MUSTARD synthesizes data in three stages: (1) It
samples a few mathematical concept seeds as the problem category. (2) Then, it
prompts a generative language model with the sampled concepts to obtain both
the problems and their step-wise formal solutions. (3) Lastly, the framework
utilizes a proof assistant (e.g., Lean Prover) to filter the valid proofs. With
the proposed MUSTARD, we present a theorem-and-proof benchmark MUSTARDSAUCE
with 5,866 valid data points. Each data point contains an informal statement,
an informal proof, and a translated formal proof that passes the prover
validation. We perform extensive analysis and demonstrate that MUSTARD
generates validated high-quality step-by-step data. We further apply the
MUSTARDSAUCE for fine-tuning smaller language models. The fine-tuned Llama 2-7B
achieves a 15.41% average relative performance gain in automated theorem
proving, and 8.18% in math word problems. Codes and data are available at
https://github.com/Eleanor-H/MUSTARD.</div><div><a href='http://arxiv.org/abs/2402.08957v2'>2402.08957v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17786v1")'>Stepwise Self-Consistent Mathematical Reasoning with Large Language
  Models</div>
<div id='2402.17786v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-24T08:22:39Z</div><div>Authors: Zilong Zhao, Yao Rong, Dongyang Guo, Emek Gözlüklü, Emir Gülboy, Enkelejda Kasneci</div><div style='padding-top: 10px; width: 80ex'>Using Large Language Models for complex mathematical reasoning is difficult,
primarily due to the complexity of multi-step reasoning. The main challenges of
this process include (1) selecting critical intermediate results to advance the
procedure, and (2) limited exploration of potential solutions. To address these
issues, we introduce a novel algorithm, namely Stepwise Self-Consistent
Chain-of-Thought (SSC-CoT). SSC-CoT employs a strategy of selecting
intermediate steps based on the intersection of various reasoning chains.
Additionally, SSC-CoT enables the model to discover critical intermediate steps
by querying a knowledge graph comprising relevant domain knowledge. To validate
SSC-CoT, we present a new dataset, TriMaster100, tailored for complex
trigonometry problems. This dataset contains 100 questions, with each solution
broken down into scored intermediate steps, facilitating a comprehensive
evaluation of the mathematical reasoning process. On TriMaster100, SSC-CoT
triples the effectiveness of the state-of-the-art methods. Furthermore, we
benchmark SSC-CoT on the widely recognized complex mathematical question
dataset, MATH level 5, and it surpasses the second-best method by 7.2% in
accuracy. Code and the TriMaster100 dataset can be found at:
https://github.com/zhao-zilong/ssc-cot.</div><div><a href='http://arxiv.org/abs/2402.17786v1'>2402.17786v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14624v1")'>MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual
  Math Problems?</div>
<div id='2403.14624v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T17:59:50Z</div><div>Authors: Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, Hongsheng Li</div><div style='padding-top: 10px; width: 80ex'>The remarkable progress of Multi-modal Large Language Models (MLLMs) has
garnered unparalleled attention, due to their superior performance in visual
contexts. However, their capabilities in visual math problem-solving remain
insufficiently evaluated and understood. We investigate current benchmarks to
incorporate excessive visual content within textual questions, which
potentially assist MLLMs in deducing answers without truly interpreting the
input diagrams. To this end, we introduce MathVerse, an all-around visual math
benchmark designed for an equitable and in-depth evaluation of MLLMs. We
meticulously collect 2,612 high-quality, multi-subject math problems with
diagrams from publicly available sources. Each problem is then transformed by
human annotators into six distinct versions, each offering varying degrees of
information content in multi-modality, contributing to 15K test samples in
total. This approach allows MathVerse to comprehensively assess whether and how
much MLLMs can truly understand the visual diagrams for mathematical reasoning.
In addition, we propose a Chain-of-Thought (CoT) evaluation strategy for a
fine-grained assessment of the output answers. Rather than naively judging True
or False, we employ GPT-4(V) to adaptively extract crucial reasoning steps, and
then score each step with detailed error analysis, which can reveal the
intermediate CoT reasoning quality by MLLMs. We hope the MathVerse benchmark
may provide unique insights to guide the future development of MLLMs. Project
page: https://mathverse-cuhk.github.io</div><div><a href='http://arxiv.org/abs/2403.14624v1'>2403.14624v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14804v1")'>Measuring Multimodal Mathematical Reasoning with MATH-Vision Dataset</div>
<div id='2402.14804v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T18:56:38Z</div><div>Authors: Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Mingjie Zhan, Hongsheng Li</div><div style='padding-top: 10px; width: 80ex'>Recent advancements in Large Multimodal Models (LMMs) have shown promising
results in mathematical reasoning within visual contexts, with models
approaching human-level performance on existing benchmarks such as MathVista.
However, we observe significant limitations in the diversity of questions and
breadth of subjects covered by these benchmarks. To address this issue, we
present the MATH-Vision (MATH-V) dataset, a meticulously curated collection of
3,040 high-quality mathematical problems with visual contexts sourced from real
math competitions. Spanning 16 distinct mathematical disciplines and graded
across 5 levels of difficulty, our dataset provides a comprehensive and diverse
set of challenges for evaluating the mathematical reasoning abilities of LMMs.
Through extensive experimentation, we unveil a notable performance gap between
current LMMs and human performance on MATH-V, underscoring the imperative for
further advancements in LMMs. Moreover, our detailed categorization allows for
a thorough error analysis of LMMs, offering valuable insights to guide future
research and development. The project is available at
https://mathvision-cuhk.github.io</div><div><a href='http://arxiv.org/abs/2402.14804v1'>2402.14804v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.13897v2")'>Science Checker Reloaded: A Bidirectional Paradigm for Transparency and
  Logical Reasoning</div>
<div id='2402.13897v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T16:09:25Z</div><div>Authors: Loïc Rakotoson, Sylvain Massip, Fréjus A. A. Laleye</div><div style='padding-top: 10px; width: 80ex'>Information retrieval is a rapidly evolving field. However it still faces
significant limitations in the scientific and industrial vast amounts of
information, such as semantic divergence and vocabulary gaps in sparse
retrieval, low precision and lack of interpretability in semantic search, or
hallucination and outdated information in generative models. In this paper, we
introduce a two-block approach to tackle these hurdles for long documents. The
first block enhances language understanding in sparse retrieval by query
expansion to retrieve relevant documents. The second block deepens the result
by providing comprehensive and informative answers to the complex question
using only the information spread in the long document, enabling bidirectional
engagement. At various stages of the pipeline, intermediate results are
presented to users to facilitate understanding of the system's reasoning. We
believe this bidirectional approach brings significant advancements in terms of
transparency, logical thinking, and comprehensive understanding in the field of
scientific information retrieval.</div><div><a href='http://arxiv.org/abs/2402.13897v2'>2402.13897v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.10963v1")'>GLoRe: When, Where, and How to Improve LLM Reasoning via Global and
  Local Refinements</div>
<div id='2402.10963v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T20:16:29Z</div><div>Authors: Alex Havrilla, Sharath Raparthy, Christoforus Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, Roberta Railneau</div><div style='padding-top: 10px; width: 80ex'>State-of-the-art language models can exhibit impressive reasoning refinement
capabilities on math, science or coding tasks. However, recent work
demonstrates that even the best models struggle to identify \textit{when and
where to refine} without access to external feedback. Outcome-based Reward
Models (\textbf{ORMs}), trained to predict correctness of the final answer
indicating when to refine, offer one convenient solution for deciding when to
refine. Process Based Reward Models (\textbf{PRMs}), trained to predict
correctness of intermediate steps, can then be used to indicate where to
refine. But they are expensive to train, requiring extensive human annotations.
In this paper, we propose Stepwise ORMs (\textbf{SORMs}) which are trained,
only on synthetic data, to approximate the expected future reward of the
optimal policy or $V^{\star}$. More specifically, SORMs are trained to predict
the correctness of the final answer when sampling the current policy many times
(rather than only once as in the case of ORMs). Our experiments show that SORMs
can more accurately detect incorrect reasoning steps compared to ORMs, thus
improving downstream accuracy when doing refinements. We then train
\textit{global} refinement models, which take only the question and a draft
solution as input and predict a corrected solution, and \textit{local}
refinement models which also take as input a critique indicating the location
of the first reasoning error. We generate training data for both models
synthetically by reusing data used to train the SORM. We find combining global
and local refinements, using the ORM as a reranker, significantly outperforms
either one individually, as well as a best of three sample baseline. With this
strategy we can improve the accuracy of a LLaMA-2 13B model (already fine-tuned
with RL) on GSM8K from 53\% to 65\% when greedily sampled.</div><div><a href='http://arxiv.org/abs/2402.10963v1'>2402.10963v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00199v1")'>Improving Socratic Question Generation using Data Augmentation and
  Preference Optimization</div>
<div id='2403.00199v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T00:08:20Z</div><div>Authors: Nischal Ashok Kumar, Andrew Lan</div><div style='padding-top: 10px; width: 80ex'>The Socratic method is a way of guiding students toward solving a problem
independently without directly revealing the solution to the problem. Although
this method has been shown to significantly improve student learning outcomes,
it remains a complex labor-intensive task for instructors. Large language
models (LLMs) can be used to augment human effort by automatically generating
Socratic questions for students. However, existing methods that involve
prompting these LLMs sometimes produce invalid outputs, e.g., those that
directly reveal the solution to the problem or provide irrelevant or premature
questions. To alleviate this problem, inspired by reinforcement learning with
AI feedback (RLAIF), we first propose a data augmentation method to enrich
existing Socratic questioning datasets with questions that are invalid in
specific ways. Next, we propose a method to optimize open-source LLMs such as
LLama 2 to prefer ground-truth questions over generated invalid ones, using
direct preference optimization (DPO). Our experiments on a Socratic questions
dataset for student code debugging show that a DPO-optimized 7B LLama 2 model
can effectively avoid generating invalid questions, and as a result,
outperforms existing state-of-the-art prompting methods.</div><div><a href='http://arxiv.org/abs/2403.00199v1'>2403.00199v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.07148v1")'>X-LoRA: Mixture of Low-Rank Adapter Experts, a Flexible Framework for
  Large Language Models with Applications in Protein Mechanics and Design</div>
<div id='2402.07148v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T10:23:34Z</div><div>Authors: Eric L. Buehler, Markus J. Buehler</div><div style='padding-top: 10px; width: 80ex'>We report a mixture of expert strategy to create fine-tuned large language
models using a deep layer-wise token-level approach based on low-rank
adaptation (LoRA). Starting with a set of pre-trained LoRA adapters, we propose
a gating strategy that uses the hidden states to dynamically mix adapted
layers, allowing the resulting X-LoRA model to draw upon different capabilities
and create never-before-used deep layer-wise combinations of adaptations are
established to solve specific tasks. The design is inspired by the biological
principles of universality and diversity, where neural network building blocks
are reused in different hierarchical manifestations. Hence, the X-LoRA model
can be easily implemented for any existing large language model (LLM) without a
need for modifications of the underlying structure. We develop a tailored
X-LoRA model that offers scientific capabilities including forward/inverse
analysis tasks and enhanced reasoning capability, focused on biomaterial
analysis, protein mechanics and design. The impact of this work include access
to readily expandable, adaptable and changeable models with strong domain
knowledge and the capability to integrate across areas of knowledge. With the
X-LoRA model featuring experts in biology, mathematics, reasoning, bio-inspired
materials, mechanics and materials, chemistry, and protein mechanics we conduct
a series of physics-focused case studies. We examine knowledge recall, protein
mechanics forward/inverse tasks, protein design, and adversarial agentic
modeling including ontological knowledge graphs. The model is capable not only
of making quantitative predictions of nanomechanical properties of proteins,
but also reasons over the results and correctly predicts likely mechanisms that
explain distinct molecular behaviors.</div><div><a href='http://arxiv.org/abs/2402.07148v1'>2402.07148v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14903v1")'>Tokenization counts: the impact of tokenization on arithmetic in
  frontier LLMs</div>
<div id='2402.14903v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T18:14:09Z</div><div>Authors: Aaditya K. Singh, DJ Strouse</div><div style='padding-top: 10px; width: 80ex'>Tokenization, the division of input text into input tokens, is an often
overlooked aspect of the large language model (LLM) pipeline and could be the
source of useful or harmful inductive biases. Historically, LLMs have relied on
byte pair encoding, without care to specific input domains. With the increased
use of LLMs for reasoning, various number-specific tokenization schemes have
been adopted, with popular models like LLaMa and PaLM opting for single-digit
tokenization while GPT-3.5 and GPT-4 have separate tokens for each 1-, 2-, and
3-digit numbers. In this work, we study the effect this choice has on numerical
reasoning through the use of arithmetic tasks. We consider left-to-right and
right-to-left tokenization for GPT-3.5 and -4, finding that right-to-left
tokenization (enforced by comma separating numbers at inference time) leads to
largely improved performance. Furthermore, we find that model errors when using
standard left-to-right tokenization follow stereotyped error patterns,
suggesting that model computations are systematic rather than approximate. We
show that the model is able to convert between tokenizations easily, thus
allowing chain-of-thought-inspired approaches to recover performance on
left-to-right tokenized inputs. We also find the gap between tokenization
directions decreases when models are scaled, possibly indicating that larger
models are better able to override this tokenization-dependent inductive bias.
In summary, our work performs the first study of how number tokenization
choices lead to differences in model performance on arithmetic tasks,
accompanied by a thorough analysis of error patterns. We hope this work
inspires practitioners to more carefully ablate number tokenization-related
choices when working towards general models of numerical reasoning.</div><div><a href='http://arxiv.org/abs/2402.14903v1'>2402.14903v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05618v1")'>OmniJet-$α$: The first cross-task foundation model for particle
  physics</div>
<div id='2403.05618v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T19:00:01Z</div><div>Authors: Joschka Birk, Anna Hallin, Gregor Kasieczka</div><div style='padding-top: 10px; width: 80ex'>Foundation models are multi-dataset and multi-task machine learning methods
that once pre-trained can be fine-tuned for a large variety of downstream
applications. The successful development of such general-purpose models for
physics data would be a major breakthrough as they could improve the achievable
physics performance while at the same time drastically reduce the required
amount of training time and data.
  We report significant progress on this challenge on several fronts. First, a
comprehensive set of evaluation methods is introduced to judge the quality of
an encoding from physics data into a representation suitable for the
autoregressive generation of particle jets with transformer architectures (the
common backbone of foundation models). These measures motivate the choice of a
higher-fidelity tokenization compared to previous works. Finally, we
demonstrate transfer learning between an unsupervised problem (jet generation)
and a classic supervised task (jet tagging) with our new OmniJet-$\alpha$
model. This is the first successful transfer between two different and actively
studied classes of tasks and constitutes a major step in the building of
foundation models for particle physics.</div><div><a href='http://arxiv.org/abs/2403.05618v1'>2403.05618v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13536v2")'>Finetuning Foundation Models for Joint Analysis Optimization</div>
<div id='2401.13536v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T15:46:25Z</div><div>Authors: Matthias Vigl, Nicole Hartman, Lukas Heinrich</div><div style='padding-top: 10px; width: 80ex'>In this work we demonstrate that significant gains in performance and data
efficiency can be achieved in High Energy Physics (HEP) by moving beyond the
standard paradigm of sequential optimization or reconstruction and analysis
components. We conceptually connect HEP reconstruction and analysis to modern
machine learning workflows such as pretraining, finetuning, domain adaptation
and high-dimensional embedding spaces and quantify the gains in the example
usecase of searches of heavy resonances decaying via an intermediate di-Higgs
system to four $b$-jets.</div><div><a href='http://arxiv.org/abs/2401.13536v2'>2401.13536v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.10239v1")'>A Language Model for Particle Tracking</div>
<div id='2402.10239v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T18:24:49Z</div><div>Authors: Andris Huang, Yash Melkani, Paolo Calafiura, Alina Lazar, Daniel Thomas Murnane, Minh-Tuan Pham, Xiangyang Ju</div><div style='padding-top: 10px; width: 80ex'>Particle tracking is crucial for almost all physics analysis programs at the
Large Hadron Collider. Deep learning models are pervasively used in particle
tracking related tasks. However, the current practice is to design and train
one deep learning model for one task with supervised learning techniques. The
trained models work well for tasks they are trained on but show no or little
generalization capabilities. We propose to unify these models with a language
model. In this paper, we present a tokenized detector representation that
allows us to train a BERT model for particle tracking. The trained BERT model,
namely TrackingBERT, offers latent detector module embedding that can be used
for other tasks. This work represents the first step towards developing a
foundational model for particle detector understanding.</div><div><a href='http://arxiv.org/abs/2402.10239v1'>2402.10239v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.05948v1")'>DE$^3$-BERT: Distance-Enhanced Early Exiting for BERT based on
  Prototypical Networks</div>
<div id='2402.05948v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T15:51:17Z</div><div>Authors: Jianing He, Qi Zhang, Weiping Ding, Duoqian Miao, Jun Zhao, Liang Hu, Longbing Cao</div><div style='padding-top: 10px; width: 80ex'>Early exiting has demonstrated its effectiveness in accelerating the
inference of pre-trained language models like BERT by dynamically adjusting the
number of layers executed. However, most existing early exiting methods only
consider local information from an individual test sample to determine their
exiting indicators, failing to leverage the global information offered by
sample population. This leads to suboptimal estimation of prediction
correctness, resulting in erroneous exiting decisions. To bridge the gap, we
explore the necessity of effectively combining both local and global
information to ensure reliable early exiting during inference. Purposefully, we
leverage prototypical networks to learn class prototypes and devise a distance
metric between samples and class prototypes. This enables us to utilize global
information for estimating the correctness of early predictions. On this basis,
we propose a novel Distance-Enhanced Early Exiting framework for BERT
(DE$^3$-BERT). DE$^3$-BERT implements a hybrid exiting strategy that
supplements classic entropy-based local information with distance-based global
information to enhance the estimation of prediction correctness for more
reliable early exiting decisions. Extensive experiments on the GLUE benchmark
demonstrate that DE$^3$-BERT consistently outperforms state-of-the-art models
under different speed-up ratios with minimal storage or computational overhead,
yielding a better trade-off between model performance and inference efficiency.
Additionally, an in-depth analysis further validates the generality and
interpretability of our method.</div><div><a href='http://arxiv.org/abs/2402.05948v1'>2402.05948v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.01081v2")'>LAB: Large-Scale Alignment for ChatBots</div>
<div id='2403.01081v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T03:48:37Z</div><div>Authors: Shivchander Sudalairaj, Abhishek Bhandwaldar, Aldo Pareja, Kai Xu, David D. Cox, Akash Srivastava</div><div style='padding-top: 10px; width: 80ex'>This work introduces LAB (Large-scale Alignment for chatBots), a novel
methodology designed to overcome the scalability challenges in the
instruction-tuning phase of large language model (LLM) training. Leveraging a
taxonomy-guided synthetic data generation process and a multi-phase tuning
framework, LAB significantly reduces reliance on expensive human annotations
and proprietary models like GPT-4. We demonstrate that LAB-trained models can
achieve competitive performance across several benchmarks compared to models
trained with traditional human-annotated or GPT-4 generated synthetic data.
Thus offering a scalable, cost-effective solution for enhancing LLM
capabilities and instruction-following behaviors without the drawbacks of
catastrophic forgetting, marking a step forward in the efficient training of
LLMs for a wide range of applications.</div><div><a href='http://arxiv.org/abs/2403.01081v2'>2403.01081v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12474v1")'>Large Language Models are Superpositions of All Characters: Attaining
  Arbitrary Role-play via Self-Alignment</div>
<div id='2401.12474v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T03:56:22Z</div><div>Authors: Keming Lu, Bowen Yu, Chang Zhou, Jingren Zhou</div><div style='padding-top: 10px; width: 80ex'>Considerable efforts have been invested in augmenting the role-playing
proficiency of open-source large language models (LLMs) by emulating
proprietary counterparts. Nevertheless, we posit that LLMs inherently harbor
role-play capabilities, owing to the extensive knowledge of characters and
potential dialogues ingrained in their vast training corpora. Thus, in this
study, we introduce Ditto, a self-alignment method for role-play. Ditto
capitalizes on character knowledge, encouraging an instruction-following LLM to
simulate role-play dialogues as a variant of reading comprehension. This method
creates a role-play training set comprising 4,000 characters, surpassing the
scale of currently available datasets by tenfold regarding the number of roles.
Subsequently, we fine-tune the LLM using this self-generated dataset to augment
its role-playing capabilities. Upon evaluating our meticulously constructed and
reproducible role-play benchmark and the roleplay subset of MT-Bench, Ditto, in
various parameter scales, consistently maintains a consistent role identity and
provides accurate role-specific knowledge in multi-turn role-play
conversations. Notably, it outperforms all open-source role-play baselines,
showcasing performance levels comparable to advanced proprietary chatbots.
Furthermore, we present the first comprehensive cross-supervision alignment
experiment in the role-play domain, revealing that the intrinsic capabilities
of LLMs confine the knowledge within role-play. Meanwhile, the role-play styles
can be easily acquired with the guidance of smaller models. We open-source
related resources at https://github.com/OFA-Sys/Ditto.</div><div><a href='http://arxiv.org/abs/2401.12474v1'>2401.12474v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11203v1")'>Exploring ChatGPT for Next-generation Information Retrieval:
  Opportunities and Challenges</div>
<div id='2402.11203v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-17T05:44:40Z</div><div>Authors: Yizheng Huang, Jimmy Huang</div><div style='padding-top: 10px; width: 80ex'>The rapid advancement of artificial intelligence (AI) has highlighted ChatGPT
as a pivotal technology in the field of information retrieval (IR).
Distinguished from its predecessors, ChatGPT offers significant benefits that
have attracted the attention of both the industry and academic communities.
While some view ChatGPT as a groundbreaking innovation, others attribute its
success to the effective integration of product development and market
strategies. The emergence of ChatGPT, alongside GPT-4, marks a new phase in
Generative AI, generating content that is distinct from training examples and
exceeding the capabilities of the prior GPT-3 model by OpenAI. Unlike the
traditional supervised learning approach in IR tasks, ChatGPT challenges
existing paradigms, bringing forth new challenges and opportunities regarding
text quality assurance, model bias, and efficiency. This paper seeks to examine
the impact of ChatGPT on IR tasks and offer insights into its potential future
developments.</div><div><a href='http://arxiv.org/abs/2402.11203v1'>2402.11203v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13613v2")'>Overview of the VLSP 2023 -- ComOM Shared Task: A Data Challenge for
  Comparative Opinion Mining from Vietnamese Product Reviews</div>
<div id='2402.13613v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T08:29:26Z</div><div>Authors: Hoang-Quynh Le, Duy-Cat Can, Khanh-Vinh Nguyen, Mai-Vu Tran</div><div style='padding-top: 10px; width: 80ex'>This paper presents a comprehensive overview of the Comparative Opinion
Mining from Vietnamese Product Reviews shared task (ComOM), held as part of the
10$^{th}$ International Workshop on Vietnamese Language and Speech Processing
(VLSP 2023). The primary objective of this shared task is to advance the field
of natural language processing by developing techniques that proficiently
extract comparative opinions from Vietnamese product reviews. Participants are
challenged to propose models that adeptly extract a comparative "quintuple"
from a comparative sentence, encompassing Subject, Object, Aspect, Predicate,
and Comparison Type Label. We construct a human-annotated dataset comprising
$120$ documents, encompassing $7427$ non-comparative sentences and $2468$
comparisons within $1798$ sentences. Participating models undergo evaluation
and ranking based on the Exact match macro-averaged quintuple F1 score.</div><div><a href='http://arxiv.org/abs/2402.13613v2'>2402.13613v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.17840v1")'>Follow My Instruction and Spill the Beans: Scalable Data Extraction from
  Retrieval-Augmented Generation Systems</div>
<div id='2402.17840v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T19:08:05Z</div><div>Authors: Zhenting Qi, Hanlin Zhang, Eric Xing, Sham Kakade, Himabindu Lakkaraju</div><div style='padding-top: 10px; width: 80ex'>Retrieval-Augmented Generation (RAG) improves pre-trained models by
incorporating external knowledge at test time to enable customized adaptation.
We study the risk of datastore leakage in Retrieval-In-Context RAG Language
Models (LMs). We show that an adversary can exploit LMs' instruction-following
capabilities to easily extract text data verbatim from the datastore of RAG
systems built with instruction-tuned LMs via prompt injection. The
vulnerability exists for a wide range of modern LMs that span Llama2,
Mistral/Mixtral, Vicuna, SOLAR, WizardLM, Qwen1.5, and Platypus2, and the
exploitability exacerbates as the model size scales up. Extending our study to
production RAG models GPTs, we design an attack that can cause datastore
leakage with a 100% success rate on 25 randomly selected customized GPTs with
at most 2 queries, and we extract text data verbatim at a rate of 41% from a
book of 77,000 words and 3% from a corpus of 1,569,000 words by prompting the
GPTs with only 100 queries generated by themselves.</div><div><a href='http://arxiv.org/abs/2402.17840v1'>2402.17840v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.16827v2")'>A Survey on Data Selection for Language Models</div>
<div id='2402.16827v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T18:54:35Z</div><div>Authors: Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, Colin Raffel, Shiyu Chang, Tatsunori Hashimoto, William Yang Wang</div><div style='padding-top: 10px; width: 80ex'>A major factor in the recent success of large language models is the use of
enormous and ever-growing text datasets for unsupervised pre-training. However,
naively training a model on all available data may not be optimal (or
feasible), as the quality of available text data can vary. Filtering out data
can also decrease the carbon footprint and financial costs of training models
by reducing the amount of training required. Data selection methods aim to
determine which candidate data points to include in the training dataset and
how to appropriately sample from the selected data points. The promise of
improved data selection methods has caused the volume of research in the area
to rapidly expand. However, because deep learning is mostly driven by empirical
evidence and experimentation on large-scale data is expensive, few
organizations have the resources for extensive data selection research.
Consequently, knowledge of effective data selection practices has become
concentrated within a few organizations, many of which do not openly share
their findings and methodologies. To narrow this gap in knowledge, we present a
comprehensive review of existing literature on data selection methods and
related research areas, providing a taxonomy of existing approaches. By
describing the current landscape of research, this work aims to accelerate
progress in data selection by establishing an entry point for new and
established researchers. Additionally, throughout this review we draw attention
to noticeable holes in the literature and conclude the paper by proposing
promising avenues for future research.</div><div><a href='http://arxiv.org/abs/2402.16827v2'>2402.16827v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07792v1")'>Empowering Federated Learning for Massive Models with NVIDIA FLARE</div>
<div id='2402.07792v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T16:59:05Z</div><div>Authors: Holger R. Roth, Ziyue Xu, Yuan-Ting Hsieh, Adithya Renduchintala, Isaac Yang, Zhihong Zhang, Yuhong Wen, Sean Yang, Kevin Lu, Kristopher Kersten, Camir Ricketts, Daguang Xu, Chester Chen, Yan Cheng, Andrew Feng</div><div style='padding-top: 10px; width: 80ex'>In the ever-evolving landscape of artificial intelligence (AI) and large
language models (LLMs), handling and leveraging data effectively has become a
critical challenge. Most state-of-the-art machine learning algorithms are
data-centric. However, as the lifeblood of model performance, necessary data
cannot always be centralized due to various factors such as privacy,
regulation, geopolitics, copyright issues, and the sheer effort required to
move vast datasets. In this paper, we explore how federated learning enabled by
NVIDIA FLARE can address these challenges with easy and scalable integration
capabilities, enabling parameter-efficient and full supervised fine-tuning of
LLMs for natural language processing and biopharmaceutical applications to
enhance their accuracy and robustness.</div><div><a href='http://arxiv.org/abs/2402.07792v1'>2402.07792v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15613v1")'>Towards Efficient Active Learning in NLP via Pretrained Representations</div>
<div id='2402.15613v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T21:28:59Z</div><div>Authors: Artem Vysogorets, Achintya Gopal</div><div style='padding-top: 10px; width: 80ex'>Fine-tuning Large Language Models (LLMs) is now a common approach for text
classification in a wide range of applications. When labeled documents are
scarce, active learning helps save annotation efforts but requires retraining
of massive models on each acquisition iteration. We drastically expedite this
process by using pretrained representations of LLMs within the active learning
loop and, once the desired amount of labeled data is acquired, fine-tuning that
or even a different pretrained LLM on this labeled data to achieve the best
performance. As verified on common text classification benchmarks with
pretrained BERT and RoBERTa as the backbone, our strategy yields similar
performance to fine-tuning all the way through the active learning loop but is
orders of magnitude less computationally expensive. The data acquired with our
procedure generalizes across pretrained networks, allowing flexibility in
choosing the final model or updating it as newer versions get released.</div><div><a href='http://arxiv.org/abs/2402.15613v1'>2402.15613v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12918v1")'>Generalizable and Stable Finetuning of Pretrained Language Models on
  Low-Resource Texts</div>
<div id='2403.12918v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T17:21:29Z</div><div>Authors: Sai Ashish Somayajula, Youwei Liang, Abhishek Singh, Li Zhang, Pengtao Xie</div><div style='padding-top: 10px; width: 80ex'>Pretrained Language Models (PLMs) have advanced Natural Language Processing
(NLP) tasks significantly, but finetuning PLMs on low-resource datasets poses
significant challenges such as instability and overfitting. Previous methods
tackle these issues by finetuning a strategically chosen subnetwork on a
downstream task, while keeping the remaining weights fixed to the pretrained
weights. However, they rely on a suboptimal criteria for sub-network selection,
leading to suboptimal solutions. To address these limitations, we propose a
regularization method based on attention-guided weight mixup for finetuning
PLMs. Our approach represents each network weight as a mixup of task-specific
weight and pretrained weight, controlled by a learnable attention parameter,
providing finer control over sub-network selection. Furthermore, we employ a
bi-level optimization (BLO) based framework on two separate splits of the
training dataset, improving generalization and combating overfitting. We
validate the efficacy of our proposed method through extensive experiments,
demonstrating its superiority over previous methods, particularly in the
context of finetuning PLMs on low-resource datasets.</div><div><a href='http://arxiv.org/abs/2403.12918v1'>2403.12918v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.14086v1")'>LexC-Gen: Generating Data for Extremely Low-Resource Languages with
  Large Language Models and Bilingual Lexicons</div>
<div id='2402.14086v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T19:20:06Z</div><div>Authors: Zheng-Xin Yong, Cristina Menghini, Stephen H. Bach</div><div style='padding-top: 10px; width: 80ex'>Data scarcity in low-resource languages can be addressed with word-to-word
translations from labeled task data in high-resource languages using bilingual
lexicons. However, bilingual lexicons often have limited lexical overlap with
task data, which results in poor translation coverage and lexicon utilization.
We propose lexicon-conditioned data generation (LexC-Gen), a method that
generates low-resource-language classification task data at scale.
Specifically, LexC-Gen first uses high-resource-language words from bilingual
lexicons to generate lexicon-compatible task data, and then it translates them
into low-resource languages with bilingual lexicons via word translation.
Across 17 extremely low-resource languages, LexC-Gen generated data is
competitive with expert-translated gold data, and yields on average 5.6 and 8.9
points improvement over existing lexicon-based word translation methods on
sentiment analysis and topic classification tasks respectively. We show that
conditioning on bilingual lexicons is the key component of LexC-Gen. LexC-Gen
is also practical -- it only needs a single GPU to generate data at scale. It
works well with open-access LLMs, and its cost is one-fifth of the cost of
GPT4-based multilingual data generation.</div><div><a href='http://arxiv.org/abs/2402.14086v1'>2402.14086v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12730v1")'>UMBCLU at SemEval-2024 Task 1A and 1C: Semantic Textual Relatedness with
  and without machine translation</div>
<div id='2402.12730v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T05:46:29Z</div><div>Authors: Shubhashis Roy Dipta, Sai Vallurupalli</div><div style='padding-top: 10px; width: 80ex'>This paper describes the system we developed for SemEval-2024 Task 1,
"Semantic Textual Relatedness for African and Asian Languages." The aim of the
task is to build a model that can identify semantic textual relatedness (STR)
between two sentences of a target language belonging to a collection of African
and Asian languages. We participated in Subtasks A and C and explored
supervised and cross-lingual training leveraging large language models (LLMs).
Pre-trained large language models have been extensively used for machine
translation and semantic similarity. Using a combination of machine translation
and sentence embedding LLMs, we developed a unified STR model, TranSem, for
subtask A and fine-tuned the T5 family of models on the STR data, FineSem, for
use in subtask C. Our model results for 7 languages in subtask A were better
than the official baseline for 3 languages and on par with the baseline for the
remaining 4 languages. Our model results for the 12 languages in subtask C
resulted in 1st place for Africaans, 2nd place for Indonesian, and 3rd place
for English with low performance for the remaining 9 languages.</div><div><a href='http://arxiv.org/abs/2402.12730v1'>2402.12730v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03053v1")'>Multi-Lingual Malaysian Embedding: Leveraging Large Language Models for
  Semantic Representations</div>
<div id='2402.03053v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T14:36:51Z</div><div>Authors: Husein Zolkepli, Aisyah Razak, Kamarul Adha, Ariff Nazhan</div><div style='padding-top: 10px; width: 80ex'>In this work, we present a comprehensive exploration of finetuning Malaysian
language models, specifically Llama2 and Mistral, on embedding tasks involving
negative and positive pairs. We release two distinct models tailored for
Semantic Similarity and Retrieval-Augmented Generation (RAG).
  For Semantic Similarity, our 600 million parameter Llama2 model outperforms
OpenAI text-embedding-ada-002 across all recall@k metrics for b.cari.com.my,
c.cari.com.my, Malay news, and Malaysian Twitter test sets.
  In the realm of RAG models, our approach proves competitive with OpenAI
text-embedding-ada-002 in the Malaysian context. Notably, our 2 billion
parameter Llama2 model achieves superior Recall@5, Recall@10 for the "Melayu"
keyword research papers dataset and excels in Recall@3, Recall@5, and Recall@10
for the lom.agc.gov.my dataset.
  These findings underscore the effectiveness of our finetuning strategy and
highlight the performance gains in both Semantic Similarity and RAG tasks.
  All models released at
https://huggingface.co/collections/mesolitica/malaysian-embedding-6523612bfe5881ad35f81b99</div><div><a href='http://arxiv.org/abs/2402.03053v1'>2402.03053v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11082v1")'>RobustSentEmbed: Robust Sentence Embeddings Using Adversarial
  Self-Supervised Contrastive Learning</div>
<div id='2403.11082v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-17T04:29:45Z</div><div>Authors: Javad Rafiei Asl, Prajwal Panzade, Eduardo Blanco, Daniel Takabi, Zhipeng Cai</div><div style='padding-top: 10px; width: 80ex'>Pre-trained language models (PLMs) have consistently demonstrated outstanding
performance across a diverse spectrum of natural language processing tasks.
Nevertheless, despite their success with unseen data, current PLM-based
representations often exhibit poor robustness in adversarial settings. In this
paper, we introduce RobustSentEmbed, a self-supervised sentence embedding
framework designed to improve both generalization and robustness in diverse
text representation tasks and against a diverse set of adversarial attacks.
Through the generation of high-risk adversarial perturbations and their
utilization in a novel objective function, RobustSentEmbed adeptly learns
high-quality and robust sentence embeddings. Our experiments confirm the
superiority of RobustSentEmbed over state-of-the-art representations.
Specifically, Our framework achieves a significant reduction in the success
rate of various adversarial attacks, notably reducing the BERTAttack success
rate by almost half (from 75.51\% to 38.81\%). The framework also yields
improvements of 1.59\% and 0.23\% in semantic textual similarity tasks and
various transfer tasks, respectively.</div><div><a href='http://arxiv.org/abs/2403.11082v1'>2403.11082v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12890v1")'>More Discriminative Sentence Embeddings via Semantic Graph Smoothing</div>
<div id='2402.12890v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T10:34:19Z</div><div>Authors: Chakib Fettal, Lazhar Labiod, Mohamed Nadif</div><div style='padding-top: 10px; width: 80ex'>This paper explores an empirical approach to learn more discriminantive
sentence representations in an unsupervised fashion. Leveraging semantic graph
smoothing, we enhance sentence embeddings obtained from pretrained models to
improve results for the text clustering and classification tasks. Our method,
validated on eight benchmarks, demonstrates consistent improvements, showcasing
the potential of semantic graph smoothing in improving sentence embeddings for
the supervised and unsupervised document categorization tasks.</div><div><a href='http://arxiv.org/abs/2402.12890v1'>2402.12890v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.15112v1")'>Text clustering with LLM embeddings</div>
<div id='2403.15112v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T11:08:48Z</div><div>Authors: Alina Petukhova, Joao P. Matos-Carvalho, Nuno Fachada</div><div style='padding-top: 10px; width: 80ex'>Text clustering is an important approach for organising the growing amount of
digital content, helping to structure and find hidden patterns in uncategorised
data. In this research, we investigated how different textual embeddings -
particularly those used in large language models (LLMs) - and clustering
algorithms affect how text datasets are clustered. A series of experiments were
conducted to assess how embeddings influence clustering results, the role
played by dimensionality reduction through summarisation, and embedding size
adjustment. Results reveal that LLM embeddings excel at capturing the nuances
of structured language, while BERT leads the lightweight options in
performance. In addition, we find that increasing embedding dimensionality and
summarisation techniques do not uniformly improve clustering efficiency,
suggesting that these strategies require careful analysis to use in real-life
models. These results highlight a complex balance between the need for nuanced
text representation and computational feasibility in text clustering
applications. This study extends traditional text clustering frameworks by
incorporating embeddings from LLMs, thereby paving the way for improved
methodologies and opening new avenues for future research in various types of
textual analysis.</div><div><a href='http://arxiv.org/abs/2403.15112v1'>2403.15112v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.00804v1")'>Uncovering Customer Issues through Topological Natural Language Analysis</div>
<div id='2403.00804v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-24T00:15:09Z</div><div>Authors: Shu-Ting Pi, Sidarth Srinivasan, Yuying Zhu, Michael Yang, Qun Liu</div><div style='padding-top: 10px; width: 80ex'>E-commerce companies deal with a high volume of customer service requests
daily. While a simple annotation system is often used to summarize the topics
of customer contacts, thoroughly exploring each specific issue can be
challenging. This presents a critical concern, especially during an emerging
outbreak where companies must quickly identify and address specific issues. To
tackle this challenge, we propose a novel machine learning algorithm that
leverages natural language techniques and topological data analysis to monitor
emerging and trending customer issues. Our approach involves an end-to-end deep
learning framework that simultaneously tags the primary question sentence of
each customer's transcript and generates sentence embedding vectors. We then
whiten the embedding vectors and use them to construct an undirected graph.
From there, we define trending and emerging issues based on the topological
properties of each transcript. We have validated our results through various
methods and found that they are highly consistent with news sources.</div><div><a href='http://arxiv.org/abs/2403.00804v1'>2403.00804v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12520v1")'>Key Information Retrieval to Classify the Unstructured Data Content of
  Preferential Trade Agreements</div>
<div id='2401.12520v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T06:30:05Z</div><div>Authors: Jiahui Zhao, Ziyi Meng, Stepan Gordeev, Zijie Pan, Dongjin Song, Sandro Steinbach, Caiwen Ding</div><div style='padding-top: 10px; width: 80ex'>With the rapid proliferation of textual data, predicting long texts has
emerged as a significant challenge in the domain of natural language
processing. Traditional text prediction methods encounter substantial
difficulties when grappling with long texts, primarily due to the presence of
redundant and irrelevant information, which impedes the model's capacity to
capture pivotal insights from the text. To address this issue, we introduce a
novel approach to long-text classification and prediction. Initially, we employ
embedding techniques to condense the long texts, aiming to diminish the
redundancy therein. Subsequently,the Bidirectional Encoder Representations from
Transformers (BERT) embedding method is utilized for text classification
training. Experimental outcomes indicate that our method realizes considerable
performance enhancements in classifying long texts of Preferential Trade
Agreements. Furthermore, the condensation of text through embedding methods not
only augments prediction accuracy but also substantially reduces computational
complexity. Overall, this paper presents a strategy for long-text prediction,
offering a valuable reference for researchers and engineers in the natural
language processing sphere.</div><div><a href='http://arxiv.org/abs/2401.12520v1'>2401.12520v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14208v2")'>Content Conditional Debiasing for Fair Text Embedding</div>
<div id='2402.14208v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T01:20:51Z</div><div>Authors: Wenlong Deng, Blair Chen, Xiaoxiao Li, Christos Thrampoulidis</div><div style='padding-top: 10px; width: 80ex'>Mitigating biases in machine learning models has gained increasing attention
in Natural Language Processing (NLP). Yet, only a few studies focus on fair
text embeddings, which are crucial yet challenging for real-world applications.
In this paper, we propose a novel method for learning fair text embeddings. We
achieve fairness while maintaining utility trade-off by ensuring conditional
independence between sensitive attributes and text embeddings conditioned on
the content. Specifically, we enforce that embeddings of texts with different
sensitive attributes but identical content maintain the same distance toward
the embedding of their corresponding neutral text. Furthermore, we address the
issue of lacking proper training data by using Large Language Models (LLMs) to
augment texts into different sensitive groups. Our extensive evaluations
demonstrate that our approach effectively improves fairness while preserving
the utility of embeddings, representing a pioneering effort in achieving
conditional independence for fair text embeddings.</div><div><a href='http://arxiv.org/abs/2402.14208v2'>2402.14208v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15132v1")'>Improving Sentence Embeddings with an Automatically Generated NLI
  Dataset</div>
<div id='2402.15132v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T06:33:51Z</div><div>Authors: Soma Sato, Hayato Tsukagoshi, Ryohei Sasano, Koichi Takeda</div><div style='padding-top: 10px; width: 80ex'>Decoder-based large language models (LLMs) have shown high performance on
many tasks in natural language processing. This is also true for sentence
embedding learning, where a decoder-based model, PromptEOL, has achieved the
best performance on semantic textual similarity (STS) tasks. However, PromptEOL
makes great use of fine-tuning with a manually annotated natural language
inference (NLI) dataset. We aim to improve sentence embeddings learned in an
unsupervised setting by automatically generating an NLI dataset with an LLM and
using it to fine-tune PromptEOL. In experiments on STS tasks, the proposed
method achieved an average Spearman's rank correlation coefficient of 82.21
with respect to human evaluation, thus outperforming existing methods without
using large, manually annotated datasets.</div><div><a href='http://arxiv.org/abs/2402.15132v1'>2402.15132v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14776v1")'>2D Matryoshka Sentence Embeddings</div>
<div id='2402.14776v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T18:35:05Z</div><div>Authors: Xianming Li, Zongxi Li, Jing Li, Haoran Xie, Qing Li</div><div style='padding-top: 10px; width: 80ex'>Common approaches rely on fixed-length embedding vectors from language models
as sentence embeddings for downstream tasks such as semantic textual similarity
(STS). Such methods are limited in their flexibility due to unknown
computational constraints and budgets across various applications. Matryoshka
Representation Learning (MRL) (Kusupati et al., 2022) encodes information at
finer granularities, i.e., with lower embedding dimensions, to adaptively
accommodate ad hoc tasks. Similar accuracy can be achieved with a smaller
embedding size, leading to speedups in downstream tasks. Despite its improved
efficiency, MRL still requires traversing all Transformer layers before
obtaining the embedding, which remains the dominant factor in time and memory
consumption. This prompts consideration of whether the fixed number of
Transformer layers affects representation quality and whether using
intermediate layers for sentence representation is feasible. In this paper, we
introduce a novel sentence embedding model called Two-dimensional Matryoshka
Sentence Embedding (2DMSE). It supports elastic settings for both embedding
sizes and Transformer layers, offering greater flexibility and efficiency than
MRL. We conduct extensive experiments on STS tasks and downstream applications.
The experimental results demonstrate the effectiveness of our proposed model in
dynamically supporting different embedding sizes and Transformer layers,
allowing it to be highly adaptable to various scenarios.</div><div><a href='http://arxiv.org/abs/2402.14776v1'>2402.14776v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15153v1")'>Self-Adaptive Reconstruction with Contrastive Learning for Unsupervised
  Sentence Embeddings</div>
<div id='2402.15153v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T07:28:31Z</div><div>Authors: Junlong Liu, Xichen Shang, Huawen Feng, Junhao Zheng, Qianli Ma</div><div style='padding-top: 10px; width: 80ex'>Unsupervised sentence embeddings task aims to convert sentences to semantic
vector representations. Most previous works directly use the sentence
representations derived from pretrained language models. However, due to the
token bias in pretrained language models, the models can not capture the
fine-grained semantics in sentences, which leads to poor predictions. To
address this issue, we propose a novel Self-Adaptive Reconstruction Contrastive
Sentence Embeddings (SARCSE) framework, which reconstructs all tokens in
sentences with an AutoEncoder to help the model to preserve more fine-grained
semantics during tokens aggregating. In addition, we proposed a self-adaptive
reconstruction loss to alleviate the token bias towards frequency. Experimental
results show that SARCSE gains significant improvements compared with the
strong baseline SimCSE on the 7 STS tasks.</div><div><a href='http://arxiv.org/abs/2402.15153v1'>2402.15153v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.02254v1")'>L3Cube-IndicNews: News-based Short Text and Long Document Classification
  Datasets in Indic Languages</div>
<div id='2401.02254v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T13:11:17Z</div><div>Authors: Aishwarya Mirashi, Srushti Sonavane, Purva Lingayat, Tejas Padhiyar, Raviraj Joshi</div><div style='padding-top: 10px; width: 80ex'>In this work, we introduce L3Cube-IndicNews, a multilingual text
classification corpus aimed at curating a high-quality dataset for Indian
regional languages, with a specific focus on news headlines and articles. We
have centered our work on 10 prominent Indic languages, including Hindi,
Bengali, Marathi, Telugu, Tamil, Gujarati, Kannada, Odia, Malayalam, and
Punjabi. Each of these news datasets comprises 10 or more classes of news
articles. L3Cube-IndicNews offers 3 distinct datasets tailored to handle
different document lengths that are classified as: Short Headlines
Classification (SHC) dataset containing the news headline and news category,
Long Document Classification (LDC) dataset containing the whole news article
and the news category, and Long Paragraph Classification (LPC) containing
sub-articles of the news and the news category. We maintain consistent labeling
across all 3 datasets for in-depth length-based analysis. We evaluate each of
these Indic language datasets using 4 different models including monolingual
BERT, multilingual Indic Sentence BERT (IndicSBERT), and IndicBERT. This
research contributes significantly to expanding the pool of available text
classification datasets and also makes it possible to develop topic
classification models for Indian regional languages. This also serves as an
excellent resource for cross-lingual analysis owing to the high overlap of
labels among languages. The datasets and models are shared publicly at
https://github.com/l3cube-pune/indic-nlp</div><div><a href='http://arxiv.org/abs/2401.02254v1'>2401.02254v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.09333v2")'>Machines Do See Color: A Guideline to Classify Different Forms of Racist
  Discourse in Large Corpora</div>
<div id='2401.09333v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T16:57:18Z</div><div>Authors: Diana Davila Gordillo, Joan Timoneda, Sebastian Vallejo Vera</div><div style='padding-top: 10px; width: 80ex'>Current methods to identify and classify racist language in text rely on
small-n qualitative approaches or large-n approaches focusing exclusively on
overt forms of racist discourse. This article provides a step-by-step
generalizable guideline to identify and classify different forms of racist
discourse in large corpora. In our approach, we start by conceptualizing racism
and its different manifestations. We then contextualize these racist
manifestations to the time and place of interest, which allows researchers to
identify their discursive form. Finally, we apply XLM-RoBERTa (XLM-R), a
cross-lingual model for supervised text classification with a cutting-edge
contextual understanding of text. We show that XLM-R and XLM-R-Racismo, our
pretrained model, outperform other state-of-the-art approaches in classifying
racism in large corpora. We illustrate our approach using a corpus of tweets
relating to the Ecuadorian ind\'igena community between 2018 and 2021.</div><div><a href='http://arxiv.org/abs/2401.09333v2'>2401.09333v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03642v1")'>Stanceosaurus 2.0: Classifying Stance Towards Russian and Spanish
  Misinformation</div>
<div id='2402.03642v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-06T02:39:59Z</div><div>Authors: Anton Lavrouk, Ian Ligon, Tarek Naous, Jonathan Zheng, Alan Ritter, Wei Xu</div><div style='padding-top: 10px; width: 80ex'>The Stanceosaurus corpus (Zheng et al., 2022) was designed to provide
high-quality, annotated, 5-way stance data extracted from Twitter, suitable for
analyzing cross-cultural and cross-lingual misinformation. In the Stanceosaurus
2.0 iteration, we extend this framework to encompass Russian and Spanish. The
former is of current significance due to prevalent misinformation amid
escalating tensions with the West and the violent incursion into Ukraine. The
latter, meanwhile, represents an enormous community that has been largely
overlooked on major social media platforms. By incorporating an additional
3,874 Spanish and Russian tweets over 41 misinformation claims, our objective
is to support research focused on these issues. To demonstrate the value of
this data, we employed zero-shot cross-lingual transfer on multilingual BERT,
yielding results on par with the initial Stanceosaurus study with a macro F1
score of 43 for both languages. This underlines the viability of stance
classification as an effective tool for identifying multicultural
misinformation.</div><div><a href='http://arxiv.org/abs/2402.03642v1'>2402.03642v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00236v1")'>Benchmarking zero-shot stance detection with FlanT5-XXL: Insights from
  training data, prompting, and decoding strategies into its near-SoTA
  performance</div>
<div id='2403.00236v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T02:33:26Z</div><div>Authors: Rachith Aiyappa, Shruthi Senthilmani, Jisun An, Haewoon Kwak, Yong-Yeol Ahn</div><div style='padding-top: 10px; width: 80ex'>We investigate the performance of LLM-based zero-shot stance detection on
tweets. Using FlanT5-XXL, an instruction-tuned open-source LLM, with the
SemEval 2016 Tasks 6A, 6B, and P-Stance datasets, we study the performance and
its variations under different prompts and decoding strategies, as well as the
potential biases of the model. We show that the zero-shot approach can match or
outperform state-of-the-art benchmarks, including fine-tuned models. We provide
various insights into its performance including the sensitivity to instructions
and prompts, the decoding strategies, the perplexity of the prompts, and to
negations and oppositions present in prompts. Finally, we ensure that the LLM
has not been trained on test datasets, and identify a positivity bias which may
partially explain the performance differences across decoding strategie</div><div><a href='http://arxiv.org/abs/2403.00236v1'>2403.00236v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03171v1")'>Homograph Attacks on Maghreb Sentiment Analyzers</div>
<div id='2402.03171v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T16:39:15Z</div><div>Authors: Fatima Zahra Qachfar, Rakesh M. Verma</div><div style='padding-top: 10px; width: 80ex'>We examine the impact of homograph attacks on the Sentiment Analysis (SA)
task of different Arabic dialects from the Maghreb North-African countries.
Homograph attacks result in a 65.3% decrease in transformer classification from
an F1-score of 0.95 to 0.33 when data is written in "Arabizi". The goal of this
study is to highlight LLMs weaknesses' and to prioritize ethical and
responsible Machine Learning.</div><div><a href='http://arxiv.org/abs/2402.03171v1'>2402.03171v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.13398v1")'>Text Categorization Can Enhance Domain-Agnostic Stopword Extraction</div>
<div id='2401.13398v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T11:52:05Z</div><div>Authors: Houcemeddine Turki, Naome A. Etori, Mohamed Ali Hadj Taieb, Abdul-Hakeem Omotayo, Chris Chinenye Emezue, Mohamed Ben Aouicha, Ayodele Awokoya, Falalu Ibrahim Lawan, Doreen Nixdorf</div><div style='padding-top: 10px; width: 80ex'>This paper investigates the role of text categorization in streamlining
stopword extraction in natural language processing (NLP), specifically focusing
on nine African languages alongside French. By leveraging the MasakhaNEWS,
African Stopwords Project, and MasakhaPOS datasets, our findings emphasize that
text categorization effectively identifies domain-agnostic stopwords with over
80% detection success rate for most examined languages. Nevertheless,
linguistic variances result in lower detection rates for certain languages.
Interestingly, we find that while over 40% of stopwords are common across news
categories, less than 15% are unique to a single category. Uncommon stopwords
add depth to text but their classification as stopwords depends on context.
Therefore combining statistical and linguistic approaches creates comprehensive
stopword lists, highlighting the value of our hybrid method. This research
enhances NLP for African languages and underscores the importance of text
categorization in stopword extraction.</div><div><a href='http://arxiv.org/abs/2401.13398v1'>2401.13398v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13085v1")'>IndiText Boost: Text Augmentation for Low Resource India Languages</div>
<div id='2401.13085v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T20:54:40Z</div><div>Authors: Onkar Litake, Niraj Yagnik, Shreyas Labhsetwar</div><div style='padding-top: 10px; width: 80ex'>Text Augmentation is an important task for low-resource languages. It helps
deal with the problem of data scarcity. A data augmentation strategy is used to
deal with the problem of data scarcity. Through the years, much work has been
done on data augmentation for the English language. In contrast, very less work
has been done on Indian languages. This is contrary to the fact that data
augmentation is used to deal with data scarcity. In this work, we focus on
implementing techniques like Easy Data Augmentation, Back Translation,
Paraphrasing, Text Generation using LLMs, and Text Expansion using LLMs for
text classification on different languages. We focus on 6 Indian languages
namely: Sindhi, Marathi, Hindi, Gujarati, Telugu, and Sanskrit. According to
our knowledge, no such work exists for text augmentation on Indian languages.
We carry out binary as well as multi-class text classification to make our
results more comparable. We get surprising results as basic data augmentation
techniques surpass LLMs.</div><div><a href='http://arxiv.org/abs/2401.13085v1'>2401.13085v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.00698v1")'>Large Language Models aren't all that you need</div>
<div id='2401.00698v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-01T08:32:50Z</div><div>Authors: Kiran Voderhobli Holla, Chaithanya Kumar, Aryan Singh</div><div style='padding-top: 10px; width: 80ex'>This paper describes the architecture and systems built towards solving the
SemEval 2023 Task 2: MultiCoNER II (Multilingual Complex Named Entity
Recognition) [1]. We evaluate two approaches (a) a traditional Conditional
Random Fields model and (b) a Large Language Model (LLM) fine-tuned with a
customized head and compare the two approaches. The novel ideas explored are:
1) Decaying auxiliary loss (with residual) - where we train the model on an
auxiliary task of Coarse-Grained NER and include this task as a part of the
loss function 2) Triplet token blending - where we explore ways of blending the
embeddings of neighboring tokens in the final NER layer prior to prediction 3)
Task-optimal heads - where we explore a variety of custom heads and learning
rates for the final layer of the LLM. We also explore multiple LLMs including
GPT-3 and experiment with a variety of dropout and other hyperparameter
settings before arriving at our final model which achieves micro &amp; macro f1 of
0.85/0.84 (on dev) and 0.67/0.61 on the test data . We show that while
pre-trained LLMs, by themselves, bring about a large improvement in scores as
compared to traditional models, we also demonstrate that tangible improvements
to the Macro-F1 score can be made by augmenting the LLM with additional
feature/loss/model engineering techniques described above.</div><div><a href='http://arxiv.org/abs/2401.00698v1'>2401.00698v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15343v1")'>NuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data</div>
<div id='2402.15343v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T14:23:51Z</div><div>Authors: Sergei Bogdanov, Alexandre Constantin, Timothée Bernard, Benoit Crabbé, Etienne Bernard</div><div style='padding-top: 10px; width: 80ex'>Large Language Models (LLMs) have shown impressive abilities in data
annotation, opening the way for new approaches to solve classic NLP problems.
In this paper, we show how to use LLMs to create NuNER, a compact language
representation model specialized in the Named Entity Recognition (NER) task.
NuNER can be fine-tuned to solve downstream NER problems in a data-efficient
way, outperforming similar-sized foundation models in the few-shot regime and
competing with much larger LLMs. We find that the size and entity-type
diversity of the pre-training dataset are key to achieving good performance. We
view NuNER as a member of the broader family of task-specific foundation
models, recently unlocked by LLMs.</div><div><a href='http://arxiv.org/abs/2402.15343v1'>2402.15343v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11103v1")'>ProgGen: Generating Named Entity Recognition Datasets Step-by-step with
  Self-Reflexive Large Language Models</div>
<div id='2403.11103v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-17T06:12:43Z</div><div>Authors: Yuzhao Heng, Chunyuan Deng, Yitong Li, Yue Yu, Yinghao Li, Rongzhi Zhang, Chao Zhang</div><div style='padding-top: 10px; width: 80ex'>Although Large Language Models (LLMs) exhibit remarkable adaptability across
domains, these models often fall short in structured knowledge extraction tasks
such as named entity recognition (NER). This paper explores an innovative,
cost-efficient strategy to harness LLMs with modest NER capabilities for
producing superior NER datasets. Our approach diverges from the basic
class-conditional prompts by instructing LLMs to self-reflect on the specific
domain, thereby generating domain-relevant attributes (such as category and
emotions for movie reviews), which are utilized for creating attribute-rich
training data. Furthermore, we preemptively generate entity terms and then
develop NER context data around these entities, effectively bypassing the LLMs'
challenges with complex structures. Our experiments across both general and
niche domains reveal significant performance enhancements over conventional
data generation methods while being more cost-effective than existing
alternatives.</div><div><a href='http://arxiv.org/abs/2403.11103v1'>2403.11103v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10825v1")'>A survey on recent advances in named entity recognition</div>
<div id='2401.10825v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T17:21:05Z</div><div>Authors: Imed Keraghel, Stanislas Morbieu, Mohamed Nadif</div><div style='padding-top: 10px; width: 80ex'>Named Entity Recognition seeks to extract substrings within a text that name
real-world objects and to determine their type (for example, whether they refer
to persons or organizations). In this survey, we first present an overview of
recent popular approaches, but we also look at graph- and transformer- based
methods including Large Language Models (LLMs) that have not had much coverage
in other surveys. Second, we focus on methods designed for datasets with scarce
annotations. Third, we evaluate the performance of the main NER implementations
on a variety of datasets with differing characteristics (as regards their
domain, their size, and their number of classes). We thus provide a deep
comparison of algorithms that are never considered together. Our experiments
shed some light on how the characteristics of datasets affect the behavior of
the methods that we compare.</div><div><a href='http://arxiv.org/abs/2401.10825v1'>2401.10825v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01439v1")'>From Words to Molecules: A Survey of Large Language Models in Chemistry</div>
<div id='2402.01439v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T14:30:48Z</div><div>Authors: Chang Liao, Yemin Yu, Yu Mei, Ying Wei</div><div style='padding-top: 10px; width: 80ex'>In recent years, Large Language Models (LLMs) have achieved significant
success in natural language processing (NLP) and various interdisciplinary
areas. However, applying LLMs to chemistry is a complex task that requires
specialized domain knowledge. This paper provides a thorough exploration of the
nuanced methodologies employed in integrating LLMs into the field of chemistry,
delving into the complexities and innovations at this interdisciplinary
juncture. Specifically, our analysis begins with examining how molecular
information is fed into LLMs through various representation and tokenization
methods. We then categorize chemical LLMs into three distinct groups based on
the domain and modality of their input data, and discuss approaches for
integrating these inputs for LLMs. Furthermore, this paper delves into the
pretraining objectives with adaptations to chemical LLMs. After that, we
explore the diverse applications of LLMs in chemistry, including novel
paradigms for their application in chemistry tasks. Finally, we identify
promising research directions, including further integration with chemical
knowledge, advancements in continual learning, and improvements in model
interpretability, paving the way for groundbreaking developments in the field.</div><div><a href='http://arxiv.org/abs/2402.01439v1'>2402.01439v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05075v1")'>Benchmarking Large Language Models for Molecule Prediction Tasks</div>
<div id='2403.05075v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T05:59:56Z</div><div>Authors: Zhiqiang Zhong, Kuangyu Zhou, Davide Mottin</div><div style='padding-top: 10px; width: 80ex'>Large Language Models (LLMs) stand at the forefront of a number of Natural
Language Processing (NLP) tasks. Despite the widespread adoption of LLMs in
NLP, much of their potential in broader fields remains largely unexplored, and
significant limitations persist in their design and implementation. Notably,
LLMs struggle with structured data, such as graphs, and often falter when
tasked with answering domain-specific questions requiring deep expertise, such
as those in biology and chemistry. In this paper, we explore a fundamental
question: Can LLMs effectively handle molecule prediction tasks? Rather than
pursuing top-tier performance, our goal is to assess how LLMs can contribute to
diverse molecule tasks. We identify several classification and regression
prediction tasks across six standard molecule datasets. Subsequently, we
carefully design a set of prompts to query LLMs on these tasks and compare
their performance with existing Machine Learning (ML) models, which include
text-based models and those specifically designed for analysing the geometric
structure of molecules. Our investigation reveals several key insights:
Firstly, LLMs generally lag behind ML models in achieving competitive
performance on molecule tasks, particularly when compared to models adept at
capturing the geometric structure of molecules, highlighting the constrained
ability of LLMs to comprehend graph data. Secondly, LLMs show promise in
enhancing the performance of ML models when used collaboratively. Lastly, we
engage in a discourse regarding the challenges and promising avenues to harness
LLMs for molecule prediction tasks. The code and models are available at
https://github.com/zhiqiangzhongddu/LLMaMol.</div><div><a href='http://arxiv.org/abs/2403.05075v1'>2403.05075v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13830v1")'>Bridging Text and Molecule: A Survey on Multimodal Frameworks for
  Molecule</div>
<div id='2403.13830v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T03:03:13Z</div><div>Authors: Yi Xiao, Xiangxin Zhou, Qiang Liu, Liang Wang</div><div style='padding-top: 10px; width: 80ex'>Artificial intelligence has demonstrated immense potential in scientific
research. Within molecular science, it is revolutionizing the traditional
computer-aided paradigm, ushering in a new era of deep learning. With recent
progress in multimodal learning and natural language processing, an emerging
trend has targeted at building multimodal frameworks to jointly model molecules
with textual domain knowledge. In this paper, we present the first systematic
survey on multimodal frameworks for molecules research. Specifically,we begin
with the development of molecular deep learning and point out the necessity to
involve textual modality. Next, we focus on recent advances in text-molecule
alignment methods, categorizing current models into two groups based on their
architectures and listing relevant pre-training tasks. Furthermore, we delves
into the utilization of large language models and prompting techniques for
molecular tasks and present significant applications in drug discovery.
Finally, we discuss the limitations in this field and highlight several
promising directions for future research.</div><div><a href='http://arxiv.org/abs/2403.13830v1'>2403.13830v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.14184v1")'>Diversity-Aware Ensembling of Language Models Based on Topological Data
  Analysis</div>
<div id='2402.14184v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T00:04:21Z</div><div>Authors: Polina Proskura, Alexey Zaytsev</div><div style='padding-top: 10px; width: 80ex'>Ensembles are important tools for improving the performance of machine
learning models. In cases related to natural language processing, ensembles
boost the performance of a method due to multiple large models available in
open source. However, existing approaches mostly rely on simple averaging of
predictions by ensembles with equal weights for each model, ignoring
differences in the quality and conformity of models. We propose to estimate
weights for ensembles of NLP models using not only knowledge of their
individual performance but also their similarity to each other. By adopting
distance measures based on Topological Data Analysis (TDA), we improve our
ensemble. The quality improves for both text classification accuracy and
relevant uncertainty estimation.</div><div><a href='http://arxiv.org/abs/2402.14184v1'>2402.14184v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01600v1")'>PLLaMa: An Open-source Large Language Model for Plant Science</div>
<div id='2401.01600v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T08:06:26Z</div><div>Authors: Xianjun Yang, Junfeng Gao, Wenxin Xue, Erik Alexandersson</div><div style='padding-top: 10px; width: 80ex'>Large Language Models (LLMs) have exhibited remarkable capabilities in
understanding and interacting with natural language across various sectors.
However, their effectiveness is limited in specialized areas requiring high
accuracy, such as plant science, due to a lack of specific expertise in these
fields. This paper introduces PLLaMa, an open-source language model that
evolved from LLaMa-2. It's enhanced with a comprehensive database, comprising
more than 1.5 million scholarly articles in plant science. This development
significantly enriches PLLaMa with extensive knowledge and proficiency in plant
and agricultural sciences. Our initial tests, involving specific datasets
related to plants and agriculture, show that PLLaMa substantially improves its
understanding of plant science-related topics. Moreover, we have formed an
international panel of professionals, including plant scientists, agricultural
engineers, and plant breeders. This team plays a crucial role in verifying the
accuracy of PLLaMa's responses to various academic inquiries, ensuring its
effective and reliable application in the field. To support further research
and development, we have made the model's checkpoints and source codes
accessible to the scientific community. These resources are available for
download at \url{https://github.com/Xianjun-Yang/PLLaMa}.</div><div><a href='http://arxiv.org/abs/2401.01600v1'>2401.01600v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06817v1")'>Analyzing Regional Impacts of Climate Change using Natural Language
  Processing Techniques</div>
<div id='2401.06817v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T16:44:59Z</div><div>Authors: Tanwi Mallick, John Murphy, Joshua David Bergerson, Duane R. Verner, John K Hutchison, Leslie-Anne Levy</div><div style='padding-top: 10px; width: 80ex'>Understanding the multifaceted effects of climate change across diverse
geographic locations is crucial for timely adaptation and the development of
effective mitigation strategies. As the volume of scientific literature on this
topic continues to grow exponentially, manually reviewing these documents has
become an immensely challenging task. Utilizing Natural Language Processing
(NLP) techniques to analyze this wealth of information presents an efficient
and scalable solution. By gathering extensive amounts of peer-reviewed articles
and studies, we can extract and process critical information about the effects
of climate change in specific regions. We employ BERT (Bidirectional Encoder
Representations from Transformers) for Named Entity Recognition (NER), which
enables us to efficiently identify specific geographies within the climate
literature. This, in turn, facilitates location-specific analyses. We conduct
region-specific climate trend analyses to pinpoint the predominant themes or
concerns related to climate change within a particular area, trace the temporal
progression of these identified issues, and evaluate their frequency, severity,
and potential development over time. These in-depth examinations of
location-specific climate data enable the creation of more customized
policy-making, adaptation, and mitigation strategies, addressing each region's
unique challenges and providing more effective solutions rooted in data-driven
insights. This approach, founded on a thorough exploration of scientific texts,
offers actionable insights to a wide range of stakeholders, from policymakers
to engineers to environmentalists. By proactively understanding these impacts,
societies are better positioned to prepare, allocate resources wisely, and
design tailored strategies to cope with future climate conditions, ensuring a
more resilient future for all.</div><div><a href='http://arxiv.org/abs/2401.06817v1'>2401.06817v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.01724v1")'>CERM: Context-aware Literature-based Discovery via Sentiment Analysis</div>
<div id='2402.01724v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-27T06:40:08Z</div><div>Authors: Julio Christian Young, Uchenna Akujuobi</div><div style='padding-top: 10px; width: 80ex'>Driven by the abundance of biomedical publications, we introduce a sentiment
analysis task to understand food-health relationship. Prior attempts to
incorporate health into recipe recommendation and analysis systems have
primarily focused on ingredient nutritional components or utilized basic
computational models trained on curated labeled data. Enhanced models that
capture the inherent relationship between food ingredients and biomedical
concepts can be more beneficial for food-related research, given the wealth of
information in biomedical texts. Considering the costly data labeling process,
these models should effectively utilize both labeled and unlabeled data. This
paper introduces Entity Relationship Sentiment Analysis (ERSA), a new task that
captures the sentiment of a text based on an entity pair. ERSA extends the
widely studied Aspect Based Sentiment Analysis (ABSA) task. Specifically, our
study concentrates on the ERSA task applied to biomedical texts, focusing on
(entity-entity) pairs of biomedical and food concepts. ERSA poses a significant
challenge compared to traditional sentiment analysis tasks, as sentence
sentiment may not align with entity relationship sentiment. Additionally, we
propose CERM, a semi-supervised architecture that combines different word
embeddings to enhance the encoding of the ERSA task. Experimental results
showcase the model's efficiency across diverse learning scenarios.</div><div><a href='http://arxiv.org/abs/2402.01724v1'>2402.01724v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.13432v1")'>DrBenchmark: A Large Language Understanding Evaluation Benchmark for
  French Biomedical Domain</div>
<div id='2402.13432v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T23:54:02Z</div><div>Authors: Yanis Labrak, Adrien Bazoge, Oumaima El Khettari, Mickael Rouvier, Pacome Constant dit Beaufils, Natalia Grabar, Beatrice Daille, Solen Quiniou, Emmanuel Morin, Pierre-Antoine Gourraud, Richard Dufour</div><div style='padding-top: 10px; width: 80ex'>The biomedical domain has sparked a significant interest in the field of
Natural Language Processing (NLP), which has seen substantial advancements with
pre-trained language models (PLMs). However, comparing these models has proven
challenging due to variations in evaluation protocols across different models.
A fair solution is to aggregate diverse downstream tasks into a benchmark,
allowing for the assessment of intrinsic PLMs qualities from various
perspectives. Although still limited to few languages, this initiative has been
undertaken in the biomedical field, notably English and Chinese. This
limitation hampers the evaluation of the latest French biomedical models, as
they are either assessed on a minimal number of tasks with non-standardized
protocols or evaluated using general downstream tasks. To bridge this research
gap and account for the unique sensitivities of French, we present the
first-ever publicly available French biomedical language understanding
benchmark called DrBenchmark. It encompasses 20 diversified tasks, including
named-entity recognition, part-of-speech tagging, question-answering, semantic
textual similarity, and classification. We evaluate 8 state-of-the-art
pre-trained masked language models (MLMs) on general and biomedical-specific
data, as well as English specific MLMs to assess their cross-lingual
capabilities. Our experiments reveal that no single model excels across all
tasks, while generalist models are sometimes still competitive.</div><div><a href='http://arxiv.org/abs/2402.13432v1'>2402.13432v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00952v1")'>MediSwift: Efficient Sparse Pre-trained Biomedical Language Models</div>
<div id='2403.00952v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T20:03:44Z</div><div>Authors: Vithursan Thangarasa, Mahmoud Salem, Shreyas Saxena, Kevin Leong, Joel Hestness, Sean Lie</div><div style='padding-top: 10px; width: 80ex'>Large language models (LLMs) are typically trained on general source data for
various domains, but a recent surge in domain-specific LLMs has shown their
potential to outperform general-purpose models in domain-specific tasks (e.g.,
biomedicine). Although domain-specific pre-training enhances efficiency and
leads to smaller models, the computational costs of training these LLMs remain
high, posing budgeting challenges. We introduce MediSwift, a suite of
biomedical LMs that leverage sparse pre-training on domain-specific biomedical
text data. By inducing up to 75% weight sparsity during the pre-training phase,
MediSwift achieves a 2-2.5x reduction in training FLOPs. Notably, all sparse
pre-training was performed on the Cerebras CS-2 system, which is specifically
designed to realize the acceleration benefits from unstructured weight
sparsity, thereby significantly enhancing the efficiency of the MediSwift
models. Through subsequent dense fine-tuning and strategic soft prompting,
MediSwift models outperform existing LLMs up to 7B parameters on biomedical
tasks, setting new benchmarks w.r.t efficiency-accuracy on tasks such as
PubMedQA. Our results show that sparse pre-training, along with dense
fine-tuning and soft prompting, offers an effective method for creating
high-performing, computationally efficient models in specialized domains.</div><div><a href='http://arxiv.org/abs/2403.00952v1'>2403.00952v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04261v1")'>Advancing Biomedical Text Mining with Community Challenges</div>
<div id='2403.04261v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T06:52:51Z</div><div>Authors: Hui Zong, Rongrong Wu, Jiaxue Cha, Erman Wu, Jiakun Li, Liang Tao, Zuofeng Li, Buzhou Tang, Bairong Shen</div><div style='padding-top: 10px; width: 80ex'>The field of biomedical research has witnessed a significant increase in the
accumulation of vast amounts of textual data from various sources such as
scientific literatures, electronic health records, clinical trial reports, and
social media. However, manually processing and analyzing these extensive and
complex resources is time-consuming and inefficient. To address this challenge,
biomedical text mining, also known as biomedical natural language processing,
has garnered great attention. Community challenge evaluation competitions have
played an important role in promoting technology innovation and
interdisciplinary collaboration in biomedical text mining research. These
challenges provide platforms for researchers to develop state-of-the-art
solutions for data mining and information processing in biomedical research. In
this article, we review the recent advances in community challenges specific to
Chinese biomedical text mining. Firstly, we collect the information of these
evaluation tasks, such as data sources and task types. Secondly, we conduct
systematic summary and comparative analysis, including named entity
recognition, entity normalization, attribute extraction, relation extraction,
event extraction, text classification, text similarity, knowledge graph
construction, question answering, text generation, and large language model
evaluation. Then, we summarize the potential clinical applications of these
community challenge tasks from translational informatics perspective. Finally,
we discuss the contributions and limitations of these community challenges,
while highlighting future directions in the era of large language models.</div><div><a href='http://arxiv.org/abs/2403.04261v1'>2403.04261v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17810v1")'>BioT5+: Towards Generalized Biological Understanding with IUPAC
  Integration and Multi-task Tuning</div>
<div id='2402.17810v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T12:43:09Z</div><div>Authors: Qizhi Pei, Lijun Wu, Kaiyuan Gao, Xiaozhuan Liang, Yin Fang, Jinhua Zhu, Shufang Xie, Tao Qin, Rui Yan</div><div style='padding-top: 10px; width: 80ex'>Recent research trends in computational biology have increasingly focused on
integrating text and bio-entity modeling, especially in the context of
molecules and proteins. However, previous efforts like BioT5 faced challenges
in generalizing across diverse tasks and lacked a nuanced understanding of
molecular structures, particularly in their textual representations (e.g.,
IUPAC). This paper introduces BioT5+, an extension of the BioT5 framework,
tailored to enhance biological research and drug discovery. BioT5+ incorporates
several novel features: integration of IUPAC names for molecular understanding,
inclusion of extensive bio-text and molecule data from sources like bioRxiv and
PubChem, the multi-task instruction tuning for generality across tasks, and a
novel numerical tokenization technique for improved processing of numerical
data. These enhancements allow BioT5+ to bridge the gap between molecular
representations and their textual descriptions, providing a more holistic
understanding of biological entities, and largely improving the grounded
reasoning of bio-text and bio-sequences. The model is pre-trained and
fine-tuned with a large number of experiments, including \emph{3 types of
problems (classification, regression, generation), 15 kinds of tasks, and 21
total benchmark datasets}, demonstrating the remarkable performance and
state-of-the-art results in most cases. BioT5+ stands out for its ability to
capture intricate relationships in biological data, thereby contributing
significantly to bioinformatics and computational biology. Our code is
available at \url{https://github.com/QizhiPei/BioT5}.</div><div><a href='http://arxiv.org/abs/2402.17810v1'>2402.17810v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.04286v1")'>Progress and Opportunities of Foundation Models in Bioinformatics</div>
<div id='2402.04286v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-06T02:29:17Z</div><div>Authors: Qing Li, Zhihang Hu, Yixuan Wang, Lei Li, Yimin Fan, Irwin King, Le Song, Yu Li</div><div style='padding-top: 10px; width: 80ex'>Bioinformatics has witnessed a paradigm shift with the increasing integration
of artificial intelligence (AI), particularly through the adoption of
foundation models (FMs). These AI techniques have rapidly advanced, addressing
historical challenges in bioinformatics such as the scarcity of annotated data
and the presence of data noise. FMs are particularly adept at handling
large-scale, unlabeled data, a common scenario in biological contexts due to
the time-consuming and costly nature of experimentally determining labeled
data. This characteristic has allowed FMs to excel and achieve notable results
in various downstream validation tasks, demonstrating their ability to
represent diverse biological entities effectively. Undoubtedly, FMs have
ushered in a new era in computational biology, especially in the realm of deep
learning. The primary goal of this survey is to conduct a systematic
investigation and summary of FMs in bioinformatics, tracing their evolution,
current research status, and the methodologies employed. Central to our focus
is the application of FMs to specific biological problems, aiming to guide the
research community in choosing appropriate FMs for their research needs. We
delve into the specifics of the problem at hand including sequence analysis,
structure prediction, function annotation, and multimodal integration,
comparing the structures and advancements against traditional methods.
Furthermore, the review analyses challenges and limitations faced by FMs in
biology, such as data noise, model explainability, and potential biases.
Finally, we outline potential development paths and strategies for FMs in
future biological research, setting the stage for continued innovation and
application in this rapidly evolving field. This comprehensive review serves
not only as an academic resource but also as a roadmap for future explorations
and applications of FMs in biology.</div><div><a href='http://arxiv.org/abs/2402.04286v1'>2402.04286v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14482v2")'>SpanSeq: Similarity-based sequence data splitting method for improved
  development and assessment of deep learning projects</div>
<div id='2402.14482v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T12:15:05Z</div><div>Authors: Alfred Ferrer Florensa, Jose Juan Almagro Armenteros, Henrik Nielsen, Frank Møller Aarestrup, Philip Thomas Lanken Conradsen Clausen</div><div style='padding-top: 10px; width: 80ex'>The use of deep learning models in computational biology has increased
massively in recent years, and is expected to do so further with the current
advances in fields like Natural Language Processing. These models, although
able to draw complex relations between input and target, are also largely
inclined to learn noisy deviations from the pool of data used during their
development. In order to assess their performance on unseen data (their
capacity to generalize), it is common to randomly split the available data in
development (train/validation) and test sets. This procedure, although
standard, has lately been shown to produce dubious assessments of
generalization due to the existing similarity between samples in the databases
used. In this work, we present SpanSeq, a database partition method for machine
learning that can scale to most biological sequences (genes, proteins and
genomes) in order to avoid data leakage between sets. We also explore the
effect of not restraining similarity between sets by reproducing the
development of the state-of-the-art model DeepLoc, not only confirming the
consequences of randomly splitting databases on the model assessment, but
expanding those repercussions to the model development. SpanSeq is available
for downloading and installing at
https://github.com/genomicepidemiology/SpanSeq.</div><div><a href='http://arxiv.org/abs/2402.14482v2'>2402.14482v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04805v1")'>Not all tickets are equal and we know it: Guiding pruning with
  domain-specific knowledge</div>
<div id='2403.04805v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T23:02:55Z</div><div>Authors: Intekhab Hossain, Jonas Fischer, Rebekka Burkholz, John Quackenbush</div><div style='padding-top: 10px; width: 80ex'>Neural structure learning is of paramount importance for scientific discovery
and interpretability. Yet, contemporary pruning algorithms that focus on
computational resource efficiency face algorithmic barriers to select a
meaningful model that aligns with domain expertise. To mitigate this challenge,
we propose DASH, which guides pruning by available domain-specific structural
information. In the context of learning dynamic gene regulatory network models,
we show that DASH combined with existing general knowledge on interaction
partners provides data-specific insights aligned with biology. For this task,
we show on synthetic data with ground truth information and two real world
applications the effectiveness of DASH, which outperforms competing methods by
a large margin and provides more meaningful biological insights. Our work shows
that domain specific structural information bears the potential to improve
model-derived scientific insights.</div><div><a href='http://arxiv.org/abs/2403.04805v1'>2403.04805v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09330v1")'>3D-based RNA function prediction tools in rnaglib</div>
<div id='2402.09330v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T17:22:03Z</div><div>Authors: Carlos Oliver, Vincent Mallet, Jérôme Waldispühl</div><div style='padding-top: 10px; width: 80ex'>Understanding the connection between complex structural features of RNA and
biological function is a fundamental challenge in evolutionary studies and in
RNA design. However, building datasets of RNA 3D structures and making
appropriate modeling choices remains time-consuming and lacks standardization.
In this chapter, we describe the use of rnaglib, to train supervised and
unsupervised machine learning-based function prediction models on datasets of
RNA 3D structures.</div><div><a href='http://arxiv.org/abs/2402.09330v1'>2402.09330v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00043v1")'>RiNALMo: General-Purpose RNA Language Models Can Generalize Well on
  Structure Prediction Tasks</div>
<div id='2403.00043v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T14:50:58Z</div><div>Authors: Rafael Josip Penić, Tin Vlašić, Roland G. Huber, Yue Wan, Mile Šikić</div><div style='padding-top: 10px; width: 80ex'>Ribonucleic acid (RNA) plays a variety of crucial roles in fundamental
biological processes. Recently, RNA has become an interesting drug target,
emphasizing the need to improve our understanding of its structures and
functions. Over the years, sequencing technologies have produced an enormous
amount of unlabeled RNA data, which hides important knowledge and potential.
Motivated by the successes of protein language models, we introduce RiboNucleic
Acid Language Model (RiNALMo) to help unveil the hidden code of RNA. RiNALMo is
the largest RNA language model to date with $650$ million parameters
pre-trained on $36$ million non-coding RNA sequences from several available
databases. RiNALMo is able to extract hidden knowledge and capture the
underlying structure information implicitly embedded within the RNA sequences.
RiNALMo achieves state-of-the-art results on several downstream tasks. Notably,
we show that its generalization capabilities can overcome the inability of
other deep learning methods for secondary structure prediction to generalize on
unseen RNA families. The code has been made publicly available on
https://github.com/lbcb-sci/RiNALMo.</div><div><a href='http://arxiv.org/abs/2403.00043v1'>2403.00043v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03234v1")'>Caduceus: Bi-Directional Equivariant Long-Range DNA Sequence Modeling</div>
<div id='2403.03234v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T01:42:51Z</div><div>Authors: Yair Schiff, Chia-Hsiang Kao, Aaron Gokaslan, Tri Dao, Albert Gu, Volodymyr Kuleshov</div><div style='padding-top: 10px; width: 80ex'>Large-scale sequence modeling has sparked rapid advances that now extend into
biology and genomics. However, modeling genomic sequences introduces challenges
such as the need to model long-range token interactions, the effects of
upstream and downstream regions of the genome, and the reverse complementarity
(RC) of DNA. Here, we propose an architecture motivated by these challenges
that builds off the long-range Mamba block, and extends it to a BiMamba
component that supports bi-directionality, and to a MambaDNA block that
additionally supports RC equivariance. We use MambaDNA as the basis of
Caduceus, the first family of RC equivariant bi-directional long-range DNA
language models, and we introduce pre-training and fine-tuning strategies that
yield Caduceus DNA foundation models. Caduceus outperforms previous long-range
models on downstream benchmarks; on a challenging long-range variant effect
prediction task, Caduceus exceeds the performance of 10x larger models that do
not leverage bi-directionality or equivariance.</div><div><a href='http://arxiv.org/abs/2403.03234v1'>2403.03234v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08075v1")'>Efficient and Scalable Fine-Tune of Language Models for Genome
  Understanding</div>
<div id='2402.08075v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T21:40:45Z</div><div>Authors: Huixin Zhan, Ying Nian Wu, Zijun Zhang</div><div style='padding-top: 10px; width: 80ex'>Although DNA foundation models have advanced the understanding of genomes,
they still face significant challenges in the limited scale and diversity of
genomic data. This limitation starkly contrasts with the success of natural
language foundation models, which thrive on substantially larger scales.
Furthermore, genome understanding involves numerous downstream genome
annotation tasks with inherent data heterogeneity, thereby necessitating more
efficient and robust fine-tuning methods tailored for genomics. Here, we
present \textsc{Lingo}: \textsc{L}anguage prefix f\textsc{In}e-tuning for
\textsc{G}en\textsc{O}mes. Unlike DNA foundation models, \textsc{Lingo}
strategically leverages natural language foundation models' contextual cues,
recalibrating their linguistic knowledge to genomic sequences. \textsc{Lingo}
further accommodates numerous, heterogeneous downstream fine-tune tasks by an
adaptive rank sampling method that prunes and stochastically reintroduces
pruned singular vectors within small computational budgets. Adaptive rank
sampling outperformed existing fine-tuning methods on all benchmarked 14 genome
understanding tasks, while requiring fewer than 2\% of trainable parameters as
genomic-specific adapters. Impressively, applying these adapters on natural
language foundation models matched or even exceeded the performance of DNA
foundation models. \textsc{Lingo} presents a new paradigm of efficient and
scalable genome understanding via genomic-specific adapters on language models.</div><div><a href='http://arxiv.org/abs/2402.08075v1'>2402.08075v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08303v4")'>ChatCell: Facilitating Single-Cell Analysis with Natural Language</div>
<div id='2402.08303v4' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T09:06:14Z</div><div>Authors: Yin Fang, Kangwei Liu, Ningyu Zhang, Xinle Deng, Penghui Yang, Zhuo Chen, Xiangru Tang, Mark Gerstein, Xiaohui Fan, Huajun Chen</div><div style='padding-top: 10px; width: 80ex'>As Large Language Models (LLMs) rapidly evolve, their influence in science is
becoming increasingly prominent. The emerging capabilities of LLMs in task
generalization and free-form dialogue can significantly advance fields like
chemistry and biology. However, the field of single-cell biology, which forms
the foundational building blocks of living organisms, still faces several
challenges. High knowledge barriers and limited scalability in current methods
restrict the full exploitation of LLMs in mastering single-cell data, impeding
direct accessibility and rapid iteration. To this end, we introduce ChatCell,
which signifies a paradigm shift by facilitating single-cell analysis with
natural language. Leveraging vocabulary adaptation and unified sequence
generation, ChatCell has acquired profound expertise in single-cell biology and
the capability to accommodate a diverse range of analysis tasks. Extensive
experiments further demonstrate ChatCell's robust performance and potential to
deepen single-cell insights, paving the way for more accessible and intuitive
exploration in this pivotal field. Our project homepage is available at
https://zjunlp.github.io/project/ChatCell.</div><div><a href='http://arxiv.org/abs/2402.08303v4'>2402.08303v4</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16901v1")'>FGBERT: Function-Driven Pre-trained Gene Language Model for Metagenomics</div>
<div id='2402.16901v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-24T13:13:17Z</div><div>Authors: ChenRui Duan, Zelin Zang, Yongjie Xu, Hang He, Zihan Liu, Zijia Song, Ju-Sheng Zheng, Stan Z. Li</div><div style='padding-top: 10px; width: 80ex'>Metagenomic data, comprising mixed multi-species genomes, are prevalent in
diverse environments like oceans and soils, significantly impacting human
health and ecological functions. However, current research relies on K-mer
representations, limiting the capture of structurally relevant gene contexts.
To address these limitations and further our understanding of complex
relationships between metagenomic sequences and their functions, we introduce a
protein-based gene representation as a context-aware and structure-relevant
tokenizer. Our approach includes Masked Gene Modeling (MGM) for gene
group-level pre-training, providing insights into inter-gene contextual
information, and Triple Enhanced Metagenomic Contrastive Learning (TEM-CL) for
gene-level pre-training to model gene sequence-function relationships. MGM and
TEM-CL constitute our novel metagenomic language model {\NAME}, pre-trained on
100 million metagenomic sequences. We demonstrate the superiority of our
proposed {\NAME} on eight datasets.</div><div><a href='http://arxiv.org/abs/2402.16901v1'>2402.16901v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14819v1")'>Endowing Protein Language Models with Structural Knowledge</div>
<div id='2401.14819v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T12:47:54Z</div><div>Authors: Dexiong Chen, Philip Hartout, Paolo Pellizzoni, Carlos Oliver, Karsten Borgwardt</div><div style='padding-top: 10px; width: 80ex'>Understanding the relationships between protein sequence, structure and
function is a long-standing biological challenge with manifold implications
from drug design to our understanding of evolution. Recently, protein language
models have emerged as the preferred method for this challenge, thanks to their
ability to harness large sequence databases. Yet, their reliance on expansive
sequence data and parameter sets limits their flexibility and practicality in
real-world scenarios. Concurrently, the recent surge in computationally
predicted protein structures unlocks new opportunities in protein
representation learning. While promising, the computational burden carried by
such complex data still hinders widely-adopted practical applications. To
address these limitations, we introduce a novel framework that enhances protein
language models by integrating protein structural data. Drawing from recent
advances in graph transformers, our approach refines the self-attention
mechanisms of pretrained language transformers by integrating structural
information with structure extractor modules. This refined model, termed
Protein Structure Transformer (PST), is further pretrained on a small protein
structure database, using the same masked language modeling objective as
traditional protein language models. Empirical evaluations of PST demonstrate
its superior parameter efficiency relative to protein language models, despite
being pretrained on a dataset comprising only 542K structures. Notably, PST
consistently outperforms the state-of-the-art foundation model for protein
sequences, ESM-2, setting a new benchmark in protein function prediction. Our
findings underscore the potential of integrating structural information into
protein language models, paving the way for more effective and efficient
protein modeling Code and pretrained models are available at
https://github.com/BorgwardtLab/PST.</div><div><a href='http://arxiv.org/abs/2401.14819v1'>2401.14819v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12995v1")'>Multi-Scale Protein Language Model for Unified Molecular Modeling</div>
<div id='2403.12995v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T13:35:41Z</div><div>Authors: Kangjie Zheng, Siyu Long, Tianyu Lu, Junwei Yang, Xinyu Dai, Ming Zhang, Zaiqing Nie, Wei-Ying Ma, Hao Zhou</div><div style='padding-top: 10px; width: 80ex'>Protein language models have demonstrated significant potential in the field
of protein engineering. However, current protein language models primarily
operate at the residue scale, which limits their ability to provide information
at the atom level. This limitation prevents us from fully exploiting the
capabilities of protein language models for applications involving both
proteins and small molecules. In this paper, we propose ms-ESM (multi-scale
ESM), a novel approach that enables multi-scale unified molecular modeling.
ms-ESM achieves this by pre-training on multi-scale code-switch protein
sequences and utilizing a multi-scale position encoding to capture
relationships among residues and atoms. Experimental results indicate that
ms-ESM surpasses previous methods in protein-molecule tasks, demonstrating the
full utilization of protein language models. Further investigations reveal that
through unified molecular modeling, ms-ESM not only gains molecular knowledge
but also retains its understanding of proteins.</div><div><a href='http://arxiv.org/abs/2403.12995v1'>2403.12995v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18567v1")'>Diffusion Language Models Are Versatile Protein Learners</div>
<div id='2402.18567v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T18:57:56Z</div><div>Authors: Xinyou Wang, Zaixiang Zheng, Fei Ye, Dongyu Xue, Shujian Huang, Quanquan Gu</div><div style='padding-top: 10px; width: 80ex'>This paper introduces diffusion protein language model (DPLM), a versatile
protein language model that demonstrates strong generative and predictive
capabilities for protein sequences. We first pre-train scalable DPLMs from
evolutionary-scale protein sequences within a generative self-supervised
discrete diffusion probabilistic framework, which generalizes language modeling
for proteins in a principled way. After pre-training, DPLM exhibits the ability
to generate structurally plausible, novel, and diverse protein sequences for
unconditional generation. We further demonstrate the proposed diffusion
generative pre-training makes DPLM possess a better understanding of proteins,
making it a superior representation learner, which can be fine-tuned for
various predictive tasks, comparing favorably to ESM2 (Lin et al., 2022).
Moreover, DPLM can be tailored for various needs, which showcases its prowess
of conditional generation in several ways: (1) conditioning on partial peptide
sequences, e.g., generating scaffolds for functional motifs with high success
rate; (2) incorporating other modalities as conditioner, e.g.,
structure-conditioned generation for inverse folding; and (3) steering sequence
generation towards desired properties, e.g., satisfying specified secondary
structures, through a plug-and-play classifier guidance.</div><div><a href='http://arxiv.org/abs/2402.18567v1'>2402.18567v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06199v1")'>xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering
  the Language of Protein</div>
<div id='2401.06199v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T15:03:17Z</div><div>Authors: Bo Chen, Xingyi Cheng, Pan Li, Yangli-ao Geng, Jing Gong, Shen Li, Zhilei Bei, Xu Tan, Boyan Wang, Xin Zeng, Chiming Liu, Aohan Zeng, Yuxiao Dong, Jie Tang, Le Song</div><div style='padding-top: 10px; width: 80ex'>Protein language models have shown remarkable success in learning biological
information from protein sequences. However, most existing models are limited
by either autoencoding or autoregressive pre-training objectives, which makes
them struggle to handle protein understanding and generation tasks
concurrently. We propose a unified protein language model, xTrimoPGLM, to
address these two types of tasks simultaneously through an innovative
pre-training framework. Our key technical contribution is an exploration of the
compatibility and the potential for joint optimization of the two types of
objectives, which has led to a strategy for training xTrimoPGLM at an
unprecedented scale of 100 billion parameters and 1 trillion training tokens.
Our extensive experiments reveal that 1) xTrimoPGLM significantly outperforms
other advanced baselines in 18 protein understanding benchmarks across four
categories. The model also facilitates an atomic-resolution view of protein
structures, leading to an advanced 3D structural prediction model that
surpasses existing language model-based tools. 2) xTrimoPGLM not only can
generate de novo protein sequences following the principles of natural ones,
but also can perform programmable generation after supervised fine-tuning (SFT)
on curated sequences. These results highlight the substantial capability and
versatility of xTrimoPGLM in understanding and generating protein sequences,
contributing to the evolving landscape of foundation models in protein science.</div><div><a href='http://arxiv.org/abs/2401.06199v1'>2401.06199v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03726v1")'>Diffusion on language model embeddings for protein sequence generation</div>
<div id='2403.03726v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T14:15:20Z</div><div>Authors: Viacheslav Meshchaninov, Pavel Strashnov, Andrey Shevtsov, Fedor Nikolaev, Nikita Ivanisenko, Olga Kardymon, Dmitry Vetrov</div><div style='padding-top: 10px; width: 80ex'>Protein design requires a deep understanding of the inherent complexities of
the protein universe. While many efforts lean towards conditional generation or
focus on specific families of proteins, the foundational task of unconditional
generation remains underexplored and undervalued. Here, we explore this pivotal
domain, introducing DiMA, a model that leverages continuous diffusion on
embeddings derived from the protein language model, ESM-2, to generate amino
acid sequences. DiMA surpasses leading solutions, including autoregressive
transformer-based and discrete diffusion models, and we quantitatively
illustrate the impact of the design choices that lead to its superior
performance. We extensively evaluate the quality, diversity, distribution
similarity, and biological relevance of the generated sequences using multiple
metrics across various modalities. Our approach consistently produces novel,
diverse protein sequences that accurately reflect the inherent structural and
functional diversity of the protein space. This work advances the field of
protein design and sets the stage for conditional models by providing a robust
framework for scalable and high-quality protein sequence generation.</div><div><a href='http://arxiv.org/abs/2403.03726v1'>2403.03726v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17156v1")'>TaxDiff: Taxonomic-Guided Diffusion Model for Protein Sequence
  Generation</div>
<div id='2402.17156v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T02:41:46Z</div><div>Authors: Lin Zongying, Li Hao, Lv Liuzhenghao, Lin Bin, Zhang Junwu, Chen Calvin Yu-Chian, Yuan Li, Tian Yonghong</div><div style='padding-top: 10px; width: 80ex'>Designing protein sequences with specific biological functions and structural
stability is crucial in biology and chemistry. Generative models already
demonstrated their capabilities for reliable protein design. However, previous
models are limited to the unconditional generation of protein sequences and
lack the controllable generation ability that is vital to biological tasks. In
this work, we propose TaxDiff, a taxonomic-guided diffusion model for
controllable protein sequence generation that combines biological species
information with the generative capabilities of diffusion models to generate
structurally stable proteins within the sequence space. Specifically, taxonomic
control information is inserted into each layer of the transformer block to
achieve fine-grained control. The combination of global and local attention
ensures the sequence consistency and structural foldability of
taxonomic-specific proteins. Extensive experiments demonstrate that TaxDiff can
consistently achieve better performance on multiple protein sequence generation
benchmarks in both taxonomic-guided controllable generation and unconditional
generation. Remarkably, the sequences generated by TaxDiff even surpass those
produced by direct-structure-generation models in terms of confidence based on
predicted structures and require only a quarter of the time of models based on
the diffusion model. The code for generating proteins and training new versions
of TaxDiff is available at:https://github.com/Linzy19/TaxDiff.</div><div><a href='http://arxiv.org/abs/2402.17156v1'>2402.17156v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14088v1")'>Protein Conformation Generation via Force-Guided SE(3) Diffusion Models</div>
<div id='2403.14088v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T02:44:08Z</div><div>Authors: Yan Wang, Lihao Wang, Yuning Shen, Yiqun Wang, Huizhuo Yuan, Yue Wu, Quanquan Gu</div><div style='padding-top: 10px; width: 80ex'>The conformational landscape of proteins is crucial to understanding their
functionality in complex biological processes. Traditional physics-based
computational methods, such as molecular dynamics (MD) simulations, suffer from
rare event sampling and long equilibration time problems, hindering their
applications in general protein systems. Recently, deep generative modeling
techniques, especially diffusion models, have been employed to generate novel
protein conformations. However, existing score-based diffusion methods cannot
properly incorporate important physical prior knowledge to guide the generation
process, causing large deviations in the sampled protein conformations from the
equilibrium distribution. In this paper, to overcome these limitations, we
propose a force-guided SE(3) diffusion model, ConfDiff, for protein
conformation generation. By incorporating a force-guided network with a mixture
of data-based score models, ConfDiff can can generate protein conformations
with rich diversity while preserving high fidelity. Experiments on a variety of
protein conformation prediction tasks, including 12 fast-folding proteins and
the Bovine Pancreatic Trypsin Inhibitor (BPTI), demonstrate that our method
surpasses the state-of-the-art method.</div><div><a href='http://arxiv.org/abs/2403.14088v1'>2403.14088v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09673v2")'>FoldToken: Learning Protein Language via Vector Quantization and Beyond</div>
<div id='2403.09673v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T12:18:51Z</div><div>Authors: Zhangyang Gao, Cheng Tan, Jue Wang, Yufei Huang, Lirong Wu, Stan Z. Li</div><div style='padding-top: 10px; width: 80ex'>Is there a foreign language describing protein sequences and structures
simultaneously? Protein structures, represented by continuous 3D points, have
long posed a challenge due to the contrasting modeling paradigms of discrete
sequences. We introduce \textbf{FoldTokenizer} to represent protein
sequence-structure as discrete symbols. This innovative approach involves
projecting residue types and structures into a discrete space, guided by a
reconstruction loss for information preservation. We refer to the learned
discrete symbols as \textbf{FoldToken}, and the sequence of FoldTokens serves
as a new protein language, transforming the protein sequence-structure into a
unified modality. We apply the created protein language on general backbone
inpainting and antibody design tasks, building the first GPT-style model
(\textbf{FoldGPT}) for sequence-structure co-generation with promising results.
Key to our success is the substantial enhancement of the vector quantization
module, Soft Conditional Vector Quantization (\textbf{SoftCVQ}).</div><div><a href='http://arxiv.org/abs/2403.09673v2'>2403.09673v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.07657v1")'>Empirical Evidence for the Fragment level Understanding on Drug
  Molecular Structure of LLMs</div>
<div id='2401.07657v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T12:53:58Z</div><div>Authors: Xiuyuan Hu, Guoqing Liu, Yang Zhao, Hao Zhang</div><div style='padding-top: 10px; width: 80ex'>AI for drug discovery has been a research hotspot in recent years, and
SMILES-based language models has been increasingly applied in drug molecular
design. However, no work has explored whether and how language models
understand the chemical spatial structure from 1D sequences. In this work, we
pre-train a transformer model on chemical language and fine-tune it toward drug
design objectives, and investigate the correspondence between high-frequency
SMILES substrings and molecular fragments. The results indicate that language
models can understand chemical structures from the perspective of molecular
fragments, and the structural knowledge learned through fine-tuning is
reflected in the high-frequency SMILES substrings generated by the model.</div><div><a href='http://arxiv.org/abs/2401.07657v1'>2401.07657v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.01481v3")'>Multi-level protein pre-training with Vabs-Net</div>
<div id='2402.01481v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T15:07:09Z</div><div>Authors: Jiale Zhao, Wanru Zhuang, Jia Song, Yaqi Li, Shuqi Lu</div><div style='padding-top: 10px; width: 80ex'>In recent years, there has been a surge in the development of 3D
structure-based pre-trained protein models, representing a significant
advancement over pre-trained protein language models in various downstream
tasks. However, most existing structure-based pre-trained models primarily
focus on the residue level, i.e., alpha carbon atoms, while ignoring other
atoms like side chain atoms. We argue that modeling proteins at both residue
and atom levels is important since the side chain atoms can also be crucial for
numerous downstream tasks, for example, molecular docking. Nevertheless, we
find that naively combining residue and atom information during pre-training
typically fails. We identify a key reason is the information leakage caused by
the inclusion of atom structure in the input, which renders residue-level
pre-training tasks trivial and results in insufficiently expressive residue
representations. To address this issue, we introduce a span mask pre-training
strategy on 3D protein chains to learn meaningful representations of both
residues and atoms. This leads to a simple yet effective approach to learning
protein representation suitable for diverse downstream tasks. Extensive
experimental results on binding site prediction and function prediction tasks
demonstrate our proposed pre-training approach significantly outperforms other
methods. Our code will be made public.</div><div><a href='http://arxiv.org/abs/2402.01481v3'>2402.01481v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12714v1")'>Equivariant Pretrained Transformer for Unified Geometric Learning on
  Multi-Domain 3D Molecules</div>
<div id='2402.12714v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T04:40:00Z</div><div>Authors: Rui Jiao, Xiangzhe Kong, Ziyang Yu, Wenbing Huang, Yang Liu</div><div style='padding-top: 10px; width: 80ex'>Pretraining on a large number of unlabeled 3D molecules has showcased
superiority in various scientific applications. However, prior efforts
typically focus on pretraining models on a specific domain, either proteins or
small molecules, missing the opportunity to leverage the cross-domain
knowledge. To mitigate this gap, we introduce Equivariant Pretrained
Transformer (EPT), a novel pretraining framework designed to harmonize the
geometric learning of small molecules and proteins. To be specific, EPT unifies
the geometric modeling of multi-domain molecules via the block-enhanced
representation that can attend a broader context of each atom. Upon transformer
framework, EPT is further enhanced with E(3) equivariance to facilitate the
accurate representation of 3D structures. Another key innovation of EPT is its
block-level pretraining task, which allows for joint pretraining on datasets
comprising both small molecules and proteins. Experimental evaluations on a
diverse group of benchmarks, including ligand binding affinity prediction,
molecular property prediction, and protein property prediction, show that EPT
significantly outperforms previous SOTA methods for affinity prediction, and
achieves the best or comparable performance with existing domain-specific
pretraining models for other tasks.</div><div><a href='http://arxiv.org/abs/2402.12714v1'>2402.12714v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11353v1")'>Solvent-Aware 2D NMR Prediction: Leveraging Multi-Tasking Training and
  Iterative Self-Training Strategies</div>
<div id='2403.11353v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-17T21:52:51Z</div><div>Authors: Yunrui Li, Hao Xu, Pengyu Hong</div><div style='padding-top: 10px; width: 80ex'>Nuclear magnetic resonance (NMR) spectroscopy plays a pivotal role in various
scientific fields, offering insights into structural information, electronic
properties and dynamic behaviors of molecules. Accurate NMR spectrum prediction
efficiently produces candidate molecules, enabling chemists to compare them
with actual experimental spectra. This process aids in confirming molecular
structures or pinpointing discrepancies, guiding further investigation. Machine
Learning (ML) has then emerged as a promising alternative approach for
predicting atomic NMR chemical shits of molecules given their structures.
Although significant progresses have been made in predicting one-dimensional
(1D) NMR, two-dimensional (2D) NMR prediction via ML remains a challenge due to
the lack of annotated NMR training datasets. To address this gap, we propose an
iterative self-training (IST) approach to train a deep learning model for
predicting atomic 2DNMR shifts and assigning peaks in experimental spectra. Our
model undergoes an initial pre-training phase employing a Multi-Task Training
(MTT) approach, which simultaneously leverages annotated 1D NMR datasets of
both $^{1}\text{H}$ and $^{13}\text{C}$ spectra to enhance its understanding of
NMR spectra. Subsequently, the pre-trained model is utilized to generate
pseudo-annotations for unlabelled 2D NMR spectra, which are subsequently used
to refine the 2D NMR prediction model. Our approach iterates between annotated
unlabelled 2D NMR data and refining our 2D NMR prediction model until
convergence. Finally, our model is able to not only accurately predict 2D NMR
but also annotate peaks in experimental 2D NMR spectra. Experimental results
show that our model is capable of accurately handling medium-sized and large
molecules, including polysaccharides, underscoring its effectiveness.</div><div><a href='http://arxiv.org/abs/2403.11353v1'>2403.11353v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.00875v1")'>Enhancing Protein Predictive Models via Proteins Data Augmentation: A
  Benchmark and New Directions</div>
<div id='2403.00875v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T07:58:29Z</div><div>Authors: Rui Sun, Lirong Wu, Haitao Lin, Yufei Huang, Stan Z. Li</div><div style='padding-top: 10px; width: 80ex'>Augmentation is an effective alternative to utilize the small amount of
labeled protein data. However, most of the existing work focuses on design-ing
new architectures or pre-training tasks, and relatively little work has studied
data augmentation for proteins. This paper extends data augmentation techniques
previously used for images and texts to proteins and then benchmarks these
techniques on a variety of protein-related tasks, providing the first
comprehensive evaluation of protein augmentation. Furthermore, we propose two
novel semantic-level protein augmentation methods, namely Integrated Gradients
Substitution and Back Translation Substitution, which enable protein
semantic-aware augmentation through saliency detection and biological
knowledge. Finally, we integrate extended and proposed augmentations into an
augmentation pool and propose a simple but effective framework, namely
Automated Protein Augmentation (APA), which can adaptively select the most
suitable augmentation combinations for different tasks. Extensive experiments
have shown that APA enhances the performance of five protein related tasks by
an average of 10.55% across three architectures compared to vanilla
implementations without augmentation, highlighting its potential to make a
great impact on the field.</div><div><a href='http://arxiv.org/abs/2403.00875v1'>2403.00875v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07955v1")'>ProtIR: Iterative Refinement between Retrievers and Predictors for
  Protein Function Annotation</div>
<div id='2402.07955v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-10T17:31:46Z</div><div>Authors: Zuobai Zhang, Jiarui Lu, Vijil Chenthamarakshan, Aurélie Lozano, Payel Das, Jian Tang</div><div style='padding-top: 10px; width: 80ex'>Protein function annotation is an important yet challenging task in biology.
Recent deep learning advancements show significant potential for accurate
function prediction by learning from protein sequences and structures.
Nevertheless, these predictor-based methods often overlook the modeling of
protein similarity, an idea commonly employed in traditional approaches using
sequence or structure retrieval tools. To fill this gap, we first study the
effect of inter-protein similarity modeling by benchmarking retriever-based
methods against predictors on protein function annotation tasks. Our results
show that retrievers can match or outperform predictors without large-scale
pre-training. Building on these insights, we introduce a novel variational
pseudo-likelihood framework, ProtIR, designed to improve function predictors by
incorporating inter-protein similarity modeling. This framework iteratively
refines knowledge between a function predictor and retriever, thereby combining
the strengths of both predictors and retrievers. ProtIR showcases around 10%
improvement over vanilla predictor-based methods. Besides, it achieves
performance on par with protein language model-based methods, yet without the
need for massive pre-training, highlighting the efficacy of our framework. Code
will be released upon acceptance.</div><div><a href='http://arxiv.org/abs/2402.07955v1'>2402.07955v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07013v2")'>AdaNovo: Adaptive \emph{De Novo} Peptide Sequencing with Conditional
  Mutual Information</div>
<div id='2403.07013v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-09T11:54:58Z</div><div>Authors: Jun Xia, Shaorong Chen, Jingbo Zhou, Tianze Ling, Wenjie Du, Sizhe Liu, Stan Z. Li</div><div style='padding-top: 10px; width: 80ex'>Tandem mass spectrometry has played a pivotal role in advancing proteomics,
enabling the analysis of protein composition in biological samples. Despite the
development of various deep learning methods for identifying amino acid
sequences (peptides) responsible for observed spectra, challenges persist in
\emph{de novo} peptide sequencing. Firstly, prior methods struggle to identify
amino acids with post-translational modifications (PTMs) due to their lower
frequency in training data compared to canonical amino acids, further resulting
in decreased peptide-level identification precision. Secondly, diverse types of
noise and missing peaks in mass spectra reduce the reliability of training data
(peptide-spectrum matches, PSMs). To address these challenges, we propose
AdaNovo, a novel framework that calculates conditional mutual information (CMI)
between the spectrum and each amino acid/peptide, using CMI for adaptive model
training. Extensive experiments demonstrate AdaNovo's state-of-the-art
performance on a 9-species benchmark, where the peptides in the training set
are almost completely disjoint from the peptides of the test sets. Moreover,
AdaNovo excels in identifying amino acids with PTMs and exhibits robustness
against data noise. The supplementary materials contain the official code.</div><div><a href='http://arxiv.org/abs/2403.07013v2'>2403.07013v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09188v1")'>Design of an basis-projected layer for sparse datasets in deep learning
  training using gc-ms spectra as a case study</div>
<div id='2403.09188v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T09:03:51Z</div><div>Authors: Yu Tang Chang, Shih Fang Chen</div><div style='padding-top: 10px; width: 80ex'>Deep learning (DL) models encompass millions or even billions of parameters
and learn complex patterns from big data. However, not all data are initially
stored in a suitable formation to effectively train a DL model, e.g., gas
chromatography-mass spectrometry (GC-MS) spectra and DNA sequence. These
datasets commonly contain many zero values, and the sparse data formation
causes difficulties in optimizing DL models. A DL module called the
basis-projected layer (BPL) was proposed to mitigate the issue by transforming
the sparse data into a dense representation. The transformed data is expected
to facilitate the gradient calculation and finetuned process in a DL training
process. The dataset, example of a sparse dataset, contained 362 specialty
coffee odorant spectra detected from GC-MS. The BPL layer was placed at the
beginning of the DL model. The tunable parameters in the layer were learnable
projected axes that were the bases of a new representation space. The layer
rotated these bases when its parameters were updated. When the number of the
bases was the same as the original dimension, the increasing percentage of the
F1 scores was 8.56%. Furthermore, when the number was set as 768 (the original
dimension was 490), the increasing percentage of the F1 score was 11.49%. The
layer not only maintained the model performance and even constructed a better
representation space in analyzing sparse datasets.</div><div><a href='http://arxiv.org/abs/2403.09188v1'>2403.09188v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.17131v1")'>Predicting O-GlcNAcylation Sites in Mammalian Proteins with Transformers
  and RNNs Trained with a New Loss Function</div>
<div id='2402.17131v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T01:53:02Z</div><div>Authors: Pedro Seber</div><div style='padding-top: 10px; width: 80ex'>Glycosylation, a protein modification, has multiple essential functional and
structural roles. O-GlcNAcylation, a subtype of glycosylation, has the
potential to be an important target for therapeutics, but methods to reliably
predict O-GlcNAcylation sites had not been available until 2023; a 2021 review
correctly noted that published models were insufficient and failed to
generalize. Moreover, many are no longer usable. In 2023, a considerably better
RNN model with an F$_1$ score of 36.17% and an MCC of 34.57% on a large dataset
was published. This article first sought to improve these metrics using
transformer encoders. While transformers displayed high performance on this
dataset, their performance was inferior to that of the previously published
RNN. We then created a new loss function, which we call the weighted focal
differentiable MCC, to improve the performance of classification models. RNN
models trained with this new function display superior performance to models
trained using the weighted cross-entropy loss; this new function can also be
used to fine-tune trained models. A two-cell RNN trained with this loss
achieves state-of-the-art performance in O-GlcNAcylation site prediction with
an F$_1$ score of 38.82% and an MCC of 38.21% on that large dataset.</div><div><a href='http://arxiv.org/abs/2402.17131v1'>2402.17131v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.10211v1")'>Improving PTM Site Prediction by Coupling of Multi-Granularity Structure
  and Multi-Scale Sequence Representation</div>
<div id='2401.10211v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T20:49:32Z</div><div>Authors: Zhengyi Li, Menglu Li, Lida Zhu, Wen Zhang</div><div style='padding-top: 10px; width: 80ex'>Protein post-translational modification (PTM) site prediction is a
fundamental task in bioinformatics. Several computational methods have been
developed to predict PTM sites. However, existing methods ignore the structure
information and merely utilize protein sequences. Furthermore, designing a more
fine-grained structure representation learning method is urgently needed as PTM
is a biological event that occurs at the atom granularity. In this paper, we
propose a PTM site prediction method by Coupling of Multi-Granularity structure
and Multi-Scale sequence representation, PTM-CMGMS for brevity. Specifically,
multigranularity structure-aware representation learning is designed to learn
neighborhood structure representations at the amino acid, atom, and whole
protein granularity from AlphaFold predicted structures, followed by utilizing
contrastive learning to optimize the structure representations.Additionally,
multi-scale sequence representation learning is used to extract context
sequence information, and motif generated by aligning all context sequences of
PTM sites assists the prediction. Extensive experiments on three datasets show
that PTM-CMGMS outperforms the state-of-the-art methods.</div><div><a href='http://arxiv.org/abs/2401.10211v1'>2401.10211v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.04478v2")'>TwinBooster: Synergising Large Language Models with Barlow Twins and
  Gradient Boosting for Enhanced Molecular Property Prediction</div>
<div id='2401.04478v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-09T10:36:20Z</div><div>Authors: Maximilian G. Schuh, Davide Boldini, Stephan A. Sieber</div><div style='padding-top: 10px; width: 80ex'>The success of drug discovery and development relies on the precise
prediction of molecular activities and properties. While in silico molecular
property prediction has shown remarkable potential, its use has been limited so
far to assays for which large amounts of data are available. In this study, we
use a fine-tuned large language model to integrate biological assays based on
their textual information, coupled with Barlow Twins, a Siamese neural network
using a novel self-supervised learning approach. This architecture uses both
assay information and molecular fingerprints to extract the true molecular
information. TwinBooster enables the prediction of properties of unseen
bioassays and molecules by providing state-of-the-art zero-shot learning tasks.
Remarkably, our artificial intelligence pipeline shows excellent performance on
the FS-Mol benchmark. This breakthrough demonstrates the application of deep
learning to critical property prediction tasks where data is typically scarce.
By accelerating the early identification of active molecules in drug discovery
and development, this method has the potential to help streamline the
identification of novel therapeutics.</div><div><a href='http://arxiv.org/abs/2401.04478v2'>2401.04478v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16882v1")'>Substrate Scope Contrastive Learning: Repurposing Human Bias to Learn
  Atomic Representations</div>
<div id='2402.16882v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T02:21:20Z</div><div>Authors: Wenhao Gao, Priyanka Raghavan, Ron Shprints, Connor W. Coley</div><div style='padding-top: 10px; width: 80ex'>Learning molecular representation is a critical step in molecular machine
learning that significantly influences modeling success, particularly in
data-scarce situations. The concept of broadly pre-training neural networks has
advanced fields such as computer vision, natural language processing, and
protein engineering. However, similar approaches for small organic molecules
have not achieved comparable success. In this work, we introduce a novel
pre-training strategy, substrate scope contrastive learning, which learns
atomic representations tailored to chemical reactivity. This method considers
the grouping of substrates and their yields in published substrate scope tables
as a measure of their similarity or dissimilarity in terms of chemical
reactivity. We focus on 20,798 aryl halides in the CAS Content Collection
spanning thousands of publications to learn a representation of aryl halide
reactivity. We validate our pre-training approach through both intuitive
visualizations and comparisons to traditional reactivity descriptors and
physical organic chemistry principles. The versatility of these embeddings is
further evidenced in their application to yield prediction, regioselectivity
prediction, and the diverse selection of new substrates. This work not only
presents a chemistry-tailored neural network pre-training strategy to learn
reactivity-aligned atomic representations, but also marks a first-of-its-kind
approach to benefit from the human bias in substrate scope design.</div><div><a href='http://arxiv.org/abs/2402.16882v1'>2402.16882v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.00024v2")'>Comparative Analysis of LLaMA and ChatGPT Embeddings for Molecule
  Embedding</div>
<div id='2402.00024v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T18:31:34Z</div><div>Authors: Shaghayegh Sadeghi, Alan Bui, Ali Forooghi, Jianguo Lu, Alioune Ngom</div><div style='padding-top: 10px; width: 80ex'>Purpose: Large Language Models (LLMs) like ChatGPT and LLaMA are increasingly
recognized for their potential in the field of cheminformatics, particularly in
interpreting Simplified Molecular Input Line Entry System (SMILES), a standard
method for representing chemical structures. These LLMs can decode SMILES
strings into vector representations, providing a novel approach to
understanding chemical graphs.
  Methods: We investigate the performance of ChatGPT and LLaMA in embedding
SMILES strings. Our evaluation focuses on two key applications: molecular
property (MP) prediction and drug-drug interaction (DDI) prediction, both
essential in drug development and healthcare.
  Results: We find that SMILES embeddings generated using LLaMA outperform
those from ChatGPT in both MP and DDI prediction tasks. Notably, LLaMA-based
SMILES embeddings show results comparable to existing methods in both
prediction tasks.
  Conclusion: The application of LLMs in cheminformatics, particularly in
utilizing SMILES embeddings, shows significant promise for advancing drug
development. This includes improving the prediction of chemical properties and
facilitating the drug discovery process. GitHub:
https://github.com/sshaghayeghs/LLaMA-VS-ChatGPT</div><div><a href='http://arxiv.org/abs/2402.00024v2'>2402.00024v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.17267v1")'>ReacLLaMA: Merging chemical and textual information in chemical
  reactivity AI models</div>
<div id='2401.17267v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T18:57:08Z</div><div>Authors: Aline Hartgers, Ramil Nugmanov, Kostiantyn Chernichenko, Joerg Kurt Wegner</div><div style='padding-top: 10px; width: 80ex'>Chemical reactivity models are developed to predict chemical reaction
outcomes in the form of classification (success/failure) or regression (product
yield) tasks. The vast majority of the reported models are trained solely on
chemical information such as reactants, products, reagents, and solvents, but
not on the details of a synthetic protocol. Herein incorporation of procedural
text with the aim to augment the Graphormer reactivity model and improve its
accuracy is presented. Two major approaches are used: training an adapter
Graphormer model that is provided with a GPT-2-derived latent representation of
the text procedure (ReacLLaMA-Adapter) and labeling an unlabeled part of a
dataset with the LLaMA 2 model followed by training the Graphormer on an
extended dataset (Zero-Shot Labeling ReacLLaMA). Both methodologies enhance the
discernment of unpromising reactions, thereby providing more accurate models
with improved specificity.</div><div><a href='http://arxiv.org/abs/2401.17267v1'>2401.17267v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.15222v2")'>Transfer Learning for the Prediction of Entity Modifiers in Clinical
  Text: Application to Opioid Use Disorder Case Detection</div>
<div id='2401.15222v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T22:19:31Z</div><div>Authors: Abdullateef I. Almudaifer, Whitney Covington, JaMor Hairston, Zachary Deitch, Ankit Anand, Caleb M. Carroll, Estera Crisan, William Bradford, Lauren Walter, Eaton Ellen, Sue S. Feldman, John D. Osborne</div><div style='padding-top: 10px; width: 80ex'>Background: The semantics of entities extracted from a clinical text can be
dramatically altered by modifiers, including entity negation, uncertainty,
conditionality, severity, and subject. Existing models for determining
modifiers of clinical entities involve regular expression or features weights
that are trained independently for each modifier.
  Methods: We develop and evaluate a multi-task transformer architecture design
where modifiers are learned and predicted jointly using the publicly available
SemEval 2015 Task 14 corpus and a new Opioid Use Disorder (OUD) data set that
contains modifiers shared with SemEval as well as novel modifiers specific for
OUD. We evaluate the effectiveness of our multi-task learning approach versus
previously published systems and assess the feasibility of transfer learning
for clinical entity modifiers when only a portion of clinical modifiers are
shared.
  Results: Our approach achieved state-of-the-art results on the ShARe corpus
from SemEval 2015 Task 14, showing an increase of 1.1% on weighted accuracy,
1.7% on unweighted accuracy, and 10% on micro F1 scores.
  Conclusions: We show that learned weights from our shared model can be
effectively transferred to a new partially matched data set, validating the use
of transfer learning for clinical text modifiers</div><div><a href='http://arxiv.org/abs/2401.15222v2'>2401.15222v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.01829v1")'>Predicting ATP binding sites in protein sequences using Deep Learning
  and Natural Language Processing</div>
<div id='2402.01829v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T18:42:39Z</div><div>Authors: Shreyas V, Swati Agarwal</div><div style='padding-top: 10px; width: 80ex'>Predicting ATP-Protein Binding sites in genes is of great significance in the
field of Biology and Medicine. The majority of research in this field has been
conducted through time- and resource-intensive 'wet experiments' in
laboratories. Over the years, researchers have been investigating computational
methods computational methods to accomplish the same goals, utilising the
strength of advanced Deep Learning and NLP algorithms. In this paper, we
propose to develop methods to classify ATP-Protein binding sites. We conducted
various experiments mainly using PSSMs and several word embeddings as features.
We used 2D CNNs and LightGBM classifiers as our chief Deep Learning Algorithms.
The MP3Vec and BERT models have also been subjected to testing in our study.
The outcomes of our experiments demonstrated improvement over the
state-of-the-art benchmarks.</div><div><a href='http://arxiv.org/abs/2402.01829v1'>2402.01829v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.18609v3")'>ICE-SEARCH: A Language Model-Driven Feature Selection Approach</div>
<div id='2402.18609v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T15:06:25Z</div><div>Authors: Tianze Yang, Tianyi Yang, Shaoshan Liu, Fuyuan Lvu, Xue Liu</div><div style='padding-top: 10px; width: 80ex'>This study unveils the In-Context Evolutionary Search (ICE-SEARCH) method,
the first work that melds language models (LMs) with evolutionary algorithms
for feature selection (FS) tasks and demonstrates its effectiveness in Medical
Predictive Analytics (MPA) applications. ICE-SEARCH harnesses the crossover and
mutation capabilities inherent in LMs within an evolutionary framework,
significantly improving FS through the model's comprehensive world knowledge
and its adaptability to a variety of roles. Our evaluation of this methodology
spans three crucial MPA tasks: stroke, cardiovascular disease, and diabetes,
where ICE-SEARCH outperforms traditional FS methods in pinpointing essential
features for medical applications. ICE-SEARCH achieves State-of-the-Art (SOTA)
performance in stroke prediction and diabetes prediction; the
Decision-Randomized ICE-SEARCH ranks as SOTA in cardiovascular disease
prediction. Our results not only demonstrate the efficacy of ICE-SEARCH in
medical FS but also underscore the versatility, efficiency, and scalability of
integrating LMs in FS tasks. The study emphasizes the critical role of
incorporating domain-specific insights, illustrating ICE-SEARCH's robustness,
generalizability, and swift convergence. This opens avenues for further
research into comprehensive and intricate FS landscapes, marking a significant
stride in the application of artificial intelligence in medical predictive
analytics.</div><div><a href='http://arxiv.org/abs/2402.18609v3'>2402.18609v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15780v1")'>Fine-Tuned Large Language Models for Symptom Recognition from Spanish
  Clinical Text</div>
<div id='2401.15780v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-28T22:11:25Z</div><div>Authors: Mai A. Shaaban, Abbas Akkasi, Adnan Khan, Majid Komeili, Mohammad Yaqub</div><div style='padding-top: 10px; width: 80ex'>The accurate recognition of symptoms in clinical reports is significantly
important in the fields of healthcare and biomedical natural language
processing. These entities serve as essential building blocks for clinical
information extraction, enabling retrieval of critical medical insights from
vast amounts of textual data. Furthermore, the ability to identify and
categorize these entities is fundamental for developing advanced clinical
decision support systems, aiding healthcare professionals in diagnosis and
treatment planning. In this study, we participated in SympTEMIST, a shared task
on the detection of symptoms, signs and findings in Spanish medical documents.
We combine a set of large language models fine-tuned with the data released by
the organizers.</div><div><a href='http://arxiv.org/abs/2401.15780v1'>2401.15780v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11227v1")'>Cheap Ways of Extracting Clinical Markers from Texts</div>
<div id='2403.11227v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-17T14:21:42Z</div><div>Authors: Anastasia Sandu, Teodor Mihailescu, Sergiu Nisioi</div><div style='padding-top: 10px; width: 80ex'>This paper describes the work of the UniBuc Archaeology team for CLPsych's
2024 Shared Task, which involved finding evidence within the text supporting
the assigned suicide risk level. Two types of evidence were required:
highlights (extracting relevant spans within the text) and summaries
(aggregating evidence into a synthesis). Our work focuses on evaluating Large
Language Models (LLM) as opposed to an alternative method that is much more
memory and resource efficient. The first approach employs a good old-fashioned
machine learning (GOML) pipeline consisting of a tf-idf vectorizer with a
logistic regression classifier, whose representative features are used to
extract relevant highlights. The second, more resource intensive, uses an LLM
for generating the summaries and is guided by chain-of-thought to provide
sequences of text indicating clinical markers.</div><div><a href='http://arxiv.org/abs/2403.11227v1'>2403.11227v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14240v1")'>Enhanced Labeling Technique for Reddit Text and Fine-Tuned Longformer
  Models for Classifying Depression Severity in English and Luganda</div>
<div id='2401.14240v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T15:28:07Z</div><div>Authors: Richard Kimera, Daniela N. Rim, Joseph Kirabira, Ubong Godwin Udomah, Heeyoul Choi</div><div style='padding-top: 10px; width: 80ex'>Depression is a global burden and one of the most challenging mental health
conditions to control. Experts can detect its severity early using the Beck
Depression Inventory (BDI) questionnaire, administer appropriate medication to
patients, and impede its progression. Due to the fear of potential
stigmatization, many patients turn to social media platforms like Reddit for
advice and assistance at various stages of their journey. This research
extracts text from Reddit to facilitate the diagnostic process. It employs a
proposed labeling approach to categorize the text and subsequently fine-tunes
the Longformer model. The model's performance is compared against baseline
models, including Naive Bayes, Random Forest, Support Vector Machines, and
Gradient Boosting. Our findings reveal that the Longformer model outperforms
the baseline models in both English (48%) and Luganda (45%) languages on a
custom-made dataset.</div><div><a href='http://arxiv.org/abs/2401.14240v1'>2401.14240v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09151v1")'>Chinese MentalBERT: Domain-Adaptive Pre-training on Social Media for
  Chinese Mental Health Text Analysis</div>
<div id='2402.09151v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T13:08:25Z</div><div>Authors: Wei Zhai, Hongzhi Qi, Qing Zhao, Jianqiang Li, Ziqi Wang, Han Wang, Bing Xiang Yang, Guanghui Fu</div><div style='padding-top: 10px; width: 80ex'>In the current environment, psychological issues are prevalent and
widespread, with social media serving as a key outlet for individuals to share
their feelings. This results in the generation of vast quantities of data
daily, where negative emotions have the potential to precipitate crisis
situations. There is a recognized need for models capable of efficient
analysis. While pre-trained language models have demonstrated their
effectiveness broadly, there's a noticeable gap in pre-trained models tailored
for specialized domains like psychology. To address this, we have collected a
huge dataset from Chinese social media platforms and enriched it with publicly
available datasets to create a comprehensive database encompassing 3.36 million
text entries. To enhance the model's applicability to psychological text
analysis, we integrated psychological lexicons into the pre-training masking
mechanism. Building on an existing Chinese language model, we performed
adaptive training to develop a model specialized for the psychological domain.
We assessed our model's effectiveness across four public benchmarks, where it
not only surpassed the performance of standard pre-trained models but also
showed a inclination for making psychologically relevant predictions. Due to
concerns regarding data privacy, the dataset will not be made publicly
available. However, we have made the pre-trained models and codes publicly
accessible to the community via:
https://github.com/zwzzzQAQ/Chinese-MentalBERT.</div><div><a href='http://arxiv.org/abs/2402.09151v1'>2402.09151v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.10908v1")'>LLM-Assisted Crisis Management: Building Advanced LLM Platforms for
  Effective Emergency Response and Public Collaboration</div>
<div id='2402.10908v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T17:50:35Z</div><div>Authors: Hakan T. Otal, M. Abdullah Canbaz</div><div style='padding-top: 10px; width: 80ex'>Emergencies and critical incidents often unfold rapidly, necessitating a
swift and effective response. In this research, we introduce a novel approach
to identify and classify emergency situations from social media posts and
direct emergency messages using an open source Large Language Model, LLAMA2.
The goal is to harness the power of natural language processing and machine
learning to assist public safety telecommunicators and huge crowds during
countrywide emergencies. Our research focuses on developing a language model
that can understand users describe their situation in the 911 call, enabling
LLAMA2 to analyze the content and offer relevant instructions to the
telecommunicator, while also creating workflows to notify government agencies
with the caller's information when necessary. Another benefit this language
model provides is its ability to assist people during a significant emergency
incident when the 911 system is overwhelmed, by assisting the users with simple
instructions and informing authorities with their location and emergency
information.</div><div><a href='http://arxiv.org/abs/2402.10908v1'>2402.10908v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13547v1")'>Integrating Large Language Models for Severity Classification in Traffic
  Incident Management: A Machine Learning Approach</div>
<div id='2403.13547v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T12:33:51Z</div><div>Authors: Artur Grigorev, Khaled Saleh, Yuming Ou, Adriana-Simona Mihaita</div><div style='padding-top: 10px; width: 80ex'>This study evaluates the impact of large language models on enhancing machine
learning processes for managing traffic incidents. It examines the extent to
which features generated by modern language models improve or match the
accuracy of predictions when classifying the severity of incidents using
accident reports. Multiple comparisons performed between combinations of
language models and machine learning algorithms, including Gradient Boosted
Decision Trees, Random Forests, and Extreme Gradient Boosting. Our research
uses both conventional and language model-derived features from texts and
incident reports, and their combinations to perform severity classification.
Incorporating features from language models with those directly obtained from
incident reports has shown to improve, or at least match, the performance of
machine learning techniques in assigning severity levels to incidents,
particularly when employing Random Forests and Extreme Gradient Boosting
methods. This comparison was quantified using the F1-score over uniformly
sampled data sets to obtain balanced severity classes. The primary contribution
of this research is in the demonstration of how Large Language Models can be
integrated into machine learning workflows for incident management, thereby
simplifying feature extraction from unstructured text and enhancing or matching
the precision of severity predictions using conventional machine learning
pipeline. The engineering application of this research is illustrated through
the effective use of these language processing models to refine the modelling
process for incident severity classification. This work provides significant
insights into the application of language processing capabilities in
combination with traditional data for improving machine learning pipelines in
the context of classifying incident severity.</div><div><a href='http://arxiv.org/abs/2403.13547v1'>2403.13547v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.06789v1")'>Information Retrieval and Classification of Real-Time Multi-Source
  Hurricane Evacuation Notices</div>
<div id='2401.06789v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-07T16:35:30Z</div><div>Authors: Tingting Zhao, Shubo Tian, Jordan Daly, Melissa Geiger, Minna Jia, Jinfeng Zhang</div><div style='padding-top: 10px; width: 80ex'>For an approaching disaster, the tracking of time-sensitive critical
information such as hurricane evacuation notices is challenging in the United
States. These notices are issued and distributed rapidly by numerous local
authorities that may spread across multiple states. They often undergo frequent
updates and are distributed through diverse online portals lacking standard
formats. In this study, we developed an approach to timely detect and track the
locally issued hurricane evacuation notices. The text data were collected
mainly with a spatially targeted web scraping method. They were manually
labeled and then classified using natural language processing techniques with
deep learning models. The classification of mandatory evacuation notices
achieved a high accuracy (recall = 96%). We used Hurricane Ian (2022) to
illustrate how real-time evacuation notices extracted from local government
sources could be redistributed with a Web GIS system. Our method applied to
future hurricanes provides live data for situation awareness to higher-level
government agencies and news media. The archived data helps scholars to study
government responses toward weather warnings and individual behaviors
influenced by evacuation history. The framework may be applied to other types
of disasters for rapid and targeted retrieval, classification, redistribution,
and archiving of real-time government orders and notifications.</div><div><a href='http://arxiv.org/abs/2401.06789v1'>2401.06789v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.05365v1")'>The Impact of Quantization on the Robustness of Transformer-based Text
  Classifiers</div>
<div id='2403.05365v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T14:55:05Z</div><div>Authors: Seyed Parsa Neshaei, Yasaman Boreshban, Gholamreza Ghassem-Sani, Seyed Abolghasem Mirroshandel</div><div style='padding-top: 10px; width: 80ex'>Transformer-based models have made remarkable advancements in various NLP
areas. Nevertheless, these models often exhibit vulnerabilities when confronted
with adversarial attacks. In this paper, we explore the effect of quantization
on the robustness of Transformer-based models. Quantization usually involves
mapping a high-precision real number to a lower-precision value, aiming at
reducing the size of the model at hand. To the best of our knowledge, this work
is the first application of quantization on the robustness of NLP models. In
our experiments, we evaluate the impact of quantization on BERT and DistilBERT
models in text classification using SST-2, Emotion, and MR datasets. We also
evaluate the performance of these models against TextFooler, PWWS, and PSO
adversarial attacks. Our findings show that quantization significantly improves
(by an average of 18.68%) the adversarial accuracy of the models. Furthermore,
we compare the effect of quantization versus that of the adversarial training
approach on robustness. Our experiments indicate that quantization increases
the robustness of the model by 18.80% on average compared to adversarial
training without imposing any extra computational overhead during training.
Therefore, our results highlight the effectiveness of quantization in improving
the robustness of NLP models.</div><div><a href='http://arxiv.org/abs/2403.05365v1'>2403.05365v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.15189v1")'>Biomedical Entity Linking as Multiple Choice Question Answering</div>
<div id='2402.15189v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T08:40:38Z</div><div>Authors: Zhenxi Lin, Ziheng Zhang, Xian Wu, Yefeng Zheng</div><div style='padding-top: 10px; width: 80ex'>Although biomedical entity linking (BioEL) has made significant progress with
pre-trained language models, challenges still exist for fine-grained and
long-tailed entities. To address these challenges, we present BioELQA, a novel
model that treats Biomedical Entity Linking as Multiple Choice Question
Answering. BioELQA first obtains candidate entities with a fast retriever,
jointly presents the mention and candidate entities to a generator, and then
outputs the predicted symbol associated with its chosen entity. This
formulation enables explicit comparison of different candidate entities, thus
capturing fine-grained interactions between mentions and entities, as well as
among entities themselves. To improve generalization for long-tailed entities,
we retrieve similar labeled training instances as clues and concatenate the
input with retrieved instances for the generator. Extensive experimental
results show that BioELQA outperforms state-of-the-art baselines on several
datasets.</div><div><a href='http://arxiv.org/abs/2402.15189v1'>2402.15189v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.07384v1")'>SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large
  Language Models by Summarizing Training Trajectories of Small Models</div>
<div id='2403.07384v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T07:45:33Z</div><div>Authors: Yu Yang, Siddhartha Mishra, Jeffrey N Chiang, Baharan Mirzasoleiman</div><div style='padding-top: 10px; width: 80ex'>Despite the effectiveness of data selection for large language models (LLMs)
during pretraining and instruction fine-tuning phases, improving data
efficiency in supervised fine-tuning (SFT) for specialized domains poses
significant challenges due to the complexity of fine-tuning data. To bridge
this gap, we introduce an effective and scalable data selection method for SFT,
SmallToLarge (S2L), which leverages training trajectories from small models to
guide the data selection for larger models. We demonstrate through extensive
experiments that S2L significantly improves data efficiency in SFT for
mathematical problem-solving, reducing the training data to just 11% of the
original MathInstruct dataset (Yue et al., 2023) to match full dataset
performance while outperforming state-of-the-art data selection algorithms by
an average of 4.7% across 6 in- and out-domain evaluation datasets. Remarkably,
selecting only 50K data for SFT, S2L achieves a 32.7% accuracy on the most
challenging MATH (Hendrycks et al., 2021) benchmark, improving Phi-2 (Li et
al., 2023b) by 16.6%. In clinical text summarization on the MIMIC-III dataset
(Johnson et al., 2016), S2L again outperforms training on the full dataset
using only 50% of the data. Notably, S2L can perform data selection using a
reference model 40x smaller than the target model, proportionally reducing the
cost of data selection.</div><div><a href='http://arxiv.org/abs/2403.07384v1'>2403.07384v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05908v1")'>EpilepsyLLM: Domain-Specific Large Language Model Fine-tuned with
  Epilepsy Medical Knowledge</div>
<div id='2401.05908v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T13:39:00Z</div><div>Authors: Xuyang Zhao, Qibin Zhao, Toshihisa Tanaka</div><div style='padding-top: 10px; width: 80ex'>With large training datasets and massive amounts of computing sources, large
language models (LLMs) achieve remarkable performance in comprehensive and
generative ability. Based on those powerful LLMs, the model fine-tuned with
domain-specific datasets posseses more specialized knowledge and thus is more
practical like medical LLMs. However, the existing fine-tuned medical LLMs are
limited to general medical knowledge with English language. For
disease-specific problems, the model's response is inaccurate and sometimes
even completely irrelevant, especially when using a language other than
English. In this work, we focus on the particular disease of Epilepsy with
Japanese language and introduce a customized LLM termed as EpilepsyLLM. Our
model is trained from the pre-trained LLM by fine-tuning technique using
datasets from the epilepsy domain. The datasets contain knowledge of basic
information about disease, common treatment methods and drugs, and important
notes in life and work. The experimental results demonstrate that EpilepsyLLM
can provide more reliable and specialized medical knowledge responses.</div><div><a href='http://arxiv.org/abs/2401.05908v1'>2401.05908v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07920v1")'>ProtLLM: An Interleaved Protein-Language LLM with Protein-as-Word
  Pre-Training</div>
<div id='2403.07920v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T01:29:55Z</div><div>Authors: Le Zhuo, Zewen Chi, Minghao Xu, Heyan Huang, Heqi Zheng, Conghui He, Xian-Ling Mao, Wentao Zhang</div><div style='padding-top: 10px; width: 80ex'>We propose ProtLLM, a versatile cross-modal large language model (LLM) for
both protein-centric and protein-language tasks. ProtLLM features a unique
dynamic protein mounting mechanism, enabling it to handle complex inputs where
the natural language text is interspersed with an arbitrary number of proteins.
Besides, we propose the protein-as-word language modeling approach to train
ProtLLM. By developing a specialized protein vocabulary, we equip the model
with the capability to predict not just natural language but also proteins from
a vast pool of candidates. Additionally, we construct a large-scale interleaved
protein-text dataset, named InterPT, for pre-training. This dataset
comprehensively encompasses both (1) structured data sources like protein
annotations and (2) unstructured data sources like biological research papers,
thereby endowing ProtLLM with crucial knowledge for understanding proteins. We
evaluate ProtLLM on classic supervised protein-centric tasks and explore its
novel protein-language applications. Experimental results demonstrate that
ProtLLM not only achieves superior performance against protein-specialized
baselines on protein-centric tasks but also induces zero-shot and in-context
learning capabilities on protein-language tasks.</div><div><a href='http://arxiv.org/abs/2403.07920v1'>2403.07920v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15713v1")'>Contrastive Learning and Mixture of Experts Enables Precise Vector
  Embeddings</div>
<div id='2401.15713v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-28T17:34:42Z</div><div>Authors: Rohan Kapur, Logan Hallee, Arjun Patel, Bohdan Khomtchouk</div><div style='padding-top: 10px; width: 80ex'>The advancement of transformer neural networks has significantly elevated the
capabilities of sentence similarity models, particularly in creating effective
vector representations of natural language inputs. However, these models face
notable challenges in domain-specific contexts, especially in highly
specialized scientific sub-fields. Traditional methods often struggle in this
regime, either overgeneralizing similarities within a niche or being overly
sensitive to minor differences, resulting in inaccurate text classification and
subpar vector representation. In an era where retrieval augmentation and search
are increasingly crucial, precise and concise numerical representations are
essential. In this paper, we target this issue by assembling niche datasets
using co-citations as a similarity metric, focusing on biomedical domains. We
employ two key strategies for fine-tuning state-of-the-art models: 1.
Domain-specific Fine-Tuning, which tailors pretrained models to a single
domain, and 2. Universal Applicability with Mixture of Experts (MoE), adapting
pretrained models with enforced routing for multiple domains simultaneously.
Our training approach emphasizes the use of abstracts for faster training,
incorporating Multiple Negative Rankings loss for efficient contrastive
learning. Notably, our MoE variants, equipped with $N$ experts, achieve the
efficacy of $N$ individual models, heralding a new era of versatile,
One-Size-Fits-All transformer networks for various tasks. This methodology
marks significant advancements in scientific text classification metrics and
holds promise for enhancing vector database search and compilation.</div><div><a href='http://arxiv.org/abs/2401.15713v1'>2401.15713v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01916v1")'>CoLe and LYS at BioASQ MESINESP8 Task: similarity based descriptor
  assignment in Spanish</div>
<div id='2402.01916v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T21:36:03Z</div><div>Authors: Francisco J. Ribadas-Pena, Shuyuan Cao, Elmurod Kuriyozov</div><div style='padding-top: 10px; width: 80ex'>In this paper, we describe our participation in the MESINESP Task of the
BioASQ biomedical semantic indexing challenge. The participating system follows
an approach based solely on conventional information retrieval tools. We have
evaluated various alternatives for extracting index terms from IBECS/LILACS
documents in order to be stored in an Apache Lucene index. Those indexed
representations are queried using the contents of the article to be annotated
and a ranked list of candidate labels is created from the retrieved documents.
We also have evaluated a sort of limited Label Powerset approach which creates
meta-labels joining pairs of DeCS labels with high co-occurrence scores, and an
alternative method based on label profile matching. Results obtained in
official runs seem to confirm the suitability of this approach for languages
like Spanish.</div><div><a href='http://arxiv.org/abs/2402.01916v1'>2402.01916v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.15010v1")'>How Important Is Tokenization in French Medical Masked Language Models?</div>
<div id='2402.15010v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T23:11:08Z</div><div>Authors: Yanis Labrak, Adrien Bazoge, Beatrice Daille, Mickael Rouvier, Richard Dufour</div><div style='padding-top: 10px; width: 80ex'>Subword tokenization has become the prevailing standard in the field of
natural language processing (NLP) over recent years, primarily due to the
widespread utilization of pre-trained language models. This shift began with
Byte-Pair Encoding (BPE) and was later followed by the adoption of
SentencePiece and WordPiece. While subword tokenization consistently
outperforms character and word-level tokenization, the precise factors
contributing to its success remain unclear. Key aspects such as the optimal
segmentation granularity for diverse tasks and languages, the influence of data
sources on tokenizers, and the role of morphological information in
Indo-European languages remain insufficiently explored. This is particularly
pertinent for biomedical terminology, characterized by specific rules governing
morpheme combinations. Despite the agglutinative nature of biomedical
terminology, existing language models do not explicitly incorporate this
knowledge, leading to inconsistent tokenization strategies for common terms. In
this paper, we seek to delve into the complexities of subword tokenization in
French biomedical domain across a variety of NLP tasks and pinpoint areas where
further enhancements can be made. We analyze classical tokenization algorithms,
including BPE and SentencePiece, and introduce an original tokenization
strategy that integrates morpheme-enriched word segmentation into existing
tokenization methods.</div><div><a href='http://arxiv.org/abs/2402.15010v1'>2402.15010v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12212v1")'>Evaluating Named Entity Recognition: Comparative Analysis of Mono- and
  Multilingual Transformer Models on Brazilian Corporate Earnings Call
  Transcriptions</div>
<div id='2403.12212v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T19:53:56Z</div><div>Authors: Ramon Abilio, Guilherme Palermo Coelho, Ana Estela Antunes da Silva</div><div style='padding-top: 10px; width: 80ex'>Named Entity Recognition (NER) is a Natural Language Processing technique for
extracting information from textual documents. However, much of the existing
research on NER has been centered around English-language documents, leaving a
gap in the availability of datasets tailored to the financial domain in
Portuguese. This study addresses the need for NER within the financial domain,
focusing on Portuguese-language texts extracted from earnings call
transcriptions of Brazilian banks. By curating a comprehensive dataset
comprising 384 transcriptions and leveraging weak supervision techniques for
annotation, we evaluate the performance of monolingual models trained on
Portuguese (BERTimbau and PTT5) and multilingual models (mBERT and mT5).
Notably, we introduce a novel approach that reframes the token classification
task as a text generation problem, enabling fine-tuning and evaluation of T5
models. Following the fine-tuning of the models, we conduct an evaluation on
the test dataset, employing performance and error metrics. Our findings reveal
that BERT-based models consistently outperform T5-based models. Furthermore,
while the multilingual models exhibit comparable macro F1-scores, BERTimbau
demonstrates superior performance over PTT5. A manual analysis of sentences
generated by PTT5 and mT5 unveils a degree of similarity ranging from 0.89 to
1.0, between the original and generated sentences. However, critical errors
emerge as both models exhibit discrepancies, such as alterations to monetary
and percentage values, underscoring the importance of accuracy and consistency
in the financial domain. Despite these challenges, PTT5 and mT5 achieve
impressive macro F1-scores of 98.52% and 98.85%, respectively, with our
proposed approach. Furthermore, our study sheds light on notable disparities in
memory and time consumption for inference across the models.</div><div><a href='http://arxiv.org/abs/2403.12212v1'>2403.12212v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01326v2")'>An Autoregressive Text-to-Graph Framework for Joint Entity and Relation
  Extraction</div>
<div id='2401.01326v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T18:32:14Z</div><div>Authors: Urchade Zaratiana, Nadi Tomeh, Pierre Holat, Thierry Charnois</div><div style='padding-top: 10px; width: 80ex'>In this paper, we propose a novel method for joint entity and relation
extraction from unstructured text by framing it as a conditional sequence
generation problem. In contrast to conventional generative information
extraction models that are left-to-right token-level generators, our approach
is \textit{span-based}. It generates a linearized graph where nodes represent
text spans and edges represent relation triplets. Our method employs a
transformer encoder-decoder architecture with pointing mechanism on a dynamic
vocabulary of spans and relation types. Our model can capture the structural
characteristics and boundaries of entities and relations through span
representations while simultaneously grounding the generated output in the
original text thanks to the pointing mechanism. Evaluation on benchmark
datasets validates the effectiveness of our approach, demonstrating competitive
results. Code is available at https://github.com/urchade/ATG.</div><div><a href='http://arxiv.org/abs/2401.01326v2'>2401.01326v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.11485v1")'>LEIA: Facilitating Cross-Lingual Knowledge Transfer in Language Models
  with Entity-based Data Augmentation</div>
<div id='2402.11485v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T07:24:34Z</div><div>Authors: Ikuya Yamada, Ryokan Ri</div><div style='padding-top: 10px; width: 80ex'>Adapting English-based large language models (LLMs) to other languages has
become increasingly popular due to the efficiency and potential of
cross-lingual transfer. However, existing language adaptation methods often
overlook the benefits of cross-lingual supervision. In this study, we introduce
LEIA, a language adaptation tuning method that utilizes Wikipedia entity names
aligned across languages. This method involves augmenting the target language
corpus with English entity names and training the model using left-to-right
language modeling. We assess LEIA on diverse question answering datasets using
7B-parameter LLMs, demonstrating significant performance gains across various
non-English languages. The source code is available at
https://github.com/studio-ousia/leia.</div><div><a href='http://arxiv.org/abs/2402.11485v1'>2402.11485v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02099v1")'>Analyzing the Evaluation of Cross-Lingual Knowledge Transfer in
  Multilingual Language Models</div>
<div id='2402.02099v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T09:41:52Z</div><div>Authors: Sara Rajaee, Christof Monz</div><div style='padding-top: 10px; width: 80ex'>Recent advances in training multilingual language models on large datasets
seem to have shown promising results in knowledge transfer across languages and
achieve high performance on downstream tasks. However, we question to what
extent the current evaluation benchmarks and setups accurately measure
zero-shot cross-lingual knowledge transfer. In this work, we challenge the
assumption that high zero-shot performance on target tasks reflects high
cross-lingual ability by introducing more challenging setups involving
instances with multiple languages. Through extensive experiments and analysis,
we show that the observed high performance of multilingual models can be
largely attributed to factors not requiring the transfer of actual linguistic
knowledge, such as task- and surface-level knowledge. More specifically, we
observe what has been transferred across languages is mostly data artifacts and
biases, especially for low-resource languages. Our findings highlight the
overlooked drawbacks of existing cross-lingual test data and evaluation setups,
calling for a more nuanced understanding of the cross-lingual capabilities of
multilingual models.</div><div><a href='http://arxiv.org/abs/2402.02099v1'>2402.02099v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.02449v1")'>Surfing the modeling of PoS taggers in low-resource scenarios</div>
<div id='2402.02449v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T11:38:12Z</div><div>Authors: Manuel Vilares Ferro, Víctor M. Darriba Bilbao, Francisco J. Ribadas-Pena, Jorge Graña Gil</div><div style='padding-top: 10px; width: 80ex'>The recent trend towards the application of deep structured techniques has
revealed the limits of huge models in natural language processing. This has
reawakened the interest in traditional machine learning algorithms, which have
proved still to be competitive in certain contexts, in particular low-resource
settings. In parallel, model selection has become an essential task to boost
performance at reasonable cost, even more so when we talk about processes
involving domains where the training and/or computational resources are scarce.
Against this backdrop, we evaluate the early estimation of learning curves as a
practical mechanism for selecting the most appropriate model in scenarios
characterized by the use of non-deep learners in resource-lean settings. On the
basis of a formal approximation model previously evaluated under conditions of
wide availability of training and validation resources, we study the
reliability of such an approach in a different and much more demanding
operationalenvironment. Using as case study the generation of PoS taggers for
Galician, a language belonging to the Western Ibero-Romance group, the
experimental results are consistent with our expectations.</div><div><a href='http://arxiv.org/abs/2402.02449v1'>2402.02449v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12973v2")'>In-Context Language Learning: Architectures and Algorithms</div>
<div id='2401.12973v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T18:59:21Z</div><div>Authors: Ekin Akyürek, Bailin Wang, Yoon Kim, Jacob Andreas</div><div style='padding-top: 10px; width: 80ex'>Large-scale neural language models exhibit a remarkable capacity for
in-context learning (ICL): they can infer novel functions from datasets
provided as input. Most of our current understanding of when and how ICL arises
comes from LMs trained on extremely simple learning problems like linear
regression and associative recall. There remains a significant gap between
these model problems and the "real" ICL exhibited by LMs trained on large text
corpora, which involves not just retrieval and function approximation but
free-form generation of language and other structured outputs. In this paper,
we study ICL through the lens of a new family of model problems we term in
context language learning (ICLL). In ICLL, LMs are presented with a set of
strings from a formal language, and must generate additional strings from the
same language. We focus on in-context learning of regular languages generated
by random finite automata. We evaluate a diverse set of neural sequence models
(including several RNNs, Transformers, and state-space model variants) on
regular ICLL tasks, aiming to answer three questions: (1) Which model classes
are empirically capable of ICLL? (2) What algorithmic solutions do successful
models implement to perform ICLL? (3) What architectural changes can improve
ICLL in less performant models? We first show that Transformers significantly
outperform neural sequence models with recurrent or convolutional
representations on ICLL tasks. Next, we provide evidence that their ability to
do so relies on specialized "n-gram heads" (higher-order variants of induction
heads) that compute input-conditional next-token distributions. Finally, we
show that hard-wiring these heads into neural models improves performance not
just on ICLL, but natural language modeling -- improving the perplexity of
340M-parameter models by up to 1.14 points (6.7%) on the SlimPajama dataset.</div><div><a href='http://arxiv.org/abs/2401.12973v2'>2401.12973v2</a></div>
</div></div>
    <div><a href="arxiv_18.html">Prev (18)</a></div>
    <div><a href="arxiv_20.html">Next (20)</a></div>
    